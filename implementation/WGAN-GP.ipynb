{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['concatenate', 'datetime', 'maximum', 'add', 'average', 'multiply', 'minimum', 'copy', 'dot', 'subtract']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data\n",
    "    \n",
    "class MinibatchDiscrimination(Layer):\n",
    "    \"\"\"Concatenates to each sample information about how different the input\n",
    "    features for that sample are from features of other samples in the same\n",
    "    minibatch, as described in Salimans et. al. (2016). Useful for preventing\n",
    "    GANs from collapsing to a single output. When using this layer, generated\n",
    "    samples and reference samples should be in separate batches.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_dim, init='glorot_uniform', weights=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "\n",
    "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
    "            initializer=self.init,\n",
    "            name='kernel',\n",
    "            regularizer=self.W_regularizer,\n",
    "            trainable=True,\n",
    "            constraint=self.W_constraint)\n",
    "\n",
    "        # Set built to true.\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
    "        diffs = K.expand_dims(activation, 3) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
    "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
    "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
    "        return K.concatenate([x, minibatch_features], 1)\n",
    "#         return minibatch_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], input_shape[1]+self.nb_kernels\n",
    "#         return input_shape[0], self.nb_kernels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_kernels': self.nb_kernels,\n",
    "                  'kernel_dim': self.kernel_dim,\n",
    "#                   'init': self.init.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms, this outputs a random point on the line\n",
    "    between each pair of input points.\n",
    "    Inheriting from _Merge is a little messy but it was the quickest solution I could think of.\n",
    "    Improvements appreciated.\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "code_folding": [
     1,
     36,
     53,
     72,
     130,
     166,
     184,
     197,
     202,
     208,
     212,
     215,
     221
    ]
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self, timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._run_dir = run_dir\n",
    "        self._img_dir = img_dir\n",
    "        self._model_dir = model_dir\n",
    "        self._generated_datesets_dir = generated_datesets_dir\n",
    "        \n",
    "        self._save_config()\n",
    "        \n",
    "        self._epoch = 0\n",
    "        self._losses = [[], []]\n",
    "\n",
    "    def build_models(self, generator_lr, critic_lr):        \n",
    "        self._generator = self._build_generator(self._latent_dim, self._timesteps)\n",
    "        self._critic = self._build_critic(self._timesteps)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self._critic.compile(loss=self._wasserstein_loss, optimizer=RMSprop(critic_lr))\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        self._critic.trainable = False\n",
    "\n",
    "        valid = self._critic(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss=self._wasserstein_loss,\n",
    "            optimizer=RMSprop(generator_lr),\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "\n",
    "    def _build_generator(self, noise_dim, timesteps):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "        generated = generator_inputs\n",
    "        \n",
    "        generated = Lambda(lambda x: K.expand_dims(x))(generated)\n",
    "        while generated.shape[1] < timesteps:\n",
    "            generated = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(generated)\n",
    "            generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(\n",
    "            1, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "        generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _build_critic(self, timesteps):\n",
    "        critic_inputs = Input((timesteps, ))\n",
    "        criticized = MinibatchDiscrimination(5, 3)(critic_inputs)\n",
    "        \n",
    "        criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "            criticized)\n",
    "        while criticized.shape[1] > 1:\n",
    "            criticized = Conv1D(\n",
    "                32, 3, activation='tanh', padding='same')(criticized)\n",
    "            criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        criticized = Flatten()(criticized)\n",
    "#         criticized = Concatenate()([criticized, critic_inputs])\n",
    "        criticized = Dense(32, activation='tanh')(criticized)\n",
    "        criticized = Dense(15, activation='tanh')(criticized)\n",
    "        criticized = Dense(1)(criticized) \n",
    "\n",
    "        critic = Model(critic_inputs, criticized, 'critic')\n",
    "        return critic\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        \n",
    "        while self._epoch < epochs:\n",
    "            self._epoch += 1\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                critic_loss_real = self._critic.train_on_batch(\n",
    "                    batch_transactions, -np.ones((half_batch, 1)))\n",
    "                critic_loss_fake = self._critic.train_on_batch(\n",
    "                    generated_transactions, np.ones((half_batch, 1)))\n",
    "                critic_loss = 0.5 * np.add(critic_loss_real,\n",
    "                                                  critic_loss_fake)\n",
    "\n",
    "#                 self._clip_weights(clip_value)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, -np.ones((batch_size, 1)))[0]\n",
    "            \n",
    "            generator_loss = 1 - generator_loss\n",
    "            critic_loss = 1 - critic_loss\n",
    "            \n",
    "            self._losses[0].append(generator_loss)\n",
    "            self._losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [C loss: %f] [G loss: %f]\" % (self._epoch, critic_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if self._epoch % img_frequency == 0:\n",
    "                self._save_imgs()\n",
    "            \n",
    "            if self._epoch % model_save_frequency == 0:\n",
    "                self._save_models()\n",
    "                \n",
    "            if self._epoch % dataset_generation_frequency == 0:\n",
    "                self._generate_dataset(epoch, dataset_generation_size)\n",
    "                \n",
    "            if self._epoch % 250 == 0:\n",
    "                self._save_losses()\n",
    "          \n",
    "        self._generate_dataset(epochs, dataset_generation_size)\n",
    "        self._save_losses(losses)\n",
    "        self._save_models()\n",
    "        self._save_imgs()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _save_imgs(self):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('%05d.png' % self._epoch)))\n",
    "        plt.savefig(str(self._img_dir / 'last.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        if self._latent_dim == 2:\n",
    "            plt.subplots(5, 5, figsize=(15, 5))\n",
    "\n",
    "            for i, v_i in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                for j, v_j in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                    plt.subplot(5, 5, i*5+j+1)\n",
    "                    plt.plot(self._generator.predict(np.array([v_i, v_j]).reshape((1, 2))).T)\n",
    "                    plt.xticks([])\n",
    "                    plt.yticks([])\n",
    "                    plt.ylim(-1, 1)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(str(self._img_dir / ('latent_space.png')))\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "\n",
    "    def _save_losses(self):\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(self._losses[0])\n",
    "        plt.plot(self._losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig(str(self._img_dir / 'losses.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        with open(str(self._run_dir / 'losses.p'), 'wb') as f:\n",
    "            pickle.dump(self._losses, f)\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        for l in self._critic.layers:\n",
    "#             if 'minibatch_discrimination' not in l.name:\n",
    "            weights = [np.clip(w, -clip_value, clip_value) for w in l.get_weights()]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            'timesteps' : self._timesteps,\n",
    "            'latent_dim' : self._latent_dim,\n",
    "            'run_dir' : self._run_dir,\n",
    "            'img_dir' : self._img_dir,\n",
    "            'model_dir' : self._model_dir,\n",
    "            'generated_datesets_dir' : self._generated_datesets_dir\n",
    "        }\n",
    "        \n",
    "        with open(str(self._run_dir / 'config.p'), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "    def _save_models(self):\n",
    "        self._gan.save(self._model_dir / 'wgan.h5')\n",
    "        self._generator.save(self._model_dir / 'generator.h5')\n",
    "        self._critic.save(self._model_dir / 'critic.h5')\n",
    "        \n",
    "    def _generate_dataset(self, epoch, dataset_generation_size):\n",
    "        z_samples = np.random.normal(0, 1, (dataset_generation_size, self._latent_dim))\n",
    "        generated_dataset = self._generator.predict(z_samples)\n",
    "        np.save(self._generated_datesets_dir / ('%d_generated_data' % epoch), generated_dataset)\n",
    "        np.save(self._generated_datesets_dir / 'last', generated_dataset)\n",
    "        \n",
    "    def get_models(self):\n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def restore_training(self):\n",
    "        load_models()\n",
    "        with open(self._run_dir / 'losses.p', 'rb') as f:\n",
    "            self._losses = pickle.load(f)\n",
    "            self._epoch = len(self._losses)\n",
    "        \n",
    "    def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        \"\"\"Calculates the gradient penalty loss for a batch of \"averaged\" samples.\n",
    "        In Improved WGANs, the 1-Lipschitz constraint is enforced by adding a term to the loss function\n",
    "        that penalizes the network if the gradient norm moves away from 1. However, it is impossible to evaluate\n",
    "        this function at all points in the input space. The compromise used in the paper is to choose random points\n",
    "        on the lines between real and generated samples, and check the gradients at these points. Note that it is the\n",
    "        gradient w.r.t. the input averaged samples, not the weights of the discriminator, that we're penalizing!\n",
    "        In order to evaluate the gradients, we must first run samples through the generator and evaluate the loss.\n",
    "        Then we get the gradients of the discriminator w.r.t. the input averaged samples.\n",
    "        The l2 norm and penalty can then be calculated for this gradient.\n",
    "        Note that this loss function requires the original averaged samples as input, but Keras only supports passing\n",
    "        y_true and y_pred to loss functions. To get around this, we make a partial() of the function with the\n",
    "        averaged_samples argument, and use that for model training.\"\"\"\n",
    "        # first get the gradients:\n",
    "        #   assuming: - that y_pred has dimensions (batch_size, 1)\n",
    "        #             - averaged_samples has dimensions (batch_size, nbr_features)\n",
    "        # gradients afterwards has dimension (batch_size, nbr_features), basically\n",
    "        # a list of nbr_features-dimensional gradient vectors\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    def load_models(self):\n",
    "        self._gan = load_model(self._model_dir / 'wgan.h5')\n",
    "        self._generator = load_model(self._model_dir / 'generator.h5')\n",
    "        self._critic = load_model(self._model_dir / 'critic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53888, 100)\n"
     ]
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../datasets/berka_dataset/usable/normalized_transactions.npy\"\n",
    "\n",
    "timesteps = 100\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions = split_data(transactions, timesteps)\n",
    "transactions = transactions[np.std(transactions, 1) > float(1e-7)]\n",
    "N, D = transactions.shape\n",
    "print(transactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50000\n",
    "n_critic = 10\n",
    "n_generator = 1\n",
    "latent_dim = 2\n",
    "generator_lr = 0.00005\n",
    "critic_lr = 0.00005\n",
    "clip_value = 0.05\n",
    "img_frequency = 250\n",
    "model_save_frequency = 3000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('wgan')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.033217] [G loss: 1.095408]\n",
      "2 [D loss: 1.064658] [G loss: 1.074012]\n",
      "3 [D loss: 1.091514] [G loss: 1.047046]\n",
      "4 [D loss: 1.132359] [G loss: 1.017008]\n",
      "5 [D loss: 1.175182] [G loss: 0.982249]\n",
      "6 [D loss: 1.229404] [G loss: 0.940496]\n",
      "7 [D loss: 1.318202] [G loss: 0.885099]\n",
      "8 [D loss: 1.389257] [G loss: 0.822847]\n",
      "9 [D loss: 1.514111] [G loss: 0.754643]\n",
      "10 [D loss: 1.599586] [G loss: 0.666971]\n",
      "11 [D loss: 1.736487] [G loss: 0.558934]\n",
      "12 [D loss: 1.874529] [G loss: 0.427570]\n",
      "13 [D loss: 2.048684] [G loss: 0.277749]\n",
      "14 [D loss: 2.197595] [G loss: 0.110656]\n",
      "15 [D loss: 2.393756] [G loss: -0.081693]\n",
      "16 [D loss: 2.548490] [G loss: -0.278859]\n",
      "17 [D loss: 2.763986] [G loss: -0.492972]\n",
      "18 [D loss: 2.952600] [G loss: -0.702622]\n",
      "19 [D loss: 3.133370] [G loss: -0.918437]\n",
      "20 [D loss: 3.336235] [G loss: -1.137592]\n",
      "21 [D loss: 3.493602] [G loss: -1.330008]\n",
      "22 [D loss: 3.650474] [G loss: -1.509967]\n",
      "23 [D loss: 3.782592] [G loss: -1.659158]\n",
      "24 [D loss: 3.901990] [G loss: -1.799540]\n",
      "25 [D loss: 4.018501] [G loss: -1.932117]\n",
      "26 [D loss: 4.119414] [G loss: -2.046924]\n",
      "27 [D loss: 4.216318] [G loss: -2.155175]\n",
      "28 [D loss: 4.304170] [G loss: -2.252088]\n",
      "29 [D loss: 4.378975] [G loss: -2.341238]\n",
      "30 [D loss: 4.455293] [G loss: -2.420378]\n",
      "31 [D loss: 4.507484] [G loss: -2.493651]\n",
      "32 [D loss: 4.587223] [G loss: -2.563525]\n",
      "33 [D loss: 4.640399] [G loss: -2.626355]\n",
      "34 [D loss: 4.701705] [G loss: -2.687964]\n",
      "35 [D loss: 4.754503] [G loss: -2.743605]\n",
      "36 [D loss: 4.803432] [G loss: -2.797378]\n",
      "37 [D loss: 4.851501] [G loss: -2.844945]\n",
      "38 [D loss: 4.894439] [G loss: -2.890027]\n",
      "39 [D loss: 4.934032] [G loss: -2.933169]\n",
      "40 [D loss: 4.976592] [G loss: -2.973927]\n",
      "41 [D loss: 5.015823] [G loss: -3.013937]\n",
      "42 [D loss: 5.052163] [G loss: -3.050858]\n",
      "43 [D loss: 5.087139] [G loss: -3.086658]\n",
      "44 [D loss: 5.120536] [G loss: -3.120392]\n",
      "45 [D loss: 5.152660] [G loss: -3.153147]\n",
      "46 [D loss: 5.183619] [G loss: -3.184109]\n",
      "47 [D loss: 5.213209] [G loss: -3.214379]\n",
      "48 [D loss: 5.242000] [G loss: -3.243424]\n",
      "49 [D loss: 5.269894] [G loss: -3.271486]\n",
      "50 [D loss: 5.296025] [G loss: -3.298818]\n",
      "51 [D loss: 5.323252] [G loss: -3.325305]\n",
      "52 [D loss: 5.349238] [G loss: -3.351299]\n",
      "53 [D loss: 5.374681] [G loss: -3.376842]\n",
      "54 [D loss: 5.399796] [G loss: -3.402132]\n",
      "55 [D loss: 5.424626] [G loss: -3.427109]\n",
      "56 [D loss: 5.448804] [G loss: -3.452053]\n",
      "57 [D loss: 5.474077] [G loss: -3.477011]\n",
      "58 [D loss: 5.498897] [G loss: -3.502028]\n",
      "59 [D loss: 5.524081] [G loss: -3.527499]\n",
      "60 [D loss: 5.549504] [G loss: -3.553287]\n",
      "61 [D loss: 5.575454] [G loss: -3.579544]\n",
      "62 [D loss: 5.601967] [G loss: -3.606238]\n",
      "63 [D loss: 5.628973] [G loss: -3.633452]\n",
      "64 [D loss: 5.656389] [G loss: -3.660884]\n",
      "65 [D loss: 5.684294] [G loss: -3.688705]\n",
      "66 [D loss: 5.712583] [G loss: -3.716891]\n",
      "67 [D loss: 5.741205] [G loss: -3.745239]\n",
      "68 [D loss: 5.770041] [G loss: -3.773591]\n",
      "69 [D loss: 5.799037] [G loss: -3.802230]\n",
      "70 [D loss: 5.827990] [G loss: -3.831001]\n",
      "71 [D loss: 5.856523] [G loss: -3.859450]\n",
      "72 [D loss: 5.883714] [G loss: -3.886651]\n",
      "73 [D loss: 5.909498] [G loss: -3.912332]\n",
      "74 [D loss: 5.934526] [G loss: -3.937425]\n",
      "75 [D loss: 5.959353] [G loss: -3.961998]\n",
      "76 [D loss: 5.983502] [G loss: -3.986059]\n",
      "77 [D loss: 6.007197] [G loss: -4.009542]\n",
      "78 [D loss: 6.030208] [G loss: -4.032458]\n",
      "79 [D loss: 6.052721] [G loss: -4.054854]\n",
      "80 [D loss: 6.074782] [G loss: -4.076793]\n",
      "81 [D loss: 6.096357] [G loss: -4.098306]\n",
      "82 [D loss: 6.117547] [G loss: -4.119426]\n",
      "83 [D loss: 6.138345] [G loss: -4.140147]\n",
      "84 [D loss: 6.158740] [G loss: -4.160516]\n",
      "85 [D loss: 6.178823] [G loss: -4.180542]\n",
      "86 [D loss: 6.198548] [G loss: -4.200243]\n",
      "87 [D loss: 6.217978] [G loss: -4.219638]\n",
      "88 [D loss: 6.237121] [G loss: -4.238745]\n",
      "89 [D loss: 6.255982] [G loss: -4.257576]\n",
      "90 [D loss: 6.274594] [G loss: -4.276156]\n",
      "91 [D loss: 6.292968] [G loss: -4.294497]\n",
      "92 [D loss: 6.311120] [G loss: -4.312625]\n",
      "93 [D loss: 6.329065] [G loss: -4.330546]\n",
      "94 [D loss: 6.346817] [G loss: -4.348272]\n",
      "95 [D loss: 6.364387] [G loss: -4.365823]\n",
      "96 [D loss: 6.381801] [G loss: -4.383209]\n",
      "97 [D loss: 6.399058] [G loss: -4.400444]\n",
      "98 [D loss: 6.416172] [G loss: -4.417539]\n",
      "99 [D loss: 6.433156] [G loss: -4.434505]\n",
      "100 [D loss: 6.450020] [G loss: -4.451350]\n",
      "101 [D loss: 6.466771] [G loss: -4.468085]\n",
      "102 [D loss: 6.483416] [G loss: -4.484715]\n",
      "103 [D loss: 6.499964] [G loss: -4.501249]\n",
      "104 [D loss: 6.516419] [G loss: -4.517692]\n",
      "105 [D loss: 6.532789] [G loss: -4.534049]\n",
      "106 [D loss: 6.549078] [G loss: -4.550327]\n",
      "107 [D loss: 6.565292] [G loss: -4.566532]\n",
      "108 [D loss: 6.581438] [G loss: -4.582668]\n",
      "109 [D loss: 6.597519] [G loss: -4.598741]\n",
      "110 [D loss: 6.613540] [G loss: -4.614753]\n",
      "111 [D loss: 6.629504] [G loss: -4.630710]\n",
      "112 [D loss: 6.645414] [G loss: -4.646614]\n",
      "113 [D loss: 6.661275] [G loss: -4.662468]\n",
      "114 [D loss: 6.677088] [G loss: -4.678275]\n",
      "115 [D loss: 6.692855] [G loss: -4.694037]\n",
      "116 [D loss: 6.708581] [G loss: -4.709758]\n",
      "117 [D loss: 6.724267] [G loss: -4.725438]\n",
      "118 [D loss: 6.739914] [G loss: -4.741080]\n",
      "119 [D loss: 6.755526] [G loss: -4.756687]\n",
      "120 [D loss: 6.771104] [G loss: -4.772261]\n",
      "121 [D loss: 6.786649] [G loss: -4.787802]\n",
      "122 [D loss: 6.802164] [G loss: -4.803312]\n",
      "123 [D loss: 6.817650] [G loss: -4.818795]\n",
      "124 [D loss: 6.833109] [G loss: -4.834250]\n",
      "125 [D loss: 6.848543] [G loss: -4.849681]\n",
      "126 [D loss: 6.863952] [G loss: -4.865087]\n",
      "127 [D loss: 6.879339] [G loss: -4.880470]\n",
      "128 [D loss: 6.894704] [G loss: -4.895832]\n",
      "129 [D loss: 6.910048] [G loss: -4.911174]\n",
      "130 [D loss: 6.925373] [G loss: -4.926497]\n",
      "131 [D loss: 6.940681] [G loss: -4.941801]\n",
      "132 [D loss: 6.955970] [G loss: -4.957088]\n",
      "133 [D loss: 6.971244] [G loss: -4.972361]\n",
      "134 [D loss: 6.986502] [G loss: -4.987617]\n",
      "135 [D loss: 7.001746] [G loss: -5.002859]\n",
      "136 [D loss: 7.016977] [G loss: -5.018087]\n",
      "137 [D loss: 7.032194] [G loss: -5.033303]\n",
      "138 [D loss: 7.047399] [G loss: -5.048507]\n",
      "139 [D loss: 7.062593] [G loss: -5.063699]\n",
      "140 [D loss: 7.077776] [G loss: -5.078880]\n",
      "141 [D loss: 7.092948] [G loss: -5.094051]\n",
      "142 [D loss: 7.108112] [G loss: -5.109213]\n",
      "143 [D loss: 7.123266] [G loss: -5.124366]\n",
      "144 [D loss: 7.138411] [G loss: -5.139510]\n",
      "145 [D loss: 7.153548] [G loss: -5.154646]\n",
      "146 [D loss: 7.168677] [G loss: -5.169774]\n",
      "147 [D loss: 7.183800] [G loss: -5.184896]\n",
      "148 [D loss: 7.198915] [G loss: -5.200010]\n",
      "149 [D loss: 7.214024] [G loss: -5.215118]\n",
      "150 [D loss: 7.229126] [G loss: -5.230220]\n",
      "151 [D loss: 7.244224] [G loss: -5.245316]\n",
      "152 [D loss: 7.259315] [G loss: -5.260406]\n",
      "153 [D loss: 7.274401] [G loss: -5.275492]\n",
      "154 [D loss: 7.289483] [G loss: -5.290573]\n",
      "155 [D loss: 7.304560] [G loss: -5.305650]\n",
      "156 [D loss: 7.319633] [G loss: -5.320722]\n",
      "157 [D loss: 7.334701] [G loss: -5.335790]\n",
      "158 [D loss: 7.349766] [G loss: -5.350855]\n",
      "159 [D loss: 7.364828] [G loss: -5.365915]\n",
      "160 [D loss: 7.379885] [G loss: -5.380973]\n",
      "161 [D loss: 7.394940] [G loss: -5.396028]\n",
      "162 [D loss: 7.409992] [G loss: -5.411078]\n",
      "163 [D loss: 7.425041] [G loss: -5.426127]\n",
      "164 [D loss: 7.440087] [G loss: -5.441173]\n",
      "165 [D loss: 7.455131] [G loss: -5.456217]\n",
      "166 [D loss: 7.470172] [G loss: -5.471257]\n",
      "167 [D loss: 7.485211] [G loss: -5.486296]\n",
      "168 [D loss: 7.500248] [G loss: -5.501333]\n",
      "169 [D loss: 7.515283] [G loss: -5.516368]\n",
      "170 [D loss: 7.530315] [G loss: -5.531400]\n",
      "171 [D loss: 7.545348] [G loss: -5.546432]\n",
      "172 [D loss: 7.560377] [G loss: -5.561461]\n",
      "173 [D loss: 7.575405] [G loss: -5.576488]\n",
      "174 [D loss: 7.590431] [G loss: -5.591515]\n",
      "175 [D loss: 7.605456] [G loss: -5.606540]\n",
      "176 [D loss: 7.620480] [G loss: -5.621563]\n",
      "177 [D loss: 7.635503] [G loss: -5.636586]\n",
      "178 [D loss: 7.650524] [G loss: -5.651608]\n",
      "179 [D loss: 7.665544] [G loss: -5.666627]\n",
      "180 [D loss: 7.680563] [G loss: -5.681646]\n",
      "181 [D loss: 7.695581] [G loss: -5.696664]\n",
      "182 [D loss: 7.710598] [G loss: -5.711681]\n",
      "183 [D loss: 7.725615] [G loss: -5.726698]\n",
      "184 [D loss: 7.740631] [G loss: -5.741713]\n",
      "185 [D loss: 7.755646] [G loss: -5.756728]\n",
      "186 [D loss: 7.770659] [G loss: -5.771741]\n",
      "187 [D loss: 7.785673] [G loss: -5.786755]\n",
      "188 [D loss: 7.800686] [G loss: -5.801767]\n",
      "189 [D loss: 7.815698] [G loss: -5.816780]\n",
      "190 [D loss: 7.830709] [G loss: -5.831790]\n",
      "191 [D loss: 7.845720] [G loss: -5.846802]\n",
      "192 [D loss: 7.860731] [G loss: -5.861812]\n",
      "193 [D loss: 7.875740] [G loss: -5.876822]\n",
      "194 [D loss: 7.890749] [G loss: -5.891831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 [D loss: 7.905759] [G loss: -5.906840]\n",
      "196 [D loss: 7.920768] [G loss: -5.921849]\n",
      "197 [D loss: 7.935776] [G loss: -5.936857]\n",
      "198 [D loss: 7.950784] [G loss: -5.951865]\n",
      "199 [D loss: 7.965791] [G loss: -5.966872]\n",
      "200 [D loss: 7.980798] [G loss: -5.981879]\n",
      "201 [D loss: 7.995806] [G loss: -5.996886]\n",
      "202 [D loss: 8.010811] [G loss: -6.011893]\n",
      "203 [D loss: 8.025818] [G loss: -6.026899]\n",
      "204 [D loss: 8.040824] [G loss: -6.041905]\n",
      "205 [D loss: 8.055830] [G loss: -6.056911]\n",
      "206 [D loss: 8.070836] [G loss: -6.071918]\n",
      "207 [D loss: 8.085842] [G loss: -6.086923]\n",
      "208 [D loss: 8.100848] [G loss: -6.101928]\n",
      "209 [D loss: 8.115853] [G loss: -6.116934]\n",
      "210 [D loss: 8.130858] [G loss: -6.131939]\n",
      "211 [D loss: 8.145863] [G loss: -6.146943]\n",
      "212 [D loss: 8.160868] [G loss: -6.161949]\n",
      "213 [D loss: 8.175872] [G loss: -6.176954]\n",
      "214 [D loss: 8.190877] [G loss: -6.191958]\n",
      "215 [D loss: 8.205882] [G loss: -6.206962]\n",
      "216 [D loss: 8.220885] [G loss: -6.221967]\n",
      "217 [D loss: 8.235889] [G loss: -6.236970]\n",
      "218 [D loss: 8.250894] [G loss: -6.251975]\n",
      "219 [D loss: 8.265898] [G loss: -6.266977]\n",
      "220 [D loss: 8.280901] [G loss: -6.281982]\n",
      "221 [D loss: 8.295905] [G loss: -6.296986]\n",
      "222 [D loss: 8.310908] [G loss: -6.311990]\n",
      "223 [D loss: 8.325912] [G loss: -6.326994]\n",
      "224 [D loss: 8.340916] [G loss: -6.341998]\n",
      "225 [D loss: 8.355921] [G loss: -6.357001]\n",
      "226 [D loss: 8.370923] [G loss: -6.372005]\n",
      "227 [D loss: 8.385927] [G loss: -6.387009]\n",
      "228 [D loss: 8.400930] [G loss: -6.402012]\n",
      "229 [D loss: 8.415935] [G loss: -6.417015]\n",
      "230 [D loss: 8.430937] [G loss: -6.432018]\n",
      "231 [D loss: 8.445941] [G loss: -6.447021]\n",
      "232 [D loss: 8.460945] [G loss: -6.462025]\n",
      "233 [D loss: 8.475947] [G loss: -6.477028]\n",
      "234 [D loss: 8.490951] [G loss: -6.492031]\n",
      "235 [D loss: 8.505954] [G loss: -6.507034]\n",
      "236 [D loss: 8.520957] [G loss: -6.522038]\n",
      "237 [D loss: 8.535961] [G loss: -6.537041]\n",
      "238 [D loss: 8.550962] [G loss: -6.552043]\n",
      "239 [D loss: 8.565966] [G loss: -6.567047]\n",
      "240 [D loss: 8.580969] [G loss: -6.582050]\n",
      "241 [D loss: 8.595972] [G loss: -6.597052]\n",
      "242 [D loss: 8.610974] [G loss: -6.612055]\n",
      "243 [D loss: 8.625978] [G loss: -6.627058]\n",
      "244 [D loss: 8.640980] [G loss: -6.642061]\n",
      "245 [D loss: 8.655984] [G loss: -6.657064]\n",
      "246 [D loss: 8.670986] [G loss: -6.672067]\n",
      "247 [D loss: 8.685989] [G loss: -6.687069]\n",
      "248 [D loss: 8.700992] [G loss: -6.702072]\n",
      "249 [D loss: 8.715995] [G loss: -6.717075]\n",
      "250 [D loss: 8.730997] [G loss: -6.732078]\n",
      "251 [D loss: 8.746000] [G loss: -6.747081]\n",
      "252 [D loss: 8.761003] [G loss: -6.762083]\n",
      "253 [D loss: 8.776006] [G loss: -6.777087]\n",
      "254 [D loss: 8.791008] [G loss: -6.792089]\n",
      "255 [D loss: 8.806011] [G loss: -6.807092]\n",
      "256 [D loss: 8.821013] [G loss: -6.822094]\n",
      "257 [D loss: 8.836017] [G loss: -6.837098]\n",
      "258 [D loss: 8.851019] [G loss: -6.852099]\n",
      "259 [D loss: 8.866021] [G loss: -6.867103]\n",
      "260 [D loss: 8.881024] [G loss: -6.882105]\n",
      "261 [D loss: 8.896027] [G loss: -6.897108]\n",
      "262 [D loss: 8.911029] [G loss: -6.912110]\n",
      "263 [D loss: 8.926032] [G loss: -6.927113]\n",
      "264 [D loss: 8.941034] [G loss: -6.942115]\n",
      "265 [D loss: 8.956037] [G loss: -6.957118]\n",
      "266 [D loss: 8.971040] [G loss: -6.972120]\n",
      "267 [D loss: 8.986042] [G loss: -6.987123]\n",
      "268 [D loss: 9.001045] [G loss: -7.002126]\n",
      "269 [D loss: 9.016047] [G loss: -7.017128]\n",
      "270 [D loss: 9.031050] [G loss: -7.032131]\n",
      "271 [D loss: 9.046053] [G loss: -7.047133]\n",
      "272 [D loss: 9.061054] [G loss: -7.062136]\n",
      "273 [D loss: 9.076057] [G loss: -7.077139]\n",
      "274 [D loss: 9.091061] [G loss: -7.092141]\n",
      "275 [D loss: 9.106063] [G loss: -7.107143]\n",
      "276 [D loss: 9.121065] [G loss: -7.122146]\n",
      "277 [D loss: 9.136068] [G loss: -7.137149]\n",
      "278 [D loss: 9.151070] [G loss: -7.152151]\n",
      "279 [D loss: 9.166073] [G loss: -7.167154]\n",
      "280 [D loss: 9.181076] [G loss: -7.182157]\n",
      "281 [D loss: 9.196077] [G loss: -7.197159]\n",
      "282 [D loss: 9.211081] [G loss: -7.212161]\n",
      "283 [D loss: 9.226084] [G loss: -7.227164]\n",
      "284 [D loss: 9.241085] [G loss: -7.242166]\n",
      "285 [D loss: 9.256088] [G loss: -7.257169]\n",
      "286 [D loss: 9.271091] [G loss: -7.272171]\n",
      "287 [D loss: 9.286093] [G loss: -7.287174]\n",
      "288 [D loss: 9.301096] [G loss: -7.302176]\n",
      "289 [D loss: 9.316098] [G loss: -7.317179]\n",
      "290 [D loss: 9.331100] [G loss: -7.332181]\n",
      "291 [D loss: 9.346103] [G loss: -7.347184]\n",
      "292 [D loss: 9.361107] [G loss: -7.362185]\n",
      "293 [D loss: 9.376108] [G loss: -7.377189]\n",
      "294 [D loss: 9.391111] [G loss: -7.392191]\n",
      "295 [D loss: 9.406113] [G loss: -7.407194]\n",
      "296 [D loss: 9.421116] [G loss: -7.422196]\n",
      "297 [D loss: 9.436118] [G loss: -7.437199]\n",
      "298 [D loss: 9.451120] [G loss: -7.452201]\n",
      "299 [D loss: 9.466123] [G loss: -7.467204]\n",
      "300 [D loss: 9.481127] [G loss: -7.482205]\n",
      "301 [D loss: 9.496128] [G loss: -7.497209]\n",
      "302 [D loss: 9.511131] [G loss: -7.512212]\n",
      "303 [D loss: 9.526133] [G loss: -7.527214]\n",
      "304 [D loss: 9.541136] [G loss: -7.542216]\n",
      "305 [D loss: 9.556138] [G loss: -7.557219]\n",
      "306 [D loss: 9.571141] [G loss: -7.572221]\n",
      "307 [D loss: 9.586143] [G loss: -7.587224]\n",
      "308 [D loss: 9.601147] [G loss: -7.602226]\n",
      "309 [D loss: 9.616148] [G loss: -7.617229]\n",
      "310 [D loss: 9.631151] [G loss: -7.632232]\n",
      "311 [D loss: 9.646152] [G loss: -7.647234]\n",
      "312 [D loss: 9.661156] [G loss: -7.662237]\n",
      "313 [D loss: 9.676158] [G loss: -7.677239]\n",
      "314 [D loss: 9.691162] [G loss: -7.692243]\n",
      "315 [D loss: 9.706163] [G loss: -7.707244]\n",
      "316 [D loss: 9.721167] [G loss: -7.722247]\n",
      "317 [D loss: 9.736168] [G loss: -7.737249]\n",
      "318 [D loss: 9.751171] [G loss: -7.752253]\n",
      "319 [D loss: 9.766172] [G loss: -7.767254]\n",
      "320 [D loss: 9.781176] [G loss: -7.782257]\n",
      "321 [D loss: 9.796178] [G loss: -7.797259]\n",
      "322 [D loss: 9.811182] [G loss: -7.812263]\n",
      "323 [D loss: 9.826183] [G loss: -7.827264]\n",
      "324 [D loss: 9.841187] [G loss: -7.842267]\n",
      "325 [D loss: 9.856188] [G loss: -7.857269]\n",
      "326 [D loss: 9.871191] [G loss: -7.872272]\n",
      "327 [D loss: 9.886193] [G loss: -7.887274]\n",
      "328 [D loss: 9.901196] [G loss: -7.902277]\n",
      "329 [D loss: 9.916199] [G loss: -7.917279]\n",
      "330 [D loss: 9.931202] [G loss: -7.932282]\n",
      "331 [D loss: 9.946203] [G loss: -7.947284]\n",
      "332 [D loss: 9.961206] [G loss: -7.962287]\n",
      "333 [D loss: 9.976208] [G loss: -7.977289]\n",
      "334 [D loss: 9.991211] [G loss: -7.992292]\n",
      "335 [D loss: 10.006212] [G loss: -8.007294]\n",
      "336 [D loss: 10.021216] [G loss: -8.022297]\n",
      "337 [D loss: 10.036218] [G loss: -8.037299]\n",
      "338 [D loss: 10.051222] [G loss: -8.052302]\n",
      "339 [D loss: 10.066223] [G loss: -8.067304]\n",
      "340 [D loss: 10.081226] [G loss: -8.082307]\n",
      "341 [D loss: 10.096228] [G loss: -8.097309]\n",
      "342 [D loss: 10.111232] [G loss: -8.112312]\n",
      "343 [D loss: 10.126232] [G loss: -8.127314]\n",
      "344 [D loss: 10.141236] [G loss: -8.142317]\n",
      "345 [D loss: 10.156238] [G loss: -8.157319]\n",
      "346 [D loss: 10.171242] [G loss: -8.172322]\n",
      "347 [D loss: 10.186243] [G loss: -8.187324]\n",
      "348 [D loss: 10.201246] [G loss: -8.202327]\n",
      "349 [D loss: 10.216248] [G loss: -8.217329]\n",
      "350 [D loss: 10.231252] [G loss: -8.232332]\n",
      "351 [D loss: 10.246254] [G loss: -8.247334]\n",
      "352 [D loss: 10.261256] [G loss: -8.262337]\n",
      "353 [D loss: 10.276258] [G loss: -8.277339]\n",
      "354 [D loss: 10.291262] [G loss: -8.292342]\n",
      "355 [D loss: 10.306263] [G loss: -8.307343]\n",
      "356 [D loss: 10.321266] [G loss: -8.322347]\n",
      "357 [D loss: 10.336267] [G loss: -8.337349]\n",
      "358 [D loss: 10.351272] [G loss: -8.352352]\n",
      "359 [D loss: 10.366274] [G loss: -8.367353]\n",
      "360 [D loss: 10.381276] [G loss: -8.382357]\n",
      "361 [D loss: 10.396277] [G loss: -8.397359]\n",
      "362 [D loss: 10.411282] [G loss: -8.412362]\n",
      "363 [D loss: 10.426283] [G loss: -8.427363]\n",
      "364 [D loss: 10.441286] [G loss: -8.442367]\n",
      "365 [D loss: 10.456287] [G loss: -8.457369]\n",
      "366 [D loss: 10.471292] [G loss: -8.472372]\n",
      "367 [D loss: 10.486294] [G loss: -8.487373]\n",
      "368 [D loss: 10.501296] [G loss: -8.502377]\n",
      "369 [D loss: 10.516297] [G loss: -8.517379]\n",
      "370 [D loss: 10.531301] [G loss: -8.532382]\n",
      "371 [D loss: 10.546303] [G loss: -8.547383]\n",
      "372 [D loss: 10.561306] [G loss: -8.562387]\n",
      "373 [D loss: 10.576307] [G loss: -8.577389]\n",
      "374 [D loss: 10.591311] [G loss: -8.592392]\n",
      "375 [D loss: 10.606314] [G loss: -8.607393]\n",
      "376 [D loss: 10.621316] [G loss: -8.622396]\n",
      "377 [D loss: 10.636317] [G loss: -8.637399]\n",
      "378 [D loss: 10.651321] [G loss: -8.652402]\n",
      "379 [D loss: 10.666323] [G loss: -8.667403]\n",
      "380 [D loss: 10.681326] [G loss: -8.682406]\n",
      "381 [D loss: 10.696327] [G loss: -8.697409]\n",
      "382 [D loss: 10.711331] [G loss: -8.712412]\n",
      "383 [D loss: 10.726334] [G loss: -8.727413]\n",
      "384 [D loss: 10.741336] [G loss: -8.742416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 [D loss: 10.756337] [G loss: -8.757419]\n",
      "386 [D loss: 10.771341] [G loss: -8.772422]\n",
      "387 [D loss: 10.786343] [G loss: -8.787423]\n",
      "388 [D loss: 10.801346] [G loss: -8.802426]\n",
      "389 [D loss: 10.816347] [G loss: -8.817429]\n",
      "390 [D loss: 10.831350] [G loss: -8.832432]\n",
      "391 [D loss: 10.846354] [G loss: -8.847433]\n",
      "392 [D loss: 10.861356] [G loss: -8.862436]\n",
      "393 [D loss: 10.876357] [G loss: -8.877439]\n",
      "394 [D loss: 10.891361] [G loss: -8.892442]\n",
      "395 [D loss: 10.906363] [G loss: -8.907443]\n",
      "396 [D loss: 10.921366] [G loss: -8.922446]\n",
      "397 [D loss: 10.936367] [G loss: -8.937449]\n",
      "398 [D loss: 10.951370] [G loss: -8.952452]\n",
      "399 [D loss: 10.966373] [G loss: -8.967453]\n",
      "400 [D loss: 10.981376] [G loss: -8.982456]\n",
      "401 [D loss: 10.996377] [G loss: -8.997458]\n",
      "402 [D loss: 11.011381] [G loss: -9.012462]\n",
      "403 [D loss: 11.026382] [G loss: -9.027463]\n",
      "404 [D loss: 11.041386] [G loss: -9.042466]\n",
      "405 [D loss: 11.056387] [G loss: -9.057468]\n",
      "406 [D loss: 11.071390] [G loss: -9.072472]\n",
      "407 [D loss: 11.086393] [G loss: -9.087473]\n",
      "408 [D loss: 11.101396] [G loss: -9.102476]\n",
      "409 [D loss: 11.116397] [G loss: -9.117478]\n",
      "410 [D loss: 11.131401] [G loss: -9.132482]\n",
      "411 [D loss: 11.146402] [G loss: -9.147483]\n",
      "412 [D loss: 11.161404] [G loss: -9.162485]\n",
      "413 [D loss: 11.176405] [G loss: -9.177485]\n",
      "414 [D loss: 11.191406] [G loss: -9.192488]\n",
      "415 [D loss: 11.206408] [G loss: -9.207488]\n",
      "416 [D loss: 11.221409] [G loss: -9.222490]\n",
      "417 [D loss: 11.236410] [G loss: -9.237491]\n",
      "418 [D loss: 11.251412] [G loss: -9.252493]\n",
      "419 [D loss: 11.266413] [G loss: -9.267493]\n",
      "420 [D loss: 11.281414] [G loss: -9.282495]\n",
      "421 [D loss: 11.296415] [G loss: -9.297496]\n",
      "422 [D loss: 11.311417] [G loss: -9.312498]\n",
      "423 [D loss: 11.326418] [G loss: -9.327498]\n",
      "424 [D loss: 11.341419] [G loss: -9.342501]\n",
      "425 [D loss: 11.356421] [G loss: -9.357501]\n",
      "426 [D loss: 11.371423] [G loss: -9.372503]\n",
      "427 [D loss: 11.386423] [G loss: -9.387504]\n",
      "428 [D loss: 11.401424] [G loss: -9.402506]\n",
      "429 [D loss: 11.416426] [G loss: -9.417506]\n",
      "430 [D loss: 11.431427] [G loss: -9.432508]\n",
      "431 [D loss: 11.446428] [G loss: -9.447509]\n",
      "432 [D loss: 11.461430] [G loss: -9.462511]\n",
      "433 [D loss: 11.476431] [G loss: -9.477511]\n",
      "434 [D loss: 11.491433] [G loss: -9.492514]\n",
      "435 [D loss: 11.506433] [G loss: -9.507514]\n",
      "436 [D loss: 11.521435] [G loss: -9.522516]\n",
      "437 [D loss: 11.536436] [G loss: -9.537517]\n",
      "438 [D loss: 11.551437] [G loss: -9.552519]\n",
      "439 [D loss: 11.566439] [G loss: -9.567519]\n",
      "440 [D loss: 11.581440] [G loss: -9.582521]\n",
      "441 [D loss: 11.596441] [G loss: -9.597522]\n",
      "442 [D loss: 11.611444] [G loss: -9.612524]\n",
      "443 [D loss: 11.626444] [G loss: -9.627524]\n",
      "444 [D loss: 11.641445] [G loss: -9.642527]\n",
      "445 [D loss: 11.656446] [G loss: -9.657527]\n",
      "446 [D loss: 11.671448] [G loss: -9.672529]\n",
      "447 [D loss: 11.686449] [G loss: -9.687530]\n",
      "448 [D loss: 11.701450] [G loss: -9.702532]\n",
      "449 [D loss: 11.716452] [G loss: -9.717532]\n",
      "450 [D loss: 11.731454] [G loss: -9.732534]\n",
      "451 [D loss: 11.746454] [G loss: -9.747534]\n",
      "452 [D loss: 11.761456] [G loss: -9.762537]\n",
      "453 [D loss: 11.776457] [G loss: -9.777537]\n",
      "454 [D loss: 11.791458] [G loss: -9.792540]\n",
      "455 [D loss: 11.806459] [G loss: -9.807539]\n",
      "456 [D loss: 11.821461] [G loss: -9.822542]\n",
      "457 [D loss: 11.836462] [G loss: -9.837543]\n",
      "458 [D loss: 11.851464] [G loss: -9.852545]\n",
      "459 [D loss: 11.866465] [G loss: -9.867544]\n",
      "460 [D loss: 11.881466] [G loss: -9.882547]\n",
      "461 [D loss: 11.896467] [G loss: -9.897548]\n",
      "462 [D loss: 11.911469] [G loss: -9.912550]\n",
      "463 [D loss: 11.926470] [G loss: -9.927549]\n",
      "464 [D loss: 11.941469] [G loss: -9.942551]\n",
      "465 [D loss: 11.956470] [G loss: -9.957551]\n",
      "466 [D loss: 11.971470] [G loss: -9.972551]\n",
      "467 [D loss: 11.986470] [G loss: -9.987550]\n",
      "468 [D loss: 12.001471] [G loss: -10.002551]\n",
      "469 [D loss: 12.016470] [G loss: -10.017550]\n",
      "470 [D loss: 12.031471] [G loss: -10.032552]\n",
      "471 [D loss: 12.046471] [G loss: -10.047550]\n",
      "472 [D loss: 12.061470] [G loss: -10.062551]\n",
      "473 [D loss: 12.076469] [G loss: -10.077552]\n",
      "474 [D loss: 12.091471] [G loss: -10.092552]\n",
      "475 [D loss: 12.106471] [G loss: -10.107551]\n",
      "476 [D loss: 12.121471] [G loss: -10.122552]\n",
      "477 [D loss: 12.136471] [G loss: -10.137551]\n",
      "478 [D loss: 12.151472] [G loss: -10.152553]\n",
      "479 [D loss: 12.166471] [G loss: -10.167551]\n",
      "480 [D loss: 12.181471] [G loss: -10.182552]\n",
      "481 [D loss: 12.196470] [G loss: -10.197553]\n",
      "482 [D loss: 12.211472] [G loss: -10.212553]\n",
      "483 [D loss: 12.226472] [G loss: -10.227551]\n",
      "484 [D loss: 12.241472] [G loss: -10.242553]\n",
      "485 [D loss: 12.256472] [G loss: -10.257552]\n",
      "486 [D loss: 12.271473] [G loss: -10.272553]\n",
      "487 [D loss: 12.286472] [G loss: -10.287552]\n",
      "488 [D loss: 12.301472] [G loss: -10.302553]\n",
      "489 [D loss: 12.316471] [G loss: -10.317554]\n",
      "490 [D loss: 12.331472] [G loss: -10.332554]\n",
      "491 [D loss: 12.346473] [G loss: -10.347552]\n",
      "492 [D loss: 12.361473] [G loss: -10.362554]\n",
      "493 [D loss: 12.376472] [G loss: -10.377553]\n",
      "494 [D loss: 12.391474] [G loss: -10.392554]\n",
      "495 [D loss: 12.406473] [G loss: -10.407553]\n",
      "496 [D loss: 12.421473] [G loss: -10.422554]\n",
      "497 [D loss: 12.436472] [G loss: -10.437554]\n",
      "498 [D loss: 12.451473] [G loss: -10.452555]\n",
      "499 [D loss: 12.466474] [G loss: -10.467553]\n",
      "500 [D loss: 12.481474] [G loss: -10.482554]\n",
      "501 [D loss: 12.496473] [G loss: -10.497554]\n",
      "502 [D loss: 12.511475] [G loss: -10.512555]\n",
      "503 [D loss: 12.526473] [G loss: -10.527553]\n",
      "504 [D loss: 12.541471] [G loss: -10.542553]\n",
      "505 [D loss: 12.556471] [G loss: -10.557551]\n",
      "506 [D loss: 12.571470] [G loss: -10.572551]\n",
      "507 [D loss: 12.586469] [G loss: -10.587548]\n",
      "508 [D loss: 12.601467] [G loss: -10.602549]\n",
      "509 [D loss: 12.616467] [G loss: -10.617547]\n",
      "510 [D loss: 12.631466] [G loss: -10.632546]\n",
      "511 [D loss: 12.646464] [G loss: -10.647544]\n",
      "512 [D loss: 12.661463] [G loss: -10.662544]\n",
      "513 [D loss: 12.676462] [G loss: -10.677543]\n",
      "514 [D loss: 12.691462] [G loss: -10.692543]\n",
      "515 [D loss: 12.706460] [G loss: -10.707539]\n",
      "516 [D loss: 12.721458] [G loss: -10.722540]\n",
      "517 [D loss: 12.736458] [G loss: -10.737538]\n",
      "518 [D loss: 12.751457] [G loss: -10.752539]\n",
      "519 [D loss: 12.766455] [G loss: -10.767534]\n",
      "520 [D loss: 12.781454] [G loss: -10.782536]\n",
      "521 [D loss: 12.796453] [G loss: -10.797534]\n",
      "522 [D loss: 12.811453] [G loss: -10.812534]\n",
      "523 [D loss: 12.826451] [G loss: -10.827530]\n",
      "524 [D loss: 12.841450] [G loss: -10.842531]\n",
      "525 [D loss: 12.856449] [G loss: -10.857530]\n",
      "526 [D loss: 12.871449] [G loss: -10.872530]\n",
      "527 [D loss: 12.886446] [G loss: -10.887526]\n",
      "528 [D loss: 12.901445] [G loss: -10.902527]\n",
      "529 [D loss: 12.916445] [G loss: -10.917525]\n",
      "530 [D loss: 12.931444] [G loss: -10.932526]\n",
      "531 [D loss: 12.946443] [G loss: -10.947521]\n",
      "532 [D loss: 12.961441] [G loss: -10.962523]\n",
      "533 [D loss: 12.976440] [G loss: -10.977521]\n",
      "534 [D loss: 12.991440] [G loss: -10.992521]\n",
      "535 [D loss: 13.006437] [G loss: -11.007517]\n",
      "536 [D loss: 13.021437] [G loss: -11.022518]\n",
      "537 [D loss: 13.036436] [G loss: -11.037517]\n",
      "538 [D loss: 13.051435] [G loss: -11.052517]\n",
      "539 [D loss: 13.066434] [G loss: -11.067513]\n",
      "540 [D loss: 13.081432] [G loss: -11.082514]\n",
      "541 [D loss: 13.096432] [G loss: -11.097512]\n",
      "542 [D loss: 13.111431] [G loss: -11.112513]\n",
      "543 [D loss: 13.126429] [G loss: -11.127508]\n",
      "544 [D loss: 13.141428] [G loss: -11.142509]\n",
      "545 [D loss: 13.156425] [G loss: -11.157506]\n",
      "546 [D loss: 13.171424] [G loss: -11.172505]\n",
      "547 [D loss: 13.186421] [G loss: -11.187500]\n",
      "548 [D loss: 13.201419] [G loss: -11.202500]\n",
      "549 [D loss: 13.216415] [G loss: -11.217497]\n",
      "550 [D loss: 13.231415] [G loss: -11.232496]\n",
      "551 [D loss: 13.246412] [G loss: -11.247492]\n",
      "552 [D loss: 13.261410] [G loss: -11.262491]\n",
      "553 [D loss: 13.276407] [G loss: -11.277488]\n",
      "554 [D loss: 13.291406] [G loss: -11.292487]\n",
      "555 [D loss: 13.306403] [G loss: -11.307482]\n",
      "556 [D loss: 13.321401] [G loss: -11.322482]\n",
      "557 [D loss: 13.336397] [G loss: -11.337479]\n",
      "558 [D loss: 13.351397] [G loss: -11.352478]\n",
      "559 [D loss: 13.366394] [G loss: -11.367474]\n",
      "560 [D loss: 13.381392] [G loss: -11.382473]\n",
      "561 [D loss: 13.396389] [G loss: -11.397470]\n",
      "562 [D loss: 13.411387] [G loss: -11.412469]\n",
      "563 [D loss: 13.426385] [G loss: -11.427464]\n",
      "564 [D loss: 13.441382] [G loss: -11.442464]\n",
      "565 [D loss: 13.456379] [G loss: -11.457460]\n",
      "566 [D loss: 13.471378] [G loss: -11.472460]\n",
      "567 [D loss: 13.486376] [G loss: -11.487455]\n",
      "568 [D loss: 13.501373] [G loss: -11.502455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 [D loss: 13.516371] [G loss: -11.517451]\n",
      "570 [D loss: 13.531369] [G loss: -11.532451]\n",
      "571 [D loss: 13.546367] [G loss: -11.547445]\n",
      "572 [D loss: 13.561364] [G loss: -11.562446]\n",
      "573 [D loss: 13.576361] [G loss: -11.577442]\n",
      "574 [D loss: 13.591360] [G loss: -11.592442]\n",
      "575 [D loss: 13.606358] [G loss: -11.607437]\n",
      "576 [D loss: 13.621355] [G loss: -11.622437]\n",
      "577 [D loss: 13.636353] [G loss: -11.637433]\n",
      "578 [D loss: 13.651351] [G loss: -11.652432]\n",
      "579 [D loss: 13.666348] [G loss: -11.667427]\n",
      "580 [D loss: 13.681346] [G loss: -11.682427]\n",
      "581 [D loss: 13.696343] [G loss: -11.697424]\n",
      "582 [D loss: 13.711342] [G loss: -11.712423]\n",
      "583 [D loss: 13.726339] [G loss: -11.727419]\n",
      "584 [D loss: 13.741337] [G loss: -11.742418]\n",
      "585 [D loss: 13.756334] [G loss: -11.757415]\n",
      "586 [D loss: 13.771333] [G loss: -11.772414]\n",
      "587 [D loss: 13.786330] [G loss: -11.787409]\n",
      "588 [D loss: 13.801328] [G loss: -11.802409]\n",
      "589 [D loss: 13.816325] [G loss: -11.817406]\n",
      "590 [D loss: 13.831324] [G loss: -11.832405]\n",
      "591 [D loss: 13.846321] [G loss: -11.847401]\n",
      "592 [D loss: 13.861319] [G loss: -11.862400]\n",
      "593 [D loss: 13.876316] [G loss: -11.877397]\n",
      "594 [D loss: 13.891315] [G loss: -11.892396]\n",
      "595 [D loss: 13.906312] [G loss: -11.907391]\n",
      "596 [D loss: 13.921309] [G loss: -11.922391]\n",
      "597 [D loss: 13.936307] [G loss: -11.937387]\n",
      "598 [D loss: 13.951305] [G loss: -11.952387]\n",
      "599 [D loss: 13.966303] [G loss: -11.967382]\n",
      "600 [D loss: 13.981300] [G loss: -11.982382]\n",
      "601 [D loss: 13.996298] [G loss: -11.997378]\n",
      "602 [D loss: 14.011296] [G loss: -12.012378]\n",
      "603 [D loss: 14.026294] [G loss: -12.027372]\n",
      "604 [D loss: 14.041291] [G loss: -12.042373]\n",
      "605 [D loss: 14.056289] [G loss: -12.057369]\n",
      "606 [D loss: 14.071287] [G loss: -12.072369]\n",
      "607 [D loss: 14.086285] [G loss: -12.087364]\n",
      "608 [D loss: 14.101280] [G loss: -12.102362]\n",
      "609 [D loss: 14.116277] [G loss: -12.117356]\n",
      "610 [D loss: 14.131271] [G loss: -12.132354]\n",
      "611 [D loss: 14.146267] [G loss: -12.147346]\n",
      "612 [D loss: 14.161263] [G loss: -12.162343]\n",
      "613 [D loss: 14.176258] [G loss: -12.177338]\n",
      "614 [D loss: 14.191254] [G loss: -12.192334]\n",
      "615 [D loss: 14.206249] [G loss: -12.207329]\n",
      "616 [D loss: 14.221244] [G loss: -12.222325]\n",
      "617 [D loss: 14.236239] [G loss: -12.237319]\n",
      "618 [D loss: 14.251234] [G loss: -12.252316]\n",
      "619 [D loss: 14.266230] [G loss: -12.267309]\n",
      "620 [D loss: 14.281225] [G loss: -12.282306]\n",
      "621 [D loss: 14.296221] [G loss: -12.297300]\n",
      "622 [D loss: 14.311216] [G loss: -12.312297]\n",
      "623 [D loss: 14.326212] [G loss: -12.327291]\n",
      "624 [D loss: 14.341207] [G loss: -12.342288]\n",
      "625 [D loss: 14.356202] [G loss: -12.357282]\n",
      "626 [D loss: 14.371197] [G loss: -12.372279]\n",
      "627 [D loss: 14.386192] [G loss: -12.387272]\n",
      "628 [D loss: 14.401188] [G loss: -12.402268]\n",
      "629 [D loss: 14.416183] [G loss: -12.417263]\n",
      "630 [D loss: 14.431179] [G loss: -12.432260]\n",
      "631 [D loss: 14.446175] [G loss: -12.447254]\n",
      "632 [D loss: 14.461169] [G loss: -12.462251]\n",
      "633 [D loss: 14.476165] [G loss: -12.477244]\n",
      "634 [D loss: 14.491159] [G loss: -12.492242]\n",
      "635 [D loss: 14.506155] [G loss: -12.507235]\n",
      "636 [D loss: 14.521151] [G loss: -12.522231]\n",
      "637 [D loss: 14.536146] [G loss: -12.537226]\n",
      "638 [D loss: 14.551142] [G loss: -12.552222]\n",
      "639 [D loss: 14.566137] [G loss: -12.567217]\n",
      "640 [D loss: 14.581132] [G loss: -12.582213]\n",
      "641 [D loss: 14.596128] [G loss: -12.597207]\n",
      "642 [D loss: 14.611122] [G loss: -12.612205]\n",
      "643 [D loss: 14.626118] [G loss: -12.627197]\n",
      "644 [D loss: 14.641113] [G loss: -12.642194]\n",
      "645 [D loss: 14.656109] [G loss: -12.657188]\n",
      "646 [D loss: 14.671104] [G loss: -12.672185]\n",
      "647 [D loss: 14.686100] [G loss: -12.687180]\n",
      "648 [D loss: 14.701095] [G loss: -12.702176]\n",
      "649 [D loss: 14.716090] [G loss: -12.717170]\n",
      "650 [D loss: 14.731085] [G loss: -12.732167]\n",
      "651 [D loss: 14.746080] [G loss: -12.747160]\n",
      "652 [D loss: 14.761076] [G loss: -12.762156]\n",
      "653 [D loss: 14.776072] [G loss: -12.777151]\n",
      "654 [D loss: 14.791067] [G loss: -12.792148]\n",
      "655 [D loss: 14.806063] [G loss: -12.807142]\n",
      "656 [D loss: 14.821057] [G loss: -12.822139]\n",
      "657 [D loss: 14.836052] [G loss: -12.837132]\n",
      "658 [D loss: 14.851048] [G loss: -12.852130]\n",
      "659 [D loss: 14.866043] [G loss: -12.867123]\n",
      "660 [D loss: 14.881039] [G loss: -12.882119]\n",
      "661 [D loss: 14.896034] [G loss: -12.897114]\n",
      "662 [D loss: 14.911030] [G loss: -12.912110]\n",
      "663 [D loss: 14.926025] [G loss: -12.927105]\n",
      "664 [D loss: 14.941020] [G loss: -12.942101]\n",
      "665 [D loss: 14.956016] [G loss: -12.957095]\n",
      "666 [D loss: 14.971010] [G loss: -12.972093]\n",
      "667 [D loss: 14.986006] [G loss: -12.987085]\n",
      "668 [D loss: 15.001001] [G loss: -13.002082]\n",
      "669 [D loss: 15.015997] [G loss: -13.017076]\n",
      "670 [D loss: 15.030993] [G loss: -13.032073]\n",
      "671 [D loss: 15.045988] [G loss: -13.047068]\n",
      "672 [D loss: 15.060983] [G loss: -13.062064]\n",
      "673 [D loss: 15.075978] [G loss: -13.077058]\n",
      "674 [D loss: 15.090973] [G loss: -13.092055]\n",
      "675 [D loss: 15.105968] [G loss: -13.107048]\n",
      "676 [D loss: 15.120964] [G loss: -13.122045]\n",
      "677 [D loss: 15.135960] [G loss: -13.137039]\n",
      "678 [D loss: 15.150955] [G loss: -13.152036]\n",
      "679 [D loss: 15.165951] [G loss: -13.167030]\n",
      "680 [D loss: 15.180945] [G loss: -13.182027]\n",
      "681 [D loss: 15.195941] [G loss: -13.197021]\n",
      "682 [D loss: 15.210936] [G loss: -13.212018]\n",
      "683 [D loss: 15.225931] [G loss: -13.227011]\n",
      "684 [D loss: 15.240927] [G loss: -13.242007]\n",
      "685 [D loss: 15.255922] [G loss: -13.257002]\n",
      "686 [D loss: 15.270918] [G loss: -13.271998]\n",
      "687 [D loss: 15.285913] [G loss: -13.286993]\n",
      "688 [D loss: 15.300908] [G loss: -13.301990]\n",
      "689 [D loss: 15.315904] [G loss: -13.316983]\n",
      "690 [D loss: 15.330898] [G loss: -13.331981]\n",
      "691 [D loss: 15.345894] [G loss: -13.346973]\n",
      "692 [D loss: 15.360889] [G loss: -13.361970]\n",
      "693 [D loss: 15.375885] [G loss: -13.376965]\n",
      "694 [D loss: 15.390881] [G loss: -13.391961]\n",
      "695 [D loss: 15.405876] [G loss: -13.406956]\n",
      "696 [D loss: 15.420871] [G loss: -13.421952]\n",
      "697 [D loss: 15.435866] [G loss: -13.436946]\n",
      "698 [D loss: 15.450861] [G loss: -13.451943]\n",
      "699 [D loss: 15.465857] [G loss: -13.466936]\n",
      "700 [D loss: 15.480852] [G loss: -13.481933]\n",
      "701 [D loss: 15.495848] [G loss: -13.496927]\n",
      "702 [D loss: 15.510843] [G loss: -13.511924]\n",
      "703 [D loss: 15.525839] [G loss: -13.526918]\n",
      "704 [D loss: 15.540833] [G loss: -13.541915]\n",
      "705 [D loss: 15.555829] [G loss: -13.556909]\n",
      "706 [D loss: 15.570824] [G loss: -13.571906]\n",
      "707 [D loss: 15.585819] [G loss: -13.586899]\n",
      "708 [D loss: 15.600815] [G loss: -13.601895]\n",
      "709 [D loss: 15.615810] [G loss: -13.616890]\n",
      "710 [D loss: 15.630806] [G loss: -13.631886]\n",
      "711 [D loss: 15.645802] [G loss: -13.646881]\n",
      "712 [D loss: 15.660796] [G loss: -13.661878]\n",
      "713 [D loss: 15.675792] [G loss: -13.676871]\n",
      "714 [D loss: 15.690786] [G loss: -13.691869]\n",
      "715 [D loss: 15.705782] [G loss: -13.706861]\n",
      "716 [D loss: 15.720778] [G loss: -13.721858]\n",
      "717 [D loss: 15.735773] [G loss: -13.736853]\n",
      "718 [D loss: 15.750769] [G loss: -13.751849]\n",
      "719 [D loss: 15.765764] [G loss: -13.766844]\n",
      "720 [D loss: 15.780759] [G loss: -13.781840]\n",
      "721 [D loss: 15.795754] [G loss: -13.796834]\n",
      "722 [D loss: 15.810749] [G loss: -13.811831]\n",
      "723 [D loss: 15.825745] [G loss: -13.826824]\n",
      "724 [D loss: 15.840740] [G loss: -13.841821]\n",
      "725 [D loss: 15.855736] [G loss: -13.856815]\n",
      "726 [D loss: 15.870731] [G loss: -13.871812]\n",
      "727 [D loss: 15.885727] [G loss: -13.886806]\n",
      "728 [D loss: 15.900722] [G loss: -13.901803]\n",
      "729 [D loss: 15.915717] [G loss: -13.916797]\n",
      "730 [D loss: 15.930712] [G loss: -13.931794]\n",
      "731 [D loss: 15.945707] [G loss: -13.946787]\n",
      "732 [D loss: 15.960703] [G loss: -13.961783]\n",
      "733 [D loss: 15.975698] [G loss: -13.976778]\n",
      "734 [D loss: 15.990694] [G loss: -13.991775]\n",
      "735 [D loss: 16.005690] [G loss: -14.006769]\n",
      "736 [D loss: 16.020684] [G loss: -14.021766]\n",
      "737 [D loss: 16.035680] [G loss: -14.036759]\n",
      "738 [D loss: 16.050674] [G loss: -14.051757]\n",
      "739 [D loss: 16.065670] [G loss: -14.066750]\n",
      "740 [D loss: 16.080666] [G loss: -14.081746]\n",
      "741 [D loss: 16.095661] [G loss: -14.096741]\n",
      "742 [D loss: 16.110657] [G loss: -14.111737]\n",
      "743 [D loss: 16.125650] [G loss: -14.126730]\n",
      "744 [D loss: 16.140644] [G loss: -14.141725]\n",
      "745 [D loss: 16.155640] [G loss: -14.156719]\n",
      "746 [D loss: 16.170633] [G loss: -14.171712]\n",
      "747 [D loss: 16.185627] [G loss: -14.186707]\n",
      "748 [D loss: 16.200621] [G loss: -14.201701]\n",
      "749 [D loss: 16.215614] [G loss: -14.216696]\n",
      "750 [D loss: 16.230610] [G loss: -14.231688]\n",
      "751 [D loss: 16.245604] [G loss: -14.246683]\n",
      "752 [D loss: 16.260597] [G loss: -14.261678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753 [D loss: 16.275593] [G loss: -14.276672]\n",
      "754 [D loss: 16.290586] [G loss: -14.291665]\n",
      "755 [D loss: 16.305580] [G loss: -14.306660]\n",
      "756 [D loss: 16.320574] [G loss: -14.321654]\n",
      "757 [D loss: 16.335567] [G loss: -14.336649]\n",
      "758 [D loss: 16.350563] [G loss: -14.351642]\n",
      "759 [D loss: 16.365557] [G loss: -14.366636]\n",
      "760 [D loss: 16.380550] [G loss: -14.381631]\n",
      "761 [D loss: 16.395546] [G loss: -14.396626]\n",
      "762 [D loss: 16.410540] [G loss: -14.411618]\n",
      "763 [D loss: 16.425533] [G loss: -14.426613]\n",
      "764 [D loss: 16.440527] [G loss: -14.441607]\n",
      "765 [D loss: 16.455521] [G loss: -14.456602]\n",
      "766 [D loss: 16.470516] [G loss: -14.471595]\n",
      "767 [D loss: 16.485510] [G loss: -14.486589]\n",
      "768 [D loss: 16.500504] [G loss: -14.501584]\n",
      "769 [D loss: 16.515499] [G loss: -14.516579]\n",
      "770 [D loss: 16.530493] [G loss: -14.531571]\n",
      "771 [D loss: 16.545486] [G loss: -14.546566]\n",
      "772 [D loss: 16.560480] [G loss: -14.561561]\n",
      "773 [D loss: 16.575474] [G loss: -14.576555]\n",
      "774 [D loss: 16.590469] [G loss: -14.591548]\n",
      "775 [D loss: 16.605463] [G loss: -14.606543]\n",
      "776 [D loss: 16.620457] [G loss: -14.621537]\n",
      "777 [D loss: 16.635452] [G loss: -14.636532]\n",
      "778 [D loss: 16.650446] [G loss: -14.651525]\n",
      "779 [D loss: 16.665440] [G loss: -14.666519]\n",
      "780 [D loss: 16.680433] [G loss: -14.681514]\n",
      "781 [D loss: 16.695427] [G loss: -14.696508]\n",
      "782 [D loss: 16.710423] [G loss: -14.711501]\n",
      "783 [D loss: 16.725416] [G loss: -14.726496]\n",
      "784 [D loss: 16.740410] [G loss: -14.741490]\n",
      "785 [D loss: 16.755405] [G loss: -14.756485]\n",
      "786 [D loss: 16.770399] [G loss: -14.771478]\n",
      "787 [D loss: 16.785393] [G loss: -14.786472]\n",
      "788 [D loss: 16.800386] [G loss: -14.801467]\n",
      "789 [D loss: 16.815380] [G loss: -14.816462]\n",
      "790 [D loss: 16.830376] [G loss: -14.831454]\n",
      "791 [D loss: 16.845369] [G loss: -14.846449]\n",
      "792 [D loss: 16.860363] [G loss: -14.861444]\n",
      "793 [D loss: 16.875359] [G loss: -14.876438]\n",
      "794 [D loss: 16.890352] [G loss: -14.891431]\n",
      "795 [D loss: 16.905346] [G loss: -14.906425]\n",
      "796 [D loss: 16.920340] [G loss: -14.921420]\n",
      "797 [D loss: 16.935333] [G loss: -14.936415]\n",
      "798 [D loss: 16.950329] [G loss: -14.951407]\n",
      "799 [D loss: 16.965322] [G loss: -14.966402]\n",
      "800 [D loss: 16.980316] [G loss: -14.981397]\n",
      "801 [D loss: 16.995312] [G loss: -14.996391]\n",
      "802 [D loss: 17.010305] [G loss: -15.011383]\n",
      "803 [D loss: 17.025299] [G loss: -15.026379]\n",
      "804 [D loss: 17.040295] [G loss: -15.041372]\n",
      "805 [D loss: 17.055286] [G loss: -15.056368]\n",
      "806 [D loss: 17.070282] [G loss: -15.071360]\n",
      "807 [D loss: 17.085278] [G loss: -15.086355]\n",
      "808 [D loss: 17.100269] [G loss: -15.101349]\n",
      "809 [D loss: 17.115265] [G loss: -15.116344]\n",
      "810 [D loss: 17.130259] [G loss: -15.131336]\n",
      "811 [D loss: 17.145252] [G loss: -15.146332]\n",
      "812 [D loss: 17.160248] [G loss: -15.161325]\n",
      "813 [D loss: 17.175240] [G loss: -15.176321]\n",
      "814 [D loss: 17.190235] [G loss: -15.191313]\n",
      "815 [D loss: 17.205231] [G loss: -15.206308]\n",
      "816 [D loss: 17.220222] [G loss: -15.221302]\n",
      "817 [D loss: 17.235218] [G loss: -15.236298]\n",
      "818 [D loss: 17.250212] [G loss: -15.251289]\n",
      "819 [D loss: 17.265205] [G loss: -15.266285]\n",
      "820 [D loss: 17.280201] [G loss: -15.281279]\n",
      "821 [D loss: 17.295193] [G loss: -15.296274]\n",
      "822 [D loss: 17.310188] [G loss: -15.311266]\n",
      "823 [D loss: 17.325184] [G loss: -15.326262]\n",
      "824 [D loss: 17.340176] [G loss: -15.341255]\n",
      "825 [D loss: 17.355171] [G loss: -15.356251]\n",
      "826 [D loss: 17.370165] [G loss: -15.371243]\n",
      "827 [D loss: 17.385159] [G loss: -15.386238]\n",
      "828 [D loss: 17.400152] [G loss: -15.401232]\n",
      "829 [D loss: 17.415146] [G loss: -15.416225]\n",
      "830 [D loss: 17.430138] [G loss: -15.431217]\n",
      "831 [D loss: 17.445131] [G loss: -15.446211]\n",
      "832 [D loss: 17.460125] [G loss: -15.461203]\n",
      "833 [D loss: 17.475117] [G loss: -15.476196]\n",
      "834 [D loss: 17.490110] [G loss: -15.491188]\n",
      "835 [D loss: 17.505104] [G loss: -15.506182]\n",
      "836 [D loss: 17.520096] [G loss: -15.521175]\n",
      "837 [D loss: 17.535088] [G loss: -15.536169]\n",
      "838 [D loss: 17.550083] [G loss: -15.551161]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir)\n",
    "gan, generator, critic = wgan.build_models(generator_lr, critic_lr)\n",
    "\n",
    "# gan.summary()\n",
    "# generator.summary()\n",
    "# critic.summary()\n",
    "        \n",
    "losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
