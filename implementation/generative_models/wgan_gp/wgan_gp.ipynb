{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import utils\n",
    "from keras.callbacks import *\n",
    "from wgan_gp import WGAN_GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_transactions_filepath = \"../../datasets/berka_dataset/usable/normalized_transactions_months.npy\"\n",
    "# timesteps = 90\n",
    "# dataset = utils.load_splitted_dataset(normalized_transactions_filepath, timesteps)\n",
    "\n",
    "timesteps = 100\n",
    "dataset = utils.load_resized_mnist(10)\n",
    "\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 5000000\n",
    "n_critic = 5\n",
    "n_generator = 1\n",
    "latent_dim = 2\n",
    "generator_lr = 0.0001\n",
    "critic_lr = 0.0001\n",
    "img_frequency = 250\n",
    "loss_frequency = 250\n",
    "latent_space_frequency = 500\n",
    "model_save_frequency = 25000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000\n",
    "gradient_penality_weight = 10\n",
    "\n",
    "root_path = Path('wgan-gp')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [C loss: -2.841575] [G loss: 1.132792]\n",
      "2 [C loss: -2.719305] [G loss: 1.162696]\n",
      "3 [C loss: -2.547925] [G loss: 1.219562]\n",
      "4 [C loss: -2.388430] [G loss: 1.233067]\n",
      "5 [C loss: -2.180931] [G loss: 1.330897]\n",
      "6 [C loss: -1.965160] [G loss: 1.391207]\n",
      "7 [C loss: -1.746520] [G loss: 1.493033]\n",
      "8 [C loss: -1.503692] [G loss: 1.596240]\n",
      "9 [C loss: -1.269786] [G loss: 1.705324]\n",
      "10 [C loss: -1.029263] [G loss: 1.774870]\n",
      "11 [C loss: -0.773249] [G loss: 1.906131]\n",
      "12 [C loss: -0.537293] [G loss: 2.083890]\n",
      "13 [C loss: -0.328291] [G loss: 2.225612]\n",
      "14 [C loss: -0.076830] [G loss: 2.409142]\n",
      "15 [C loss: 0.128587] [G loss: 2.590662]\n",
      "16 [C loss: 0.321149] [G loss: 2.763374]\n",
      "17 [C loss: 0.511777] [G loss: 2.957081]\n",
      "18 [C loss: 0.665391] [G loss: 3.109946]\n",
      "19 [C loss: 0.815340] [G loss: 3.137566]\n",
      "20 [C loss: 0.938381] [G loss: 3.272118]\n",
      "21 [C loss: 1.061438] [G loss: 3.338140]\n",
      "22 [C loss: 1.149360] [G loss: 3.365562]\n",
      "23 [C loss: 1.249129] [G loss: 3.374706]\n",
      "24 [C loss: 1.363085] [G loss: 3.385421]\n",
      "25 [C loss: 1.463766] [G loss: 3.366568]\n",
      "26 [C loss: 1.582868] [G loss: 3.210425]\n",
      "27 [C loss: 1.692373] [G loss: 3.169219]\n",
      "28 [C loss: 1.825310] [G loss: 3.030846]\n",
      "29 [C loss: 1.953863] [G loss: 2.940307]\n",
      "30 [C loss: 2.097759] [G loss: 2.794129]\n",
      "31 [C loss: 2.232031] [G loss: 2.741356]\n",
      "32 [C loss: 2.372265] [G loss: 2.660853]\n",
      "33 [C loss: 2.533570] [G loss: 2.535672]\n",
      "34 [C loss: 2.679480] [G loss: 2.422904]\n",
      "35 [C loss: 2.851577] [G loss: 2.344327]\n",
      "36 [C loss: 3.008082] [G loss: 2.291920]\n",
      "37 [C loss: 3.099092] [G loss: 2.169191]\n",
      "38 [C loss: 3.255242] [G loss: 1.992006]\n",
      "39 [C loss: 3.386770] [G loss: 1.915114]\n",
      "40 [C loss: 3.498857] [G loss: 1.853464]\n",
      "41 [C loss: 3.554624] [G loss: 1.804399]\n",
      "42 [C loss: 3.641318] [G loss: 1.709145]\n",
      "43 [C loss: 3.694038] [G loss: 1.744118]\n",
      "44 [C loss: 3.748239] [G loss: 1.732847]\n",
      "45 [C loss: 3.737278] [G loss: 1.647166]\n",
      "46 [C loss: 3.820662] [G loss: 1.743976]\n",
      "47 [C loss: 3.813419] [G loss: 1.759508]\n",
      "48 [C loss: 3.847991] [G loss: 1.827378]\n",
      "49 [C loss: 3.876614] [G loss: 1.785734]\n",
      "50 [C loss: 3.875038] [G loss: 1.882615]\n",
      "51 [C loss: 3.845860] [G loss: 1.924795]\n",
      "52 [C loss: 3.926747] [G loss: 2.001601]\n",
      "53 [C loss: 3.886913] [G loss: 2.030061]\n",
      "54 [C loss: 3.898474] [G loss: 2.144995]\n",
      "55 [C loss: 3.918519] [G loss: 2.157045]\n",
      "56 [C loss: 3.974934] [G loss: 2.286915]\n",
      "57 [C loss: 3.969076] [G loss: 2.362869]\n",
      "58 [C loss: 3.926022] [G loss: 2.475650]\n",
      "59 [C loss: 3.973204] [G loss: 2.490239]\n",
      "60 [C loss: 3.978966] [G loss: 2.528644]\n",
      "61 [C loss: 3.950478] [G loss: 2.559560]\n",
      "62 [C loss: 3.922246] [G loss: 2.656981]\n",
      "63 [C loss: 3.938603] [G loss: 2.712024]\n",
      "64 [C loss: 3.953022] [G loss: 2.629826]\n",
      "65 [C loss: 3.954976] [G loss: 2.590715]\n",
      "66 [C loss: 3.908945] [G loss: 2.663004]\n",
      "67 [C loss: 3.910760] [G loss: 2.721845]\n",
      "68 [C loss: 3.931299] [G loss: 2.729134]\n",
      "69 [C loss: 3.886359] [G loss: 2.501155]\n",
      "70 [C loss: 3.907832] [G loss: 2.619565]\n",
      "71 [C loss: 3.884040] [G loss: 2.580695]\n",
      "72 [C loss: 3.872417] [G loss: 2.450233]\n",
      "73 [C loss: 3.891705] [G loss: 2.514846]\n",
      "74 [C loss: 3.892585] [G loss: 2.658710]\n",
      "75 [C loss: 3.843382] [G loss: 2.665907]\n",
      "76 [C loss: 3.847214] [G loss: 2.587555]\n",
      "77 [C loss: 3.838462] [G loss: 2.568546]\n",
      "78 [C loss: 3.837533] [G loss: 2.584265]\n",
      "79 [C loss: 3.815199] [G loss: 2.528107]\n",
      "80 [C loss: 3.815221] [G loss: 2.362347]\n",
      "81 [C loss: 3.792732] [G loss: 2.488522]\n",
      "82 [C loss: 3.835749] [G loss: 2.566240]\n",
      "83 [C loss: 3.753249] [G loss: 2.591795]\n",
      "84 [C loss: 3.782879] [G loss: 2.615304]\n",
      "85 [C loss: 3.739516] [G loss: 2.557458]\n",
      "86 [C loss: 3.741660] [G loss: 2.580675]\n",
      "87 [C loss: 3.720185] [G loss: 2.650332]\n",
      "88 [C loss: 3.712605] [G loss: 2.537214]\n",
      "89 [C loss: 3.723808] [G loss: 2.666915]\n",
      "90 [C loss: 3.659688] [G loss: 2.606132]\n",
      "91 [C loss: 3.680087] [G loss: 2.628243]\n",
      "92 [C loss: 3.634135] [G loss: 2.728866]\n",
      "93 [C loss: 3.677929] [G loss: 2.719415]\n",
      "94 [C loss: 3.646846] [G loss: 2.639290]\n",
      "95 [C loss: 3.633191] [G loss: 2.732516]\n",
      "96 [C loss: 3.633432] [G loss: 2.574334]\n",
      "97 [C loss: 3.606825] [G loss: 2.665892]\n",
      "98 [C loss: 3.619632] [G loss: 2.657810]\n",
      "99 [C loss: 3.549852] [G loss: 2.734918]\n",
      "100 [C loss: 3.555117] [G loss: 2.687125]\n",
      "101 [C loss: 3.584467] [G loss: 2.809109]\n",
      "102 [C loss: 3.566127] [G loss: 2.764946]\n",
      "103 [C loss: 3.545470] [G loss: 2.803033]\n",
      "104 [C loss: 3.562221] [G loss: 2.806834]\n",
      "105 [C loss: 3.525623] [G loss: 2.821747]\n",
      "106 [C loss: 3.491205] [G loss: 2.841568]\n",
      "107 [C loss: 3.511903] [G loss: 2.861382]\n",
      "108 [C loss: 3.511746] [G loss: 2.880768]\n",
      "109 [C loss: 3.469483] [G loss: 2.866742]\n",
      "110 [C loss: 3.477525] [G loss: 2.785288]\n",
      "111 [C loss: 3.443319] [G loss: 2.925201]\n",
      "112 [C loss: 3.437616] [G loss: 2.955864]\n",
      "113 [C loss: 3.440867] [G loss: 3.011683]\n",
      "114 [C loss: 3.428368] [G loss: 3.013694]\n",
      "115 [C loss: 3.412348] [G loss: 3.057436]\n",
      "116 [C loss: 3.376920] [G loss: 3.002077]\n",
      "117 [C loss: 3.387162] [G loss: 2.999661]\n",
      "118 [C loss: 3.297105] [G loss: 3.068587]\n",
      "119 [C loss: 3.349812] [G loss: 3.107055]\n",
      "120 [C loss: 3.349232] [G loss: 3.080200]\n",
      "121 [C loss: 3.316138] [G loss: 3.157526]\n",
      "122 [C loss: 3.300675] [G loss: 3.136007]\n",
      "123 [C loss: 3.276343] [G loss: 3.186205]\n",
      "124 [C loss: 3.252433] [G loss: 3.187936]\n",
      "125 [C loss: 3.283329] [G loss: 3.143764]\n",
      "126 [C loss: 3.231247] [G loss: 3.162347]\n",
      "127 [C loss: 3.251370] [G loss: 3.302640]\n",
      "128 [C loss: 3.222668] [G loss: 3.261582]\n",
      "129 [C loss: 3.192468] [G loss: 3.203286]\n",
      "130 [C loss: 3.193538] [G loss: 3.364785]\n",
      "131 [C loss: 3.194439] [G loss: 3.202200]\n",
      "132 [C loss: 3.141909] [G loss: 3.285204]\n",
      "133 [C loss: 3.183544] [G loss: 3.325095]\n",
      "134 [C loss: 3.144366] [G loss: 3.324169]\n",
      "135 [C loss: 3.152763] [G loss: 3.412468]\n",
      "136 [C loss: 3.103811] [G loss: 3.412575]\n",
      "137 [C loss: 3.132157] [G loss: 3.407287]\n",
      "138 [C loss: 3.091846] [G loss: 3.465048]\n",
      "139 [C loss: 3.074837] [G loss: 3.407775]\n",
      "140 [C loss: 3.073225] [G loss: 3.389602]\n",
      "141 [C loss: 3.102927] [G loss: 3.412647]\n",
      "142 [C loss: 3.018039] [G loss: 3.561105]\n",
      "143 [C loss: 3.047745] [G loss: 3.402066]\n",
      "144 [C loss: 3.059201] [G loss: 3.535815]\n",
      "145 [C loss: 3.023140] [G loss: 3.463295]\n",
      "146 [C loss: 3.015842] [G loss: 3.586636]\n",
      "147 [C loss: 2.984434] [G loss: 3.523783]\n",
      "148 [C loss: 2.967397] [G loss: 3.629401]\n",
      "149 [C loss: 2.961822] [G loss: 3.602112]\n",
      "150 [C loss: 2.949552] [G loss: 3.556531]\n",
      "151 [C loss: 2.934160] [G loss: 3.645036]\n",
      "152 [C loss: 2.939944] [G loss: 3.704041]\n",
      "153 [C loss: 2.900862] [G loss: 3.635976]\n",
      "154 [C loss: 2.899309] [G loss: 3.758018]\n",
      "155 [C loss: 2.887167] [G loss: 3.666433]\n",
      "156 [C loss: 2.879817] [G loss: 3.787115]\n",
      "157 [C loss: 2.891588] [G loss: 3.788453]\n",
      "158 [C loss: 2.817056] [G loss: 3.729860]\n",
      "159 [C loss: 2.837426] [G loss: 3.827839]\n",
      "160 [C loss: 2.816393] [G loss: 3.820630]\n",
      "161 [C loss: 2.793227] [G loss: 3.775558]\n",
      "162 [C loss: 2.786952] [G loss: 3.954620]\n",
      "163 [C loss: 2.827967] [G loss: 3.894799]\n",
      "164 [C loss: 2.769485] [G loss: 3.934273]\n",
      "165 [C loss: 2.775993] [G loss: 3.814440]\n",
      "166 [C loss: 2.738801] [G loss: 3.895780]\n",
      "167 [C loss: 2.761051] [G loss: 3.904816]\n",
      "168 [C loss: 2.716804] [G loss: 3.867430]\n",
      "169 [C loss: 2.698506] [G loss: 3.987363]\n",
      "170 [C loss: 2.687931] [G loss: 4.009051]\n",
      "171 [C loss: 2.693331] [G loss: 4.094916]\n",
      "172 [C loss: 2.685183] [G loss: 3.898898]\n",
      "173 [C loss: 2.675454] [G loss: 3.903386]\n",
      "174 [C loss: 2.642907] [G loss: 3.921150]\n",
      "175 [C loss: 2.619638] [G loss: 3.935294]\n",
      "176 [C loss: 2.637313] [G loss: 4.042197]\n",
      "177 [C loss: 2.619606] [G loss: 3.880079]\n",
      "178 [C loss: 2.618105] [G loss: 4.027918]\n",
      "179 [C loss: 2.579261] [G loss: 4.025972]\n",
      "180 [C loss: 2.597076] [G loss: 4.027341]\n",
      "181 [C loss: 2.596206] [G loss: 4.169472]\n",
      "182 [C loss: 2.565399] [G loss: 4.052928]\n",
      "183 [C loss: 2.529037] [G loss: 3.912218]\n",
      "184 [C loss: 2.541042] [G loss: 4.097123]\n",
      "185 [C loss: 2.525789] [G loss: 4.040543]\n",
      "186 [C loss: 2.514235] [G loss: 4.160676]\n",
      "187 [C loss: 2.490592] [G loss: 4.101504]\n",
      "188 [C loss: 2.533227] [G loss: 4.078129]\n",
      "189 [C loss: 2.470869] [G loss: 4.049763]\n",
      "190 [C loss: 2.491984] [G loss: 4.175226]\n",
      "191 [C loss: 2.464527] [G loss: 4.220201]\n",
      "192 [C loss: 2.456326] [G loss: 4.080182]\n",
      "193 [C loss: 2.438694] [G loss: 4.055119]\n",
      "194 [C loss: 2.414660] [G loss: 4.135841]\n",
      "195 [C loss: 2.409578] [G loss: 4.211236]\n",
      "196 [C loss: 2.426138] [G loss: 4.221616]\n",
      "197 [C loss: 2.404432] [G loss: 4.321775]\n",
      "198 [C loss: 2.374756] [G loss: 4.255567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 [C loss: 2.404924] [G loss: 4.215025]\n",
      "200 [C loss: 2.362736] [G loss: 4.192605]\n",
      "201 [C loss: 2.383812] [G loss: 4.211568]\n",
      "202 [C loss: 2.337744] [G loss: 4.165071]\n",
      "203 [C loss: 2.352346] [G loss: 4.134031]\n",
      "204 [C loss: 2.340656] [G loss: 4.120289]\n",
      "205 [C loss: 2.321086] [G loss: 4.073788]\n",
      "206 [C loss: 2.316569] [G loss: 4.319292]\n",
      "207 [C loss: 2.296055] [G loss: 4.203050]\n",
      "208 [C loss: 2.290715] [G loss: 4.240356]\n",
      "209 [C loss: 2.284574] [G loss: 4.305873]\n",
      "210 [C loss: 2.276279] [G loss: 4.208456]\n",
      "211 [C loss: 2.253073] [G loss: 4.166924]\n",
      "212 [C loss: 2.249021] [G loss: 4.109298]\n",
      "213 [C loss: 2.252174] [G loss: 4.045542]\n",
      "214 [C loss: 2.236474] [G loss: 4.107843]\n",
      "215 [C loss: 2.229902] [G loss: 4.315563]\n",
      "216 [C loss: 2.212293] [G loss: 4.215955]\n",
      "217 [C loss: 2.220914] [G loss: 4.205770]\n",
      "218 [C loss: 2.191167] [G loss: 4.076331]\n",
      "219 [C loss: 2.203444] [G loss: 4.114055]\n",
      "220 [C loss: 2.180731] [G loss: 4.098160]\n",
      "221 [C loss: 2.170637] [G loss: 4.183797]\n",
      "222 [C loss: 2.172136] [G loss: 4.122784]\n",
      "223 [C loss: 2.189243] [G loss: 4.276225]\n",
      "224 [C loss: 2.164920] [G loss: 4.046957]\n",
      "225 [C loss: 2.163455] [G loss: 3.949826]\n",
      "226 [C loss: 2.142574] [G loss: 4.042062]\n",
      "227 [C loss: 2.147225] [G loss: 4.178198]\n",
      "228 [C loss: 2.145790] [G loss: 3.948230]\n",
      "229 [C loss: 2.107388] [G loss: 3.902764]\n",
      "230 [C loss: 2.146006] [G loss: 3.945436]\n",
      "231 [C loss: 2.108034] [G loss: 4.073144]\n",
      "232 [C loss: 2.115929] [G loss: 4.126091]\n",
      "233 [C loss: 2.103966] [G loss: 4.169350]\n",
      "234 [C loss: 2.074537] [G loss: 4.093958]\n",
      "235 [C loss: 2.104270] [G loss: 3.908879]\n",
      "236 [C loss: 2.076087] [G loss: 4.051241]\n",
      "237 [C loss: 2.068345] [G loss: 3.971733]\n",
      "238 [C loss: 2.071379] [G loss: 4.053947]\n",
      "239 [C loss: 2.050717] [G loss: 4.010732]\n",
      "240 [C loss: 2.074158] [G loss: 3.944793]\n",
      "241 [C loss: 2.063737] [G loss: 3.976502]\n",
      "242 [C loss: 2.050569] [G loss: 4.095374]\n",
      "243 [C loss: 2.026051] [G loss: 4.011426]\n",
      "244 [C loss: 2.004697] [G loss: 3.947739]\n",
      "245 [C loss: 2.037023] [G loss: 3.972191]\n",
      "246 [C loss: 2.002948] [G loss: 4.070879]\n",
      "247 [C loss: 2.020228] [G loss: 3.888816]\n",
      "248 [C loss: 2.027324] [G loss: 3.916646]\n",
      "249 [C loss: 1.998451] [G loss: 3.742913]\n",
      "250 [C loss: 2.016316] [G loss: 4.077525]\n",
      "251 [C loss: 2.024117] [G loss: 3.959251]\n",
      "252 [C loss: 2.000620] [G loss: 3.919759]\n",
      "253 [C loss: 1.999255] [G loss: 3.786487]\n",
      "254 [C loss: 1.989249] [G loss: 3.872058]\n",
      "255 [C loss: 1.953580] [G loss: 3.730579]\n",
      "256 [C loss: 1.960869] [G loss: 3.874557]\n",
      "257 [C loss: 1.955891] [G loss: 3.868489]\n",
      "258 [C loss: 1.959395] [G loss: 3.848441]\n",
      "259 [C loss: 1.931354] [G loss: 3.791389]\n",
      "260 [C loss: 1.955754] [G loss: 3.854223]\n",
      "261 [C loss: 1.950320] [G loss: 3.744650]\n",
      "262 [C loss: 1.945360] [G loss: 3.658472]\n",
      "263 [C loss: 1.930430] [G loss: 3.679578]\n",
      "264 [C loss: 1.942084] [G loss: 3.693045]\n",
      "265 [C loss: 1.932092] [G loss: 3.678516]\n",
      "266 [C loss: 1.934440] [G loss: 3.544161]\n",
      "267 [C loss: 1.900742] [G loss: 3.643354]\n",
      "268 [C loss: 1.914391] [G loss: 3.796806]\n",
      "269 [C loss: 1.917702] [G loss: 3.737203]\n",
      "270 [C loss: 1.884962] [G loss: 3.502139]\n",
      "271 [C loss: 1.892581] [G loss: 3.287427]\n",
      "272 [C loss: 1.880323] [G loss: 3.427736]\n",
      "273 [C loss: 1.885224] [G loss: 3.669604]\n",
      "274 [C loss: 1.873326] [G loss: 3.624830]\n",
      "275 [C loss: 1.869207] [G loss: 3.544267]\n",
      "276 [C loss: 1.866893] [G loss: 3.336674]\n",
      "277 [C loss: 1.846227] [G loss: 3.458243]\n",
      "278 [C loss: 1.865232] [G loss: 3.555595]\n",
      "279 [C loss: 1.856064] [G loss: 3.382363]\n",
      "280 [C loss: 1.857238] [G loss: 3.616821]\n",
      "281 [C loss: 1.839319] [G loss: 3.755505]\n",
      "282 [C loss: 1.855495] [G loss: 3.550856]\n",
      "283 [C loss: 1.830762] [G loss: 3.289602]\n",
      "284 [C loss: 1.879018] [G loss: 3.418041]\n",
      "285 [C loss: 1.836246] [G loss: 3.491175]\n",
      "286 [C loss: 1.825911] [G loss: 3.533566]\n",
      "287 [C loss: 1.817766] [G loss: 3.470546]\n",
      "288 [C loss: 1.816383] [G loss: 3.382964]\n",
      "289 [C loss: 1.805201] [G loss: 3.399126]\n",
      "290 [C loss: 1.821469] [G loss: 3.202446]\n",
      "291 [C loss: 1.822681] [G loss: 3.130330]\n",
      "292 [C loss: 1.817318] [G loss: 3.300409]\n",
      "293 [C loss: 1.819332] [G loss: 3.475990]\n",
      "294 [C loss: 1.832427] [G loss: 3.170922]\n",
      "295 [C loss: 1.802503] [G loss: 3.119170]\n",
      "296 [C loss: 1.818076] [G loss: 3.009843]\n",
      "297 [C loss: 1.790939] [G loss: 3.028573]\n",
      "298 [C loss: 1.774083] [G loss: 3.076056]\n",
      "299 [C loss: 1.801606] [G loss: 3.302207]\n",
      "300 [C loss: 1.810108] [G loss: 3.156865]\n",
      "301 [C loss: 1.776148] [G loss: 3.167967]\n",
      "302 [C loss: 1.793152] [G loss: 2.969943]\n",
      "303 [C loss: 1.767029] [G loss: 2.862800]\n",
      "304 [C loss: 1.771294] [G loss: 2.915908]\n",
      "305 [C loss: 1.778737] [G loss: 3.125112]\n",
      "306 [C loss: 1.762982] [G loss: 3.274527]\n",
      "307 [C loss: 1.742366] [G loss: 3.050518]\n",
      "308 [C loss: 1.779066] [G loss: 3.068637]\n",
      "309 [C loss: 1.755744] [G loss: 2.900891]\n",
      "310 [C loss: 1.757008] [G loss: 2.999738]\n",
      "311 [C loss: 1.747198] [G loss: 3.192073]\n",
      "312 [C loss: 1.742645] [G loss: 3.142346]\n",
      "313 [C loss: 1.728441] [G loss: 2.958039]\n",
      "314 [C loss: 1.734225] [G loss: 2.775625]\n",
      "315 [C loss: 1.725791] [G loss: 2.859527]\n",
      "316 [C loss: 1.721587] [G loss: 2.979828]\n",
      "317 [C loss: 1.737694] [G loss: 2.972589]\n",
      "318 [C loss: 1.718688] [G loss: 2.941206]\n",
      "319 [C loss: 1.719870] [G loss: 2.853380]\n",
      "320 [C loss: 1.731484] [G loss: 2.659636]\n",
      "321 [C loss: 1.742553] [G loss: 2.817946]\n",
      "322 [C loss: 1.709681] [G loss: 2.913276]\n",
      "323 [C loss: 1.724214] [G loss: 2.811926]\n",
      "324 [C loss: 1.744205] [G loss: 2.959979]\n",
      "325 [C loss: 1.734544] [G loss: 2.864915]\n",
      "326 [C loss: 1.690873] [G loss: 2.632912]\n",
      "327 [C loss: 1.701409] [G loss: 2.911605]\n",
      "328 [C loss: 1.710965] [G loss: 2.794213]\n",
      "329 [C loss: 1.690744] [G loss: 2.641442]\n",
      "330 [C loss: 1.712621] [G loss: 2.596307]\n",
      "331 [C loss: 1.734178] [G loss: 2.702522]\n",
      "332 [C loss: 1.690063] [G loss: 2.634559]\n",
      "333 [C loss: 1.707268] [G loss: 2.792130]\n",
      "334 [C loss: 1.688560] [G loss: 2.610251]\n",
      "335 [C loss: 1.677624] [G loss: 2.663577]\n",
      "336 [C loss: 1.709796] [G loss: 2.454911]\n",
      "337 [C loss: 1.683834] [G loss: 2.757071]\n",
      "338 [C loss: 1.670145] [G loss: 2.704735]\n",
      "339 [C loss: 1.662952] [G loss: 2.506375]\n",
      "340 [C loss: 1.708849] [G loss: 2.446917]\n",
      "341 [C loss: 1.682416] [G loss: 2.667419]\n",
      "342 [C loss: 1.670599] [G loss: 2.612596]\n",
      "343 [C loss: 1.678663] [G loss: 2.478974]\n",
      "344 [C loss: 1.676995] [G loss: 2.601055]\n",
      "345 [C loss: 1.663101] [G loss: 2.621047]\n",
      "346 [C loss: 1.675103] [G loss: 2.655314]\n",
      "347 [C loss: 1.655463] [G loss: 2.561093]\n",
      "348 [C loss: 1.678746] [G loss: 2.707644]\n",
      "349 [C loss: 1.673020] [G loss: 2.556127]\n",
      "350 [C loss: 1.667756] [G loss: 2.505859]\n",
      "351 [C loss: 1.661865] [G loss: 2.454319]\n",
      "352 [C loss: 1.642801] [G loss: 2.442333]\n",
      "353 [C loss: 1.650983] [G loss: 2.534536]\n",
      "354 [C loss: 1.676917] [G loss: 2.481813]\n",
      "355 [C loss: 1.648496] [G loss: 2.527993]\n",
      "356 [C loss: 1.628551] [G loss: 2.673873]\n",
      "357 [C loss: 1.633752] [G loss: 2.650177]\n",
      "358 [C loss: 1.641179] [G loss: 2.480051]\n",
      "359 [C loss: 1.671982] [G loss: 2.437210]\n",
      "360 [C loss: 1.662549] [G loss: 2.578395]\n",
      "361 [C loss: 1.629362] [G loss: 2.425234]\n",
      "362 [C loss: 1.635553] [G loss: 2.348248]\n",
      "363 [C loss: 1.623096] [G loss: 2.591320]\n",
      "364 [C loss: 1.629398] [G loss: 2.365425]\n",
      "365 [C loss: 1.641999] [G loss: 2.284868]\n",
      "366 [C loss: 1.636451] [G loss: 2.274728]\n",
      "367 [C loss: 1.644736] [G loss: 2.070634]\n",
      "368 [C loss: 1.630782] [G loss: 2.518710]\n",
      "369 [C loss: 1.603545] [G loss: 2.373525]\n",
      "370 [C loss: 1.645670] [G loss: 2.622789]\n",
      "371 [C loss: 1.630562] [G loss: 2.436925]\n",
      "372 [C loss: 1.620766] [G loss: 2.321093]\n",
      "373 [C loss: 1.648659] [G loss: 2.308428]\n",
      "374 [C loss: 1.637354] [G loss: 2.347067]\n",
      "375 [C loss: 1.632750] [G loss: 2.320961]\n",
      "376 [C loss: 1.615912] [G loss: 2.554616]\n",
      "377 [C loss: 1.607399] [G loss: 2.284872]\n",
      "378 [C loss: 1.606016] [G loss: 1.987924]\n",
      "379 [C loss: 1.603929] [G loss: 2.052284]\n",
      "380 [C loss: 1.612270] [G loss: 2.202706]\n",
      "381 [C loss: 1.605082] [G loss: 2.216118]\n",
      "382 [C loss: 1.615757] [G loss: 2.418287]\n",
      "383 [C loss: 1.607065] [G loss: 2.429268]\n",
      "384 [C loss: 1.623052] [G loss: 2.210379]\n",
      "385 [C loss: 1.613076] [G loss: 1.890977]\n",
      "386 [C loss: 1.625676] [G loss: 2.068232]\n",
      "387 [C loss: 1.604172] [G loss: 2.115979]\n",
      "388 [C loss: 1.606128] [G loss: 2.238900]\n",
      "389 [C loss: 1.624641] [G loss: 2.050010]\n",
      "390 [C loss: 1.591214] [G loss: 2.171697]\n",
      "391 [C loss: 1.578361] [G loss: 2.338314]\n",
      "392 [C loss: 1.597693] [G loss: 2.084119]\n",
      "393 [C loss: 1.600554] [G loss: 1.919145]\n",
      "394 [C loss: 1.583295] [G loss: 1.765165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 [C loss: 1.618807] [G loss: 1.815562]\n",
      "396 [C loss: 1.589751] [G loss: 1.968971]\n",
      "397 [C loss: 1.574741] [G loss: 1.719409]\n",
      "398 [C loss: 1.588323] [G loss: 1.734147]\n",
      "399 [C loss: 1.582126] [G loss: 1.876178]\n",
      "400 [C loss: 1.586849] [G loss: 2.148906]\n",
      "401 [C loss: 1.588051] [G loss: 2.158376]\n",
      "402 [C loss: 1.585164] [G loss: 1.799545]\n",
      "403 [C loss: 1.573168] [G loss: 1.926984]\n",
      "404 [C loss: 1.589682] [G loss: 1.961890]\n",
      "405 [C loss: 1.560594] [G loss: 2.041560]\n",
      "406 [C loss: 1.556835] [G loss: 1.835124]\n",
      "407 [C loss: 1.579972] [G loss: 1.614364]\n",
      "408 [C loss: 1.581429] [G loss: 1.637514]\n",
      "409 [C loss: 1.541134] [G loss: 1.402994]\n",
      "410 [C loss: 1.586438] [G loss: 1.696685]\n",
      "411 [C loss: 1.546169] [G loss: 1.835757]\n",
      "412 [C loss: 1.577208] [G loss: 1.719589]\n",
      "413 [C loss: 1.577323] [G loss: 1.743749]\n",
      "414 [C loss: 1.571736] [G loss: 1.703491]\n",
      "415 [C loss: 1.522874] [G loss: 1.620181]\n",
      "416 [C loss: 1.540247] [G loss: 1.949568]\n",
      "417 [C loss: 1.591661] [G loss: 1.760473]\n",
      "418 [C loss: 1.557000] [G loss: 1.752320]\n",
      "419 [C loss: 1.551088] [G loss: 1.693157]\n",
      "420 [C loss: 1.578534] [G loss: 1.402660]\n",
      "421 [C loss: 1.559501] [G loss: 1.773064]\n",
      "422 [C loss: 1.553494] [G loss: 1.624764]\n",
      "423 [C loss: 1.600105] [G loss: 1.590243]\n",
      "424 [C loss: 1.578860] [G loss: 1.761725]\n",
      "425 [C loss: 1.580059] [G loss: 1.507515]\n",
      "426 [C loss: 1.564620] [G loss: 1.690612]\n",
      "427 [C loss: 1.552948] [G loss: 1.797716]\n",
      "428 [C loss: 1.526051] [G loss: 1.729727]\n",
      "429 [C loss: 1.537613] [G loss: 1.450191]\n",
      "430 [C loss: 1.522950] [G loss: 1.529734]\n",
      "431 [C loss: 1.558907] [G loss: 1.704830]\n",
      "432 [C loss: 1.565591] [G loss: 1.956530]\n",
      "433 [C loss: 1.559677] [G loss: 1.808311]\n",
      "434 [C loss: 1.563029] [G loss: 1.655497]\n",
      "435 [C loss: 1.537241] [G loss: 1.717896]\n",
      "436 [C loss: 1.559753] [G loss: 1.601562]\n",
      "437 [C loss: 1.531676] [G loss: 1.464334]\n",
      "438 [C loss: 1.535396] [G loss: 1.566074]\n",
      "439 [C loss: 1.531374] [G loss: 1.659813]\n",
      "440 [C loss: 1.543628] [G loss: 1.747968]\n",
      "441 [C loss: 1.539890] [G loss: 1.784679]\n",
      "442 [C loss: 1.561265] [G loss: 1.868293]\n",
      "443 [C loss: 1.583036] [G loss: 1.726110]\n",
      "444 [C loss: 1.565439] [G loss: 1.612769]\n",
      "445 [C loss: 1.543673] [G loss: 1.436563]\n",
      "446 [C loss: 1.559775] [G loss: 1.561973]\n",
      "447 [C loss: 1.530464] [G loss: 1.663232]\n",
      "448 [C loss: 1.521676] [G loss: 1.847306]\n",
      "449 [C loss: 1.560179] [G loss: 1.951730]\n",
      "450 [C loss: 1.541176] [G loss: 1.946803]\n",
      "451 [C loss: 1.551269] [G loss: 1.830264]\n",
      "452 [C loss: 1.522852] [G loss: 1.536997]\n",
      "453 [C loss: 1.563232] [G loss: 1.445777]\n",
      "454 [C loss: 1.538082] [G loss: 1.620699]\n",
      "455 [C loss: 1.542680] [G loss: 1.650269]\n",
      "456 [C loss: 1.533195] [G loss: 1.796481]\n",
      "457 [C loss: 1.525991] [G loss: 1.693901]\n",
      "458 [C loss: 1.515194] [G loss: 1.730160]\n",
      "459 [C loss: 1.543545] [G loss: 1.198879]\n",
      "460 [C loss: 1.561359] [G loss: 1.164221]\n",
      "461 [C loss: 1.516122] [G loss: 1.584448]\n",
      "462 [C loss: 1.516456] [G loss: 1.689096]\n",
      "463 [C loss: 1.498532] [G loss: 1.634249]\n",
      "464 [C loss: 1.561307] [G loss: 1.563731]\n",
      "465 [C loss: 1.532321] [G loss: 1.379325]\n",
      "466 [C loss: 1.514575] [G loss: 1.164615]\n",
      "467 [C loss: 1.514398] [G loss: 1.354380]\n",
      "468 [C loss: 1.542120] [G loss: 1.325020]\n",
      "469 [C loss: 1.540610] [G loss: 1.845175]\n",
      "470 [C loss: 1.524375] [G loss: 1.733868]\n",
      "471 [C loss: 1.509037] [G loss: 1.382253]\n",
      "472 [C loss: 1.525394] [G loss: 1.513947]\n",
      "473 [C loss: 1.507043] [G loss: 1.670345]\n",
      "474 [C loss: 1.530347] [G loss: 1.705450]\n",
      "475 [C loss: 1.501415] [G loss: 1.619448]\n",
      "476 [C loss: 1.516060] [G loss: 1.766075]\n",
      "477 [C loss: 1.503298] [G loss: 1.764242]\n",
      "478 [C loss: 1.520095] [G loss: 1.816933]\n",
      "479 [C loss: 1.488137] [G loss: 1.568112]\n",
      "480 [C loss: 1.482686] [G loss: 1.368520]\n",
      "481 [C loss: 1.507520] [G loss: 1.307247]\n",
      "482 [C loss: 1.540884] [G loss: 1.291178]\n",
      "483 [C loss: 1.487633] [G loss: 1.521491]\n",
      "484 [C loss: 1.523652] [G loss: 1.532180]\n",
      "485 [C loss: 1.488108] [G loss: 1.514526]\n",
      "486 [C loss: 1.549197] [G loss: 1.181140]\n",
      "487 [C loss: 1.483713] [G loss: 1.220690]\n",
      "488 [C loss: 1.524294] [G loss: 1.345075]\n",
      "489 [C loss: 1.524917] [G loss: 0.940302]\n",
      "490 [C loss: 1.498792] [G loss: 1.002388]\n",
      "491 [C loss: 1.530702] [G loss: 1.154097]\n",
      "492 [C loss: 1.509084] [G loss: 1.355417]\n",
      "493 [C loss: 1.522759] [G loss: 1.546419]\n",
      "494 [C loss: 1.515700] [G loss: 1.294177]\n",
      "495 [C loss: 1.509029] [G loss: 1.289547]\n",
      "496 [C loss: 1.537379] [G loss: 1.264255]\n",
      "497 [C loss: 1.501464] [G loss: 1.167297]\n",
      "498 [C loss: 1.518888] [G loss: 1.313089]\n",
      "499 [C loss: 1.489532] [G loss: 1.165410]\n",
      "500 [C loss: 1.484222] [G loss: 1.335065]\n",
      "501 [C loss: 1.500236] [G loss: 1.624907]\n",
      "502 [C loss: 1.475488] [G loss: 1.572468]\n",
      "503 [C loss: 1.492297] [G loss: 1.371127]\n",
      "504 [C loss: 1.500221] [G loss: 1.236155]\n",
      "505 [C loss: 1.479020] [G loss: 1.007308]\n",
      "506 [C loss: 1.498380] [G loss: 1.397417]\n",
      "507 [C loss: 1.482546] [G loss: 1.535961]\n",
      "508 [C loss: 1.489611] [G loss: 1.612189]\n",
      "509 [C loss: 1.509912] [G loss: 1.616446]\n",
      "510 [C loss: 1.511560] [G loss: 1.372926]\n",
      "511 [C loss: 1.491770] [G loss: 1.355790]\n",
      "512 [C loss: 1.496739] [G loss: 1.400639]\n",
      "513 [C loss: 1.504364] [G loss: 1.491671]\n",
      "514 [C loss: 1.450287] [G loss: 1.766859]\n",
      "515 [C loss: 1.480460] [G loss: 1.356556]\n",
      "516 [C loss: 1.474821] [G loss: 1.562940]\n",
      "517 [C loss: 1.499707] [G loss: 1.782807]\n",
      "518 [C loss: 1.476393] [G loss: 1.947013]\n",
      "519 [C loss: 1.488107] [G loss: 1.558560]\n",
      "520 [C loss: 1.491635] [G loss: 1.218825]\n",
      "521 [C loss: 1.474203] [G loss: 1.682166]\n",
      "522 [C loss: 1.501227] [G loss: 1.644376]\n",
      "523 [C loss: 1.443980] [G loss: 1.287284]\n",
      "524 [C loss: 1.466170] [G loss: 1.440467]\n",
      "525 [C loss: 1.449736] [G loss: 1.333363]\n",
      "526 [C loss: 1.483732] [G loss: 1.420553]\n",
      "527 [C loss: 1.476006] [G loss: 1.307084]\n",
      "528 [C loss: 1.472011] [G loss: 1.407386]\n",
      "529 [C loss: 1.468551] [G loss: 1.570839]\n",
      "530 [C loss: 1.474765] [G loss: 1.520840]\n",
      "531 [C loss: 1.477496] [G loss: 1.291631]\n",
      "532 [C loss: 1.494088] [G loss: 1.162339]\n",
      "533 [C loss: 1.497157] [G loss: 1.431055]\n",
      "534 [C loss: 1.450337] [G loss: 1.556433]\n",
      "535 [C loss: 1.460198] [G loss: 1.481820]\n",
      "536 [C loss: 1.468419] [G loss: 1.443695]\n",
      "537 [C loss: 1.447640] [G loss: 1.373768]\n",
      "538 [C loss: 1.486979] [G loss: 1.428144]\n",
      "539 [C loss: 1.453775] [G loss: 1.518716]\n",
      "540 [C loss: 1.457667] [G loss: 1.441500]\n",
      "541 [C loss: 1.473809] [G loss: 1.341664]\n",
      "542 [C loss: 1.477600] [G loss: 1.520846]\n",
      "543 [C loss: 1.460510] [G loss: 1.328532]\n",
      "544 [C loss: 1.480393] [G loss: 1.360210]\n",
      "545 [C loss: 1.487629] [G loss: 1.433374]\n",
      "546 [C loss: 1.476413] [G loss: 1.594010]\n",
      "547 [C loss: 1.443466] [G loss: 1.804008]\n",
      "548 [C loss: 1.432268] [G loss: 1.479123]\n",
      "549 [C loss: 1.453711] [G loss: 1.173686]\n",
      "550 [C loss: 1.488078] [G loss: 1.545791]\n",
      "551 [C loss: 1.473739] [G loss: 1.458109]\n",
      "552 [C loss: 1.473008] [G loss: 1.188585]\n",
      "553 [C loss: 1.457059] [G loss: 1.255961]\n",
      "554 [C loss: 1.448470] [G loss: 1.174700]\n",
      "555 [C loss: 1.448231] [G loss: 1.267385]\n",
      "556 [C loss: 1.463504] [G loss: 1.462222]\n",
      "557 [C loss: 1.470574] [G loss: 1.375402]\n",
      "558 [C loss: 1.443778] [G loss: 1.270034]\n",
      "559 [C loss: 1.454755] [G loss: 1.012760]\n",
      "560 [C loss: 1.462169] [G loss: 1.194256]\n",
      "561 [C loss: 1.427654] [G loss: 1.403666]\n",
      "562 [C loss: 1.434569] [G loss: 1.303178]\n",
      "563 [C loss: 1.466771] [G loss: 1.288781]\n",
      "564 [C loss: 1.437194] [G loss: 1.544589]\n",
      "565 [C loss: 1.423811] [G loss: 1.711703]\n",
      "566 [C loss: 1.445814] [G loss: 1.427680]\n",
      "567 [C loss: 1.436357] [G loss: 1.293296]\n",
      "568 [C loss: 1.455590] [G loss: 1.525833]\n",
      "569 [C loss: 1.483486] [G loss: 1.752428]\n",
      "570 [C loss: 1.435301] [G loss: 1.878456]\n",
      "571 [C loss: 1.464831] [G loss: 1.617437]\n",
      "572 [C loss: 1.434871] [G loss: 1.471914]\n",
      "573 [C loss: 1.460053] [G loss: 1.272459]\n",
      "574 [C loss: 1.441759] [G loss: 1.321073]\n",
      "575 [C loss: 1.441363] [G loss: 1.421503]\n",
      "576 [C loss: 1.455714] [G loss: 1.461536]\n",
      "577 [C loss: 1.417917] [G loss: 1.473263]\n",
      "578 [C loss: 1.438765] [G loss: 1.421782]\n",
      "579 [C loss: 1.446911] [G loss: 1.580678]\n",
      "580 [C loss: 1.439221] [G loss: 1.559912]\n",
      "581 [C loss: 1.465263] [G loss: 1.220347]\n",
      "582 [C loss: 1.459574] [G loss: 1.108960]\n",
      "583 [C loss: 1.437586] [G loss: 1.356440]\n",
      "584 [C loss: 1.475679] [G loss: 1.598390]\n",
      "585 [C loss: 1.491719] [G loss: 1.763274]\n",
      "586 [C loss: 1.429946] [G loss: 1.353232]\n",
      "587 [C loss: 1.414919] [G loss: 1.045168]\n",
      "588 [C loss: 1.400027] [G loss: 1.304470]\n",
      "589 [C loss: 1.411311] [G loss: 1.671708]\n",
      "590 [C loss: 1.426237] [G loss: 1.825594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [C loss: 1.433954] [G loss: 1.341084]\n",
      "592 [C loss: 1.418543] [G loss: 1.046482]\n",
      "593 [C loss: 1.432860] [G loss: 1.131120]\n",
      "594 [C loss: 1.424783] [G loss: 1.373312]\n",
      "595 [C loss: 1.399369] [G loss: 1.729615]\n",
      "596 [C loss: 1.421103] [G loss: 1.586642]\n",
      "597 [C loss: 1.397702] [G loss: 1.140708]\n",
      "598 [C loss: 1.416128] [G loss: 1.431813]\n",
      "599 [C loss: 1.410417] [G loss: 1.547763]\n",
      "600 [C loss: 1.422967] [G loss: 1.794289]\n",
      "601 [C loss: 1.416348] [G loss: 1.825730]\n",
      "602 [C loss: 1.419306] [G loss: 1.659190]\n",
      "603 [C loss: 1.405529] [G loss: 1.491499]\n",
      "604 [C loss: 1.432182] [G loss: 1.624097]\n",
      "605 [C loss: 1.399512] [G loss: 1.642584]\n",
      "606 [C loss: 1.404789] [G loss: 1.810345]\n",
      "607 [C loss: 1.425356] [G loss: 1.677471]\n",
      "608 [C loss: 1.420566] [G loss: 1.285986]\n",
      "609 [C loss: 1.381275] [G loss: 1.187983]\n",
      "610 [C loss: 1.425554] [G loss: 1.459214]\n",
      "611 [C loss: 1.413662] [G loss: 1.655899]\n",
      "612 [C loss: 1.397548] [G loss: 1.597897]\n",
      "613 [C loss: 1.402552] [G loss: 1.695887]\n",
      "614 [C loss: 1.387908] [G loss: 1.563937]\n",
      "615 [C loss: 1.416764] [G loss: 1.428675]\n",
      "616 [C loss: 1.421842] [G loss: 1.199671]\n",
      "617 [C loss: 1.416472] [G loss: 1.408047]\n",
      "618 [C loss: 1.422473] [G loss: 1.734249]\n",
      "619 [C loss: 1.418728] [G loss: 1.746993]\n",
      "620 [C loss: 1.390403] [G loss: 1.524310]\n",
      "621 [C loss: 1.438067] [G loss: 1.350327]\n",
      "622 [C loss: 1.446868] [G loss: 1.288313]\n",
      "623 [C loss: 1.383070] [G loss: 1.543829]\n",
      "624 [C loss: 1.389085] [G loss: 1.429072]\n",
      "625 [C loss: 1.390417] [G loss: 1.534552]\n",
      "626 [C loss: 1.415724] [G loss: 1.514200]\n",
      "627 [C loss: 1.434083] [G loss: 1.216971]\n",
      "628 [C loss: 1.402486] [G loss: 1.240079]\n",
      "629 [C loss: 1.412546] [G loss: 1.406144]\n",
      "630 [C loss: 1.398633] [G loss: 1.480140]\n",
      "631 [C loss: 1.388095] [G loss: 1.728086]\n",
      "632 [C loss: 1.405652] [G loss: 1.517000]\n",
      "633 [C loss: 1.422679] [G loss: 1.568690]\n",
      "634 [C loss: 1.398131] [G loss: 1.334971]\n",
      "635 [C loss: 1.399192] [G loss: 1.076234]\n",
      "636 [C loss: 1.396123] [G loss: 1.329750]\n",
      "637 [C loss: 1.442927] [G loss: 1.774123]\n",
      "638 [C loss: 1.378203] [G loss: 1.832690]\n",
      "639 [C loss: 1.387339] [G loss: 1.830834]\n",
      "640 [C loss: 1.410999] [G loss: 1.259940]\n",
      "641 [C loss: 1.394522] [G loss: 1.269312]\n",
      "642 [C loss: 1.369373] [G loss: 1.727658]\n",
      "643 [C loss: 1.368017] [G loss: 1.843962]\n",
      "644 [C loss: 1.375385] [G loss: 1.872330]\n",
      "645 [C loss: 1.404450] [G loss: 1.838076]\n",
      "646 [C loss: 1.413762] [G loss: 1.888899]\n",
      "647 [C loss: 1.402682] [G loss: 1.234731]\n",
      "648 [C loss: 1.388789] [G loss: 1.407480]\n",
      "649 [C loss: 1.392813] [G loss: 1.620697]\n",
      "650 [C loss: 1.416247] [G loss: 1.947439]\n",
      "651 [C loss: 1.413699] [G loss: 2.070180]\n",
      "652 [C loss: 1.418089] [G loss: 1.941509]\n",
      "653 [C loss: 1.387000] [G loss: 1.597509]\n",
      "654 [C loss: 1.412344] [G loss: 1.434111]\n",
      "655 [C loss: 1.401717] [G loss: 1.698093]\n",
      "656 [C loss: 1.398217] [G loss: 1.840766]\n",
      "657 [C loss: 1.412535] [G loss: 1.919285]\n",
      "658 [C loss: 1.373073] [G loss: 1.931548]\n",
      "659 [C loss: 1.387572] [G loss: 1.766565]\n",
      "660 [C loss: 1.389771] [G loss: 1.512537]\n",
      "661 [C loss: 1.387278] [G loss: 2.136407]\n",
      "662 [C loss: 1.394996] [G loss: 2.391571]\n",
      "663 [C loss: 1.429551] [G loss: 2.144200]\n",
      "664 [C loss: 1.388730] [G loss: 1.849636]\n",
      "665 [C loss: 1.386365] [G loss: 1.533563]\n",
      "666 [C loss: 1.381258] [G loss: 1.312717]\n",
      "667 [C loss: 1.387341] [G loss: 1.735904]\n",
      "668 [C loss: 1.399860] [G loss: 1.940933]\n",
      "669 [C loss: 1.384291] [G loss: 1.878460]\n",
      "670 [C loss: 1.389803] [G loss: 1.456244]\n",
      "671 [C loss: 1.418378] [G loss: 1.162743]\n",
      "672 [C loss: 1.391968] [G loss: 1.381419]\n",
      "673 [C loss: 1.410076] [G loss: 1.722019]\n",
      "674 [C loss: 1.392496] [G loss: 1.939050]\n",
      "675 [C loss: 1.397543] [G loss: 1.698538]\n",
      "676 [C loss: 1.382055] [G loss: 1.569454]\n",
      "677 [C loss: 1.375711] [G loss: 1.573995]\n",
      "678 [C loss: 1.378529] [G loss: 1.713679]\n",
      "679 [C loss: 1.399072] [G loss: 1.745997]\n",
      "680 [C loss: 1.403031] [G loss: 2.234693]\n",
      "681 [C loss: 1.424141] [G loss: 2.067463]\n",
      "682 [C loss: 1.384804] [G loss: 1.648182]\n",
      "683 [C loss: 1.407874] [G loss: 1.598446]\n",
      "684 [C loss: 1.379372] [G loss: 1.376979]\n",
      "685 [C loss: 1.372419] [G loss: 1.683671]\n",
      "686 [C loss: 1.402814] [G loss: 2.267349]\n",
      "687 [C loss: 1.384469] [G loss: 2.532431]\n",
      "688 [C loss: 1.390245] [G loss: 2.245496]\n",
      "689 [C loss: 1.398774] [G loss: 1.712422]\n",
      "690 [C loss: 1.368578] [G loss: 1.618300]\n",
      "691 [C loss: 1.391737] [G loss: 1.663679]\n",
      "692 [C loss: 1.389149] [G loss: 1.844028]\n",
      "693 [C loss: 1.377344] [G loss: 1.661802]\n",
      "694 [C loss: 1.362161] [G loss: 1.940648]\n",
      "695 [C loss: 1.370031] [G loss: 1.874579]\n",
      "696 [C loss: 1.409152] [G loss: 1.639043]\n",
      "697 [C loss: 1.391048] [G loss: 1.506953]\n",
      "698 [C loss: 1.368448] [G loss: 1.421285]\n",
      "699 [C loss: 1.366631] [G loss: 1.754727]\n",
      "700 [C loss: 1.386937] [G loss: 1.822683]\n",
      "701 [C loss: 1.375155] [G loss: 1.747219]\n",
      "702 [C loss: 1.402171] [G loss: 1.450956]\n",
      "703 [C loss: 1.382934] [G loss: 1.283539]\n",
      "704 [C loss: 1.391095] [G loss: 1.310867]\n",
      "705 [C loss: 1.391999] [G loss: 1.524297]\n",
      "706 [C loss: 1.386315] [G loss: 1.305749]\n",
      "707 [C loss: 1.372465] [G loss: 1.201246]\n",
      "708 [C loss: 1.360892] [G loss: 0.955958]\n",
      "709 [C loss: 1.381545] [G loss: 0.918061]\n",
      "710 [C loss: 1.375471] [G loss: 1.047785]\n",
      "711 [C loss: 1.380710] [G loss: 1.390496]\n",
      "712 [C loss: 1.402677] [G loss: 1.720145]\n",
      "713 [C loss: 1.383463] [G loss: 1.744610]\n",
      "714 [C loss: 1.366199] [G loss: 1.493090]\n",
      "715 [C loss: 1.359280] [G loss: 1.116056]\n",
      "716 [C loss: 1.359910] [G loss: 1.219689]\n",
      "717 [C loss: 1.376469] [G loss: 1.033819]\n",
      "718 [C loss: 1.380063] [G loss: 1.444745]\n",
      "719 [C loss: 1.380208] [G loss: 1.388543]\n",
      "720 [C loss: 1.366299] [G loss: 1.106288]\n",
      "721 [C loss: 1.387071] [G loss: 0.964253]\n",
      "722 [C loss: 1.370262] [G loss: 0.890467]\n",
      "723 [C loss: 1.377313] [G loss: 0.836623]\n",
      "724 [C loss: 1.372979] [G loss: 0.888679]\n",
      "725 [C loss: 1.355913] [G loss: 1.124035]\n",
      "726 [C loss: 1.389983] [G loss: 1.462954]\n",
      "727 [C loss: 1.365723] [G loss: 1.471836]\n",
      "728 [C loss: 1.380089] [G loss: 1.221835]\n",
      "729 [C loss: 1.385583] [G loss: 1.185849]\n",
      "730 [C loss: 1.369137] [G loss: 1.181714]\n",
      "731 [C loss: 1.399945] [G loss: 1.269532]\n",
      "732 [C loss: 1.385050] [G loss: 1.741563]\n",
      "733 [C loss: 1.366946] [G loss: 1.887012]\n",
      "734 [C loss: 1.362553] [G loss: 1.886971]\n",
      "735 [C loss: 1.369912] [G loss: 1.883464]\n",
      "736 [C loss: 1.376120] [G loss: 2.089779]\n",
      "737 [C loss: 1.359330] [G loss: 2.289649]\n",
      "738 [C loss: 1.358287] [G loss: 2.002162]\n",
      "739 [C loss: 1.350940] [G loss: 1.643028]\n",
      "740 [C loss: 1.374761] [G loss: 1.507015]\n",
      "741 [C loss: 1.365916] [G loss: 1.758058]\n",
      "742 [C loss: 1.365912] [G loss: 1.810812]\n",
      "743 [C loss: 1.362304] [G loss: 1.772822]\n",
      "744 [C loss: 1.375307] [G loss: 1.494208]\n",
      "745 [C loss: 1.344900] [G loss: 1.761512]\n",
      "746 [C loss: 1.378238] [G loss: 1.991332]\n",
      "747 [C loss: 1.367629] [G loss: 1.622084]\n",
      "748 [C loss: 1.372387] [G loss: 1.348996]\n",
      "749 [C loss: 1.359714] [G loss: 1.156426]\n",
      "750 [C loss: 1.384020] [G loss: 1.491471]\n",
      "751 [C loss: 1.343933] [G loss: 1.766796]\n",
      "752 [C loss: 1.366465] [G loss: 1.855888]\n",
      "753 [C loss: 1.369986] [G loss: 1.491286]\n",
      "754 [C loss: 1.358043] [G loss: 1.429603]\n",
      "755 [C loss: 1.357843] [G loss: 1.842226]\n",
      "756 [C loss: 1.369961] [G loss: 2.157195]\n",
      "757 [C loss: 1.356686] [G loss: 1.937255]\n",
      "758 [C loss: 1.345277] [G loss: 1.655670]\n",
      "759 [C loss: 1.399081] [G loss: 1.720888]\n",
      "760 [C loss: 1.375673] [G loss: 1.708097]\n",
      "761 [C loss: 1.362212] [G loss: 1.433301]\n",
      "762 [C loss: 1.355771] [G loss: 1.249667]\n",
      "763 [C loss: 1.362968] [G loss: 1.659353]\n",
      "764 [C loss: 1.360424] [G loss: 1.923233]\n",
      "765 [C loss: 1.353969] [G loss: 1.990938]\n",
      "766 [C loss: 1.338790] [G loss: 1.790406]\n",
      "767 [C loss: 1.374319] [G loss: 1.311965]\n",
      "768 [C loss: 1.335438] [G loss: 1.264155]\n",
      "769 [C loss: 1.349883] [G loss: 1.601382]\n",
      "770 [C loss: 1.337308] [G loss: 1.966888]\n",
      "771 [C loss: 1.357304] [G loss: 2.083944]\n",
      "772 [C loss: 1.350020] [G loss: 1.800698]\n",
      "773 [C loss: 1.362312] [G loss: 1.024450]\n",
      "774 [C loss: 1.344386] [G loss: 1.033908]\n",
      "775 [C loss: 1.341889] [G loss: 1.334198]\n",
      "776 [C loss: 1.364182] [G loss: 1.785358]\n",
      "777 [C loss: 1.361644] [G loss: 1.987091]\n",
      "778 [C loss: 1.361813] [G loss: 1.637849]\n",
      "779 [C loss: 1.366519] [G loss: 1.454153]\n",
      "780 [C loss: 1.358220] [G loss: 1.061473]\n",
      "781 [C loss: 1.354550] [G loss: 1.201291]\n",
      "782 [C loss: 1.381103] [G loss: 1.530200]\n",
      "783 [C loss: 1.338623] [G loss: 1.629452]\n",
      "784 [C loss: 1.356293] [G loss: 1.828774]\n",
      "785 [C loss: 1.362142] [G loss: 2.252226]\n",
      "786 [C loss: 1.347898] [G loss: 2.142374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787 [C loss: 1.349171] [G loss: 1.892007]\n",
      "788 [C loss: 1.385580] [G loss: 2.026464]\n",
      "789 [C loss: 1.370872] [G loss: 2.122526]\n",
      "790 [C loss: 1.345283] [G loss: 1.932282]\n",
      "791 [C loss: 1.356934] [G loss: 1.594385]\n",
      "792 [C loss: 1.367451] [G loss: 1.486170]\n",
      "793 [C loss: 1.353296] [G loss: 1.881435]\n",
      "794 [C loss: 1.351569] [G loss: 2.247994]\n",
      "795 [C loss: 1.364089] [G loss: 2.056398]\n",
      "796 [C loss: 1.363627] [G loss: 1.473999]\n",
      "797 [C loss: 1.338356] [G loss: 1.693297]\n",
      "798 [C loss: 1.353756] [G loss: 1.654984]\n",
      "799 [C loss: 1.348402] [G loss: 2.006421]\n",
      "800 [C loss: 1.308546] [G loss: 1.792029]\n",
      "801 [C loss: 1.321998] [G loss: 1.353310]\n",
      "802 [C loss: 1.369467] [G loss: 1.293284]\n",
      "803 [C loss: 1.367649] [G loss: 1.377439]\n",
      "804 [C loss: 1.385367] [G loss: 1.316509]\n",
      "805 [C loss: 1.371125] [G loss: 1.293992]\n",
      "806 [C loss: 1.347557] [G loss: 1.103965]\n",
      "807 [C loss: 1.366812] [G loss: 0.856441]\n",
      "808 [C loss: 1.332121] [G loss: 0.978162]\n",
      "809 [C loss: 1.329022] [G loss: 1.409789]\n",
      "810 [C loss: 1.368463] [G loss: 2.038348]\n",
      "811 [C loss: 1.354816] [G loss: 1.936364]\n",
      "812 [C loss: 1.350807] [G loss: 1.639685]\n",
      "813 [C loss: 1.344984] [G loss: 1.325285]\n",
      "814 [C loss: 1.373145] [G loss: 1.367934]\n",
      "815 [C loss: 1.335703] [G loss: 1.547478]\n",
      "816 [C loss: 1.326254] [G loss: 1.673201]\n",
      "817 [C loss: 1.324113] [G loss: 1.531399]\n",
      "818 [C loss: 1.378515] [G loss: 1.444377]\n",
      "819 [C loss: 1.341523] [G loss: 1.415384]\n",
      "820 [C loss: 1.351385] [G loss: 1.213360]\n",
      "821 [C loss: 1.341003] [G loss: 1.226781]\n",
      "822 [C loss: 1.339534] [G loss: 1.203529]\n",
      "823 [C loss: 1.345698] [G loss: 1.577108]\n",
      "824 [C loss: 1.320916] [G loss: 1.961824]\n",
      "825 [C loss: 1.354429] [G loss: 1.878229]\n",
      "826 [C loss: 1.336518] [G loss: 1.618948]\n",
      "827 [C loss: 1.364445] [G loss: 1.665126]\n",
      "828 [C loss: 1.314134] [G loss: 1.501757]\n",
      "829 [C loss: 1.335491] [G loss: 1.784666]\n",
      "830 [C loss: 1.355096] [G loss: 1.939173]\n",
      "831 [C loss: 1.353374] [G loss: 2.031979]\n",
      "832 [C loss: 1.342020] [G loss: 2.044326]\n",
      "833 [C loss: 1.335448] [G loss: 1.846253]\n",
      "834 [C loss: 1.359621] [G loss: 1.677035]\n",
      "835 [C loss: 1.339993] [G loss: 1.729057]\n",
      "836 [C loss: 1.345941] [G loss: 1.938465]\n",
      "837 [C loss: 1.334617] [G loss: 2.412208]\n",
      "838 [C loss: 1.360552] [G loss: 2.297635]\n",
      "839 [C loss: 1.311989] [G loss: 1.894274]\n",
      "840 [C loss: 1.335720] [G loss: 1.586321]\n",
      "841 [C loss: 1.318071] [G loss: 1.496619]\n",
      "842 [C loss: 1.309525] [G loss: 1.736623]\n",
      "843 [C loss: 1.335767] [G loss: 2.167279]\n",
      "844 [C loss: 1.338821] [G loss: 2.024983]\n",
      "845 [C loss: 1.318816] [G loss: 1.960847]\n",
      "846 [C loss: 1.300982] [G loss: 1.652131]\n",
      "847 [C loss: 1.337544] [G loss: 1.963691]\n",
      "848 [C loss: 1.362231] [G loss: 2.432195]\n",
      "849 [C loss: 1.364084] [G loss: 2.317669]\n",
      "850 [C loss: 1.338802] [G loss: 2.445302]\n",
      "851 [C loss: 1.306241] [G loss: 2.006142]\n",
      "852 [C loss: 1.313194] [G loss: 1.659800]\n",
      "853 [C loss: 1.339169] [G loss: 1.752557]\n",
      "854 [C loss: 1.338605] [G loss: 1.834012]\n",
      "855 [C loss: 1.336949] [G loss: 1.793459]\n",
      "856 [C loss: 1.316138] [G loss: 1.648868]\n",
      "857 [C loss: 1.324710] [G loss: 1.596649]\n",
      "858 [C loss: 1.355491] [G loss: 1.774766]\n",
      "859 [C loss: 1.320750] [G loss: 1.749656]\n",
      "860 [C loss: 1.344246] [G loss: 1.904978]\n",
      "861 [C loss: 1.317768] [G loss: 1.865986]\n",
      "862 [C loss: 1.318282] [G loss: 1.357904]\n",
      "863 [C loss: 1.345094] [G loss: 1.397881]\n",
      "864 [C loss: 1.333221] [G loss: 1.771021]\n",
      "865 [C loss: 1.325296] [G loss: 1.615157]\n",
      "866 [C loss: 1.302271] [G loss: 1.536280]\n",
      "867 [C loss: 1.311949] [G loss: 1.364732]\n",
      "868 [C loss: 1.326231] [G loss: 1.552522]\n",
      "869 [C loss: 1.329000] [G loss: 1.565413]\n",
      "870 [C loss: 1.301052] [G loss: 1.336913]\n",
      "871 [C loss: 1.354985] [G loss: 1.184144]\n",
      "872 [C loss: 1.339325] [G loss: 1.620830]\n",
      "873 [C loss: 1.340182] [G loss: 1.825984]\n",
      "874 [C loss: 1.337686] [G loss: 1.577357]\n",
      "875 [C loss: 1.329179] [G loss: 1.348584]\n",
      "876 [C loss: 1.347074] [G loss: 1.374220]\n",
      "877 [C loss: 1.344734] [G loss: 1.969467]\n",
      "878 [C loss: 1.327363] [G loss: 2.334211]\n",
      "879 [C loss: 1.318613] [G loss: 2.124670]\n",
      "880 [C loss: 1.316730] [G loss: 1.695856]\n",
      "881 [C loss: 1.357713] [G loss: 1.360938]\n",
      "882 [C loss: 1.342796] [G loss: 1.223753]\n",
      "883 [C loss: 1.325234] [G loss: 1.063564]\n",
      "884 [C loss: 1.317609] [G loss: 1.080258]\n",
      "885 [C loss: 1.314168] [G loss: 1.649743]\n",
      "886 [C loss: 1.297866] [G loss: 1.796424]\n",
      "887 [C loss: 1.302001] [G loss: 1.681709]\n",
      "888 [C loss: 1.309430] [G loss: 1.720690]\n",
      "889 [C loss: 1.343085] [G loss: 2.023716]\n",
      "890 [C loss: 1.318237] [G loss: 1.840612]\n",
      "891 [C loss: 1.315163] [G loss: 1.819528]\n",
      "892 [C loss: 1.319628] [G loss: 1.520670]\n",
      "893 [C loss: 1.315885] [G loss: 1.395782]\n",
      "894 [C loss: 1.304072] [G loss: 1.558001]\n",
      "895 [C loss: 1.327228] [G loss: 1.762987]\n",
      "896 [C loss: 1.348413] [G loss: 1.841398]\n",
      "897 [C loss: 1.324616] [G loss: 1.674177]\n",
      "898 [C loss: 1.316572] [G loss: 1.098716]\n",
      "899 [C loss: 1.321329] [G loss: 0.725786]\n",
      "900 [C loss: 1.306153] [G loss: 0.929608]\n",
      "901 [C loss: 1.316841] [G loss: 1.244751]\n",
      "902 [C loss: 1.306102] [G loss: 1.316919]\n",
      "903 [C loss: 1.287916] [G loss: 1.718431]\n",
      "904 [C loss: 1.306436] [G loss: 2.446774]\n",
      "905 [C loss: 1.320692] [G loss: 2.384060]\n",
      "906 [C loss: 1.294828] [G loss: 2.458631]\n",
      "907 [C loss: 1.326112] [G loss: 2.066549]\n",
      "908 [C loss: 1.317766] [G loss: 1.858582]\n",
      "909 [C loss: 1.315464] [G loss: 1.960283]\n",
      "910 [C loss: 1.294399] [G loss: 1.947863]\n",
      "911 [C loss: 1.297880] [G loss: 1.650742]\n",
      "912 [C loss: 1.305083] [G loss: 1.404438]\n",
      "913 [C loss: 1.315131] [G loss: 1.272647]\n",
      "914 [C loss: 1.303607] [G loss: 1.652013]\n",
      "915 [C loss: 1.296921] [G loss: 1.848563]\n",
      "916 [C loss: 1.331443] [G loss: 1.912441]\n",
      "917 [C loss: 1.307105] [G loss: 2.002716]\n",
      "918 [C loss: 1.293414] [G loss: 1.514048]\n",
      "919 [C loss: 1.307931] [G loss: 1.203786]\n",
      "920 [C loss: 1.340527] [G loss: 1.317827]\n",
      "921 [C loss: 1.325475] [G loss: 1.411094]\n",
      "922 [C loss: 1.331047] [G loss: 1.406589]\n",
      "923 [C loss: 1.329302] [G loss: 1.240896]\n",
      "924 [C loss: 1.326793] [G loss: 0.823576]\n",
      "925 [C loss: 1.309567] [G loss: 0.751614]\n",
      "926 [C loss: 1.277985] [G loss: 1.041191]\n",
      "927 [C loss: 1.285525] [G loss: 1.268618]\n",
      "928 [C loss: 1.305979] [G loss: 1.562509]\n",
      "929 [C loss: 1.322320] [G loss: 1.658734]\n",
      "930 [C loss: 1.314415] [G loss: 1.697200]\n",
      "931 [C loss: 1.293530] [G loss: 1.812256]\n",
      "932 [C loss: 1.290079] [G loss: 1.672046]\n",
      "933 [C loss: 1.318527] [G loss: 1.636438]\n",
      "934 [C loss: 1.286617] [G loss: 1.459311]\n",
      "935 [C loss: 1.301822] [G loss: 1.625027]\n",
      "936 [C loss: 1.302269] [G loss: 1.793195]\n",
      "937 [C loss: 1.311177] [G loss: 1.799304]\n",
      "938 [C loss: 1.307780] [G loss: 1.740060]\n",
      "939 [C loss: 1.303393] [G loss: 1.817206]\n",
      "940 [C loss: 1.293376] [G loss: 1.469860]\n",
      "941 [C loss: 1.292800] [G loss: 1.201483]\n",
      "942 [C loss: 1.307803] [G loss: 1.320309]\n",
      "943 [C loss: 1.295564] [G loss: 1.572411]\n",
      "944 [C loss: 1.334594] [G loss: 1.801668]\n",
      "945 [C loss: 1.319462] [G loss: 1.925418]\n",
      "946 [C loss: 1.320748] [G loss: 1.812434]\n",
      "947 [C loss: 1.314096] [G loss: 1.532791]\n",
      "948 [C loss: 1.310106] [G loss: 1.616206]\n",
      "949 [C loss: 1.311145] [G loss: 1.615502]\n",
      "950 [C loss: 1.288506] [G loss: 1.558233]\n",
      "951 [C loss: 1.309514] [G loss: 1.700818]\n",
      "952 [C loss: 1.303072] [G loss: 1.593018]\n",
      "953 [C loss: 1.330662] [G loss: 1.189369]\n",
      "954 [C loss: 1.310856] [G loss: 1.309880]\n",
      "955 [C loss: 1.325283] [G loss: 1.514282]\n",
      "956 [C loss: 1.319185] [G loss: 1.751787]\n",
      "957 [C loss: 1.301243] [G loss: 1.976447]\n",
      "958 [C loss: 1.282936] [G loss: 1.895929]\n",
      "959 [C loss: 1.330263] [G loss: 2.412565]\n",
      "960 [C loss: 1.322728] [G loss: 2.488973]\n",
      "961 [C loss: 1.291952] [G loss: 2.752293]\n",
      "962 [C loss: 1.300301] [G loss: 2.609858]\n",
      "963 [C loss: 1.299954] [G loss: 2.068398]\n",
      "964 [C loss: 1.301485] [G loss: 1.861806]\n",
      "965 [C loss: 1.313480] [G loss: 1.960468]\n",
      "966 [C loss: 1.319902] [G loss: 1.629692]\n",
      "967 [C loss: 1.287314] [G loss: 1.440026]\n",
      "968 [C loss: 1.298261] [G loss: 1.266698]\n",
      "969 [C loss: 1.329369] [G loss: 1.246456]\n",
      "970 [C loss: 1.315921] [G loss: 1.144785]\n",
      "971 [C loss: 1.294870] [G loss: 0.957138]\n",
      "972 [C loss: 1.306048] [G loss: 0.693190]\n",
      "973 [C loss: 1.332909] [G loss: 0.569252]\n",
      "974 [C loss: 1.315583] [G loss: 0.877927]\n",
      "975 [C loss: 1.309891] [G loss: 1.281576]\n",
      "976 [C loss: 1.324131] [G loss: 1.594644]\n",
      "977 [C loss: 1.323142] [G loss: 1.704904]\n",
      "978 [C loss: 1.286294] [G loss: 1.601701]\n",
      "979 [C loss: 1.330951] [G loss: 1.694172]\n",
      "980 [C loss: 1.302221] [G loss: 1.883185]\n",
      "981 [C loss: 1.289177] [G loss: 2.219307]\n",
      "982 [C loss: 1.302888] [G loss: 2.365383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 [C loss: 1.274167] [G loss: 2.008940]\n",
      "984 [C loss: 1.307014] [G loss: 2.092629]\n",
      "985 [C loss: 1.340194] [G loss: 2.284325]\n",
      "986 [C loss: 1.304825] [G loss: 2.238057]\n",
      "987 [C loss: 1.332509] [G loss: 1.739884]\n",
      "988 [C loss: 1.310427] [G loss: 1.411964]\n",
      "989 [C loss: 1.330900] [G loss: 1.398077]\n",
      "990 [C loss: 1.324478] [G loss: 1.365718]\n",
      "991 [C loss: 1.320834] [G loss: 1.543361]\n",
      "992 [C loss: 1.300075] [G loss: 1.245538]\n",
      "993 [C loss: 1.322408] [G loss: 1.257530]\n",
      "994 [C loss: 1.325397] [G loss: 1.223513]\n",
      "995 [C loss: 1.301423] [G loss: 1.555962]\n",
      "996 [C loss: 1.278483] [G loss: 1.878845]\n",
      "997 [C loss: 1.302545] [G loss: 1.934757]\n",
      "998 [C loss: 1.286581] [G loss: 1.844386]\n",
      "999 [C loss: 1.317577] [G loss: 2.006173]\n",
      "1000 [C loss: 1.332655] [G loss: 2.203289]\n",
      "1001 [C loss: 1.320765] [G loss: 1.707955]\n",
      "1002 [C loss: 1.314848] [G loss: 1.160808]\n",
      "1003 [C loss: 1.305682] [G loss: 1.488153]\n",
      "1004 [C loss: 1.293415] [G loss: 1.878467]\n",
      "1005 [C loss: 1.311964] [G loss: 2.210608]\n",
      "1006 [C loss: 1.271424] [G loss: 1.977275]\n",
      "1007 [C loss: 1.320664] [G loss: 1.875331]\n",
      "1008 [C loss: 1.315625] [G loss: 2.031925]\n",
      "1009 [C loss: 1.332660] [G loss: 2.129691]\n",
      "1010 [C loss: 1.308584] [G loss: 2.177685]\n",
      "1011 [C loss: 1.325311] [G loss: 2.121353]\n",
      "1012 [C loss: 1.318503] [G loss: 1.776160]\n",
      "1013 [C loss: 1.315836] [G loss: 1.395585]\n",
      "1014 [C loss: 1.311859] [G loss: 1.663950]\n",
      "1015 [C loss: 1.312719] [G loss: 1.771744]\n",
      "1016 [C loss: 1.297435] [G loss: 2.144844]\n",
      "1017 [C loss: 1.316997] [G loss: 2.238757]\n",
      "1018 [C loss: 1.288278] [G loss: 1.634186]\n",
      "1019 [C loss: 1.316029] [G loss: 1.685429]\n",
      "1020 [C loss: 1.307025] [G loss: 1.760244]\n",
      "1021 [C loss: 1.308755] [G loss: 2.128796]\n",
      "1022 [C loss: 1.303354] [G loss: 2.164339]\n",
      "1023 [C loss: 1.316476] [G loss: 1.896214]\n",
      "1024 [C loss: 1.310604] [G loss: 1.433975]\n",
      "1025 [C loss: 1.293929] [G loss: 1.450526]\n",
      "1026 [C loss: 1.308194] [G loss: 1.652281]\n",
      "1027 [C loss: 1.302914] [G loss: 1.582529]\n",
      "1028 [C loss: 1.280253] [G loss: 1.391906]\n",
      "1029 [C loss: 1.313693] [G loss: 1.118947]\n",
      "1030 [C loss: 1.312648] [G loss: 0.881988]\n",
      "1031 [C loss: 1.290203] [G loss: 1.060298]\n",
      "1032 [C loss: 1.317126] [G loss: 1.511603]\n",
      "1033 [C loss: 1.310889] [G loss: 1.851193]\n",
      "1034 [C loss: 1.334674] [G loss: 2.029486]\n",
      "1035 [C loss: 1.301094] [G loss: 2.066751]\n",
      "1036 [C loss: 1.291119] [G loss: 2.040719]\n",
      "1037 [C loss: 1.277030] [G loss: 1.921244]\n",
      "1038 [C loss: 1.288701] [G loss: 2.075142]\n",
      "1039 [C loss: 1.262774] [G loss: 2.160534]\n",
      "1040 [C loss: 1.282879] [G loss: 1.840164]\n",
      "1041 [C loss: 1.258754] [G loss: 1.704447]\n",
      "1042 [C loss: 1.276893] [G loss: 1.371606]\n",
      "1043 [C loss: 1.342910] [G loss: 1.209763]\n",
      "1044 [C loss: 1.314618] [G loss: 1.043759]\n",
      "1045 [C loss: 1.313776] [G loss: 0.787073]\n",
      "1046 [C loss: 1.323451] [G loss: 1.049389]\n",
      "1047 [C loss: 1.324222] [G loss: 1.023295]\n",
      "1048 [C loss: 1.325335] [G loss: 1.271987]\n",
      "1049 [C loss: 1.272792] [G loss: 1.200707]\n",
      "1050 [C loss: 1.280560] [G loss: 1.274959]\n",
      "1051 [C loss: 1.265985] [G loss: 1.466353]\n",
      "1052 [C loss: 1.307855] [G loss: 2.033495]\n",
      "1053 [C loss: 1.284193] [G loss: 2.535500]\n",
      "1054 [C loss: 1.295639] [G loss: 2.430522]\n",
      "1055 [C loss: 1.281772] [G loss: 2.373235]\n",
      "1056 [C loss: 1.284270] [G loss: 2.093152]\n",
      "1057 [C loss: 1.301989] [G loss: 1.748282]\n",
      "1058 [C loss: 1.301024] [G loss: 2.065377]\n",
      "1059 [C loss: 1.328853] [G loss: 2.103896]\n",
      "1060 [C loss: 1.311569] [G loss: 1.859857]\n",
      "1061 [C loss: 1.305898] [G loss: 1.831034]\n",
      "1062 [C loss: 1.289968] [G loss: 1.693442]\n",
      "1063 [C loss: 1.321761] [G loss: 2.035118]\n",
      "1064 [C loss: 1.299963] [G loss: 1.836412]\n",
      "1065 [C loss: 1.280404] [G loss: 1.570570]\n",
      "1066 [C loss: 1.279150] [G loss: 1.439635]\n",
      "1067 [C loss: 1.280710] [G loss: 1.598273]\n",
      "1068 [C loss: 1.282934] [G loss: 1.700413]\n",
      "1069 [C loss: 1.325253] [G loss: 2.032387]\n",
      "1070 [C loss: 1.302780] [G loss: 2.144857]\n",
      "1071 [C loss: 1.276173] [G loss: 1.269561]\n",
      "1072 [C loss: 1.296549] [G loss: 0.822043]\n",
      "1073 [C loss: 1.289873] [G loss: 0.901118]\n",
      "1074 [C loss: 1.278851] [G loss: 1.400550]\n",
      "1075 [C loss: 1.323236] [G loss: 2.273914]\n",
      "1076 [C loss: 1.292588] [G loss: 2.518445]\n",
      "1077 [C loss: 1.330902] [G loss: 2.343331]\n",
      "1078 [C loss: 1.280478] [G loss: 1.984690]\n",
      "1079 [C loss: 1.315562] [G loss: 1.776023]\n",
      "1080 [C loss: 1.303999] [G loss: 1.871992]\n",
      "1081 [C loss: 1.313993] [G loss: 2.352210]\n",
      "1082 [C loss: 1.300243] [G loss: 2.524823]\n",
      "1083 [C loss: 1.286770] [G loss: 2.306288]\n",
      "1084 [C loss: 1.265326] [G loss: 1.721031]\n",
      "1085 [C loss: 1.290200] [G loss: 1.168554]\n",
      "1086 [C loss: 1.302231] [G loss: 0.988626]\n",
      "1087 [C loss: 1.273015] [G loss: 1.311823]\n",
      "1088 [C loss: 1.308938] [G loss: 1.796578]\n",
      "1089 [C loss: 1.316537] [G loss: 2.060539]\n",
      "1090 [C loss: 1.288772] [G loss: 1.702206]\n",
      "1091 [C loss: 1.303223] [G loss: 1.260356]\n",
      "1092 [C loss: 1.285309] [G loss: 1.180587]\n",
      "1093 [C loss: 1.317202] [G loss: 1.385563]\n",
      "1094 [C loss: 1.306961] [G loss: 1.595729]\n",
      "1095 [C loss: 1.296494] [G loss: 1.671838]\n",
      "1096 [C loss: 1.303704] [G loss: 2.146227]\n",
      "1097 [C loss: 1.277287] [G loss: 2.020908]\n",
      "1098 [C loss: 1.280186] [G loss: 1.997808]\n",
      "1099 [C loss: 1.280814] [G loss: 2.204375]\n",
      "1100 [C loss: 1.308627] [G loss: 1.929483]\n",
      "1101 [C loss: 1.290944] [G loss: 1.666255]\n",
      "1102 [C loss: 1.271795] [G loss: 1.287804]\n",
      "1103 [C loss: 1.266945] [G loss: 1.466256]\n",
      "1104 [C loss: 1.281336] [G loss: 1.949980]\n",
      "1105 [C loss: 1.265519] [G loss: 2.078248]\n",
      "1106 [C loss: 1.274326] [G loss: 2.335069]\n",
      "1107 [C loss: 1.291248] [G loss: 2.459104]\n",
      "1108 [C loss: 1.289699] [G loss: 2.310678]\n",
      "1109 [C loss: 1.297927] [G loss: 2.077703]\n",
      "1110 [C loss: 1.282745] [G loss: 1.957630]\n",
      "1111 [C loss: 1.258058] [G loss: 1.779798]\n",
      "1112 [C loss: 1.280973] [G loss: 1.784627]\n",
      "1113 [C loss: 1.292898] [G loss: 1.390128]\n",
      "1114 [C loss: 1.275520] [G loss: 1.087982]\n",
      "1115 [C loss: 1.275964] [G loss: 1.540555]\n",
      "1116 [C loss: 1.276889] [G loss: 1.720855]\n",
      "1117 [C loss: 1.271178] [G loss: 1.617617]\n",
      "1118 [C loss: 1.273591] [G loss: 1.186324]\n",
      "1119 [C loss: 1.271140] [G loss: 1.108608]\n",
      "1120 [C loss: 1.265921] [G loss: 1.521261]\n",
      "1121 [C loss: 1.306394] [G loss: 1.761382]\n",
      "1122 [C loss: 1.264013] [G loss: 2.245771]\n",
      "1123 [C loss: 1.299667] [G loss: 2.131100]\n",
      "1124 [C loss: 1.279351] [G loss: 1.857972]\n",
      "1125 [C loss: 1.290735] [G loss: 1.717129]\n",
      "1126 [C loss: 1.307915] [G loss: 1.629318]\n",
      "1127 [C loss: 1.283237] [G loss: 1.568360]\n",
      "1128 [C loss: 1.306422] [G loss: 1.497276]\n",
      "1129 [C loss: 1.287503] [G loss: 1.879105]\n",
      "1130 [C loss: 1.312261] [G loss: 2.376627]\n",
      "1131 [C loss: 1.264336] [G loss: 2.231874]\n",
      "1132 [C loss: 1.277415] [G loss: 1.776606]\n",
      "1133 [C loss: 1.311079] [G loss: 1.492333]\n",
      "1134 [C loss: 1.276521] [G loss: 1.740453]\n",
      "1135 [C loss: 1.279725] [G loss: 2.135243]\n",
      "1136 [C loss: 1.275662] [G loss: 2.291784]\n",
      "1137 [C loss: 1.236466] [G loss: 2.004221]\n",
      "1138 [C loss: 1.263309] [G loss: 1.768961]\n",
      "1139 [C loss: 1.269368] [G loss: 1.688029]\n",
      "1140 [C loss: 1.283878] [G loss: 1.703255]\n",
      "1141 [C loss: 1.301075] [G loss: 1.299237]\n",
      "1142 [C loss: 1.303834] [G loss: 0.747785]\n",
      "1143 [C loss: 1.314125] [G loss: 0.524468]\n",
      "1144 [C loss: 1.284841] [G loss: 0.697977]\n",
      "1145 [C loss: 1.289940] [G loss: 1.105422]\n",
      "1146 [C loss: 1.278960] [G loss: 1.468403]\n",
      "1147 [C loss: 1.237084] [G loss: 1.743155]\n",
      "1148 [C loss: 1.252420] [G loss: 1.582540]\n",
      "1149 [C loss: 1.267246] [G loss: 1.350429]\n",
      "1150 [C loss: 1.268575] [G loss: 1.619702]\n",
      "1151 [C loss: 1.309242] [G loss: 1.747657]\n",
      "1152 [C loss: 1.303916] [G loss: 1.790700]\n",
      "1153 [C loss: 1.304347] [G loss: 1.799059]\n",
      "1154 [C loss: 1.286224] [G loss: 1.820133]\n",
      "1155 [C loss: 1.266744] [G loss: 1.523668]\n",
      "1156 [C loss: 1.270342] [G loss: 1.686756]\n",
      "1157 [C loss: 1.258299] [G loss: 1.654945]\n",
      "1158 [C loss: 1.262025] [G loss: 1.610535]\n",
      "1159 [C loss: 1.273288] [G loss: 1.848193]\n",
      "1160 [C loss: 1.272514] [G loss: 1.493048]\n",
      "1161 [C loss: 1.301970] [G loss: 1.251207]\n",
      "1162 [C loss: 1.274268] [G loss: 1.357401]\n",
      "1163 [C loss: 1.326960] [G loss: 1.510437]\n",
      "1164 [C loss: 1.285265] [G loss: 1.369405]\n",
      "1165 [C loss: 1.268104] [G loss: 1.224276]\n",
      "1166 [C loss: 1.288361] [G loss: 1.107502]\n",
      "1167 [C loss: 1.267535] [G loss: 1.608534]\n",
      "1168 [C loss: 1.252997] [G loss: 2.261541]\n",
      "1169 [C loss: 1.233752] [G loss: 2.462577]\n",
      "1170 [C loss: 1.250133] [G loss: 2.059804]\n",
      "1171 [C loss: 1.270731] [G loss: 1.715204]\n",
      "1172 [C loss: 1.281011] [G loss: 1.608962]\n",
      "1173 [C loss: 1.285877] [G loss: 1.771138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174 [C loss: 1.299992] [G loss: 1.901868]\n",
      "1175 [C loss: 1.275913] [G loss: 2.451016]\n",
      "1176 [C loss: 1.260097] [G loss: 2.194910]\n",
      "1177 [C loss: 1.272727] [G loss: 2.101171]\n",
      "1178 [C loss: 1.278755] [G loss: 1.822089]\n",
      "1179 [C loss: 1.259534] [G loss: 1.571430]\n",
      "1180 [C loss: 1.264562] [G loss: 1.752012]\n",
      "1181 [C loss: 1.275454] [G loss: 2.035580]\n",
      "1182 [C loss: 1.253853] [G loss: 1.630207]\n",
      "1183 [C loss: 1.277659] [G loss: 1.178162]\n",
      "1184 [C loss: 1.286110] [G loss: 0.675935]\n",
      "1185 [C loss: 1.277779] [G loss: 0.427232]\n",
      "1186 [C loss: 1.242798] [G loss: 0.679443]\n",
      "1187 [C loss: 1.266471] [G loss: 1.102147]\n",
      "1188 [C loss: 1.257669] [G loss: 1.635063]\n",
      "1189 [C loss: 1.254506] [G loss: 1.631006]\n",
      "1190 [C loss: 1.247322] [G loss: 1.487933]\n",
      "1191 [C loss: 1.270123] [G loss: 1.472587]\n",
      "1192 [C loss: 1.270038] [G loss: 1.434884]\n",
      "1193 [C loss: 1.290365] [G loss: 1.866904]\n",
      "1194 [C loss: 1.297720] [G loss: 2.218270]\n",
      "1195 [C loss: 1.271752] [G loss: 2.269550]\n",
      "1196 [C loss: 1.279768] [G loss: 1.530436]\n",
      "1197 [C loss: 1.291214] [G loss: 1.206318]\n",
      "1198 [C loss: 1.269799] [G loss: 1.379581]\n",
      "1199 [C loss: 1.247420] [G loss: 1.949723]\n",
      "1200 [C loss: 1.285991] [G loss: 2.091539]\n",
      "1201 [C loss: 1.250893] [G loss: 1.868617]\n",
      "1202 [C loss: 1.259485] [G loss: 1.167314]\n",
      "1203 [C loss: 1.302507] [G loss: 0.417127]\n",
      "1204 [C loss: 1.283576] [G loss: 0.500226]\n",
      "1205 [C loss: 1.300868] [G loss: 0.823176]\n",
      "1206 [C loss: 1.281355] [G loss: 0.882965]\n",
      "1207 [C loss: 1.269592] [G loss: 0.963251]\n",
      "1208 [C loss: 1.249051] [G loss: 0.878267]\n",
      "1209 [C loss: 1.268692] [G loss: 1.069727]\n",
      "1210 [C loss: 1.249103] [G loss: 1.346168]\n",
      "1211 [C loss: 1.260733] [G loss: 1.393792]\n",
      "1212 [C loss: 1.261561] [G loss: 1.129705]\n",
      "1213 [C loss: 1.276683] [G loss: 1.456735]\n",
      "1214 [C loss: 1.271339] [G loss: 1.573193]\n",
      "1215 [C loss: 1.251573] [G loss: 1.307902]\n",
      "1216 [C loss: 1.269485] [G loss: 1.015057]\n",
      "1217 [C loss: 1.264979] [G loss: 1.069824]\n",
      "1218 [C loss: 1.251529] [G loss: 1.449902]\n",
      "1219 [C loss: 1.248856] [G loss: 1.475731]\n",
      "1220 [C loss: 1.262289] [G loss: 1.101755]\n",
      "1221 [C loss: 1.260512] [G loss: 0.800743]\n",
      "1222 [C loss: 1.262173] [G loss: 0.977069]\n",
      "1223 [C loss: 1.293394] [G loss: 1.606086]\n",
      "1224 [C loss: 1.290413] [G loss: 1.635021]\n",
      "1225 [C loss: 1.276062] [G loss: 1.176157]\n",
      "1226 [C loss: 1.237651] [G loss: 1.355137]\n",
      "1227 [C loss: 1.261954] [G loss: 1.610007]\n",
      "1228 [C loss: 1.243859] [G loss: 1.991111]\n",
      "1229 [C loss: 1.262101] [G loss: 2.141316]\n",
      "1230 [C loss: 1.265845] [G loss: 2.206080]\n",
      "1231 [C loss: 1.261542] [G loss: 2.004259]\n",
      "1232 [C loss: 1.244186] [G loss: 1.608315]\n",
      "1233 [C loss: 1.273397] [G loss: 1.162974]\n",
      "1234 [C loss: 1.282370] [G loss: 1.033007]\n",
      "1235 [C loss: 1.271119] [G loss: 1.277768]\n",
      "1236 [C loss: 1.277052] [G loss: 1.196392]\n",
      "1237 [C loss: 1.257369] [G loss: 0.717135]\n",
      "1238 [C loss: 1.248099] [G loss: 0.659990]\n",
      "1239 [C loss: 1.271449] [G loss: 0.505152]\n",
      "1240 [C loss: 1.248387] [G loss: 0.722116]\n",
      "1241 [C loss: 1.276347] [G loss: 0.707054]\n",
      "1242 [C loss: 1.276083] [G loss: 0.761126]\n",
      "1243 [C loss: 1.305392] [G loss: 0.949555]\n",
      "1244 [C loss: 1.256494] [G loss: 1.052370]\n",
      "1245 [C loss: 1.277688] [G loss: 0.983181]\n",
      "1246 [C loss: 1.245310] [G loss: 1.277516]\n",
      "1247 [C loss: 1.278232] [G loss: 1.490938]\n",
      "1248 [C loss: 1.268797] [G loss: 1.684939]\n",
      "1249 [C loss: 1.256202] [G loss: 1.692748]\n",
      "1250 [C loss: 1.259145] [G loss: 1.772340]\n",
      "1251 [C loss: 1.301260] [G loss: 1.906401]\n",
      "1252 [C loss: 1.266207] [G loss: 1.983665]\n",
      "1253 [C loss: 1.249899] [G loss: 1.268848]\n",
      "1254 [C loss: 1.277094] [G loss: 0.802056]\n",
      "1255 [C loss: 1.279164] [G loss: 0.849781]\n",
      "1256 [C loss: 1.282669] [G loss: 0.636668]\n",
      "1257 [C loss: 1.280036] [G loss: 0.632359]\n",
      "1258 [C loss: 1.264353] [G loss: 0.592480]\n",
      "1259 [C loss: 1.266640] [G loss: 0.593843]\n",
      "1260 [C loss: 1.288676] [G loss: 0.558709]\n",
      "1261 [C loss: 1.276257] [G loss: 0.618229]\n",
      "1262 [C loss: 1.269948] [G loss: 1.094075]\n",
      "1263 [C loss: 1.287030] [G loss: 1.816769]\n",
      "1264 [C loss: 1.267601] [G loss: 1.946423]\n",
      "1265 [C loss: 1.249901] [G loss: 2.077848]\n",
      "1266 [C loss: 1.250048] [G loss: 1.336810]\n",
      "1267 [C loss: 1.263521] [G loss: 0.949916]\n",
      "1268 [C loss: 1.277518] [G loss: 0.832801]\n",
      "1269 [C loss: 1.307450] [G loss: 0.940500]\n",
      "1270 [C loss: 1.304832] [G loss: 1.080087]\n",
      "1271 [C loss: 1.289940] [G loss: 0.772371]\n",
      "1272 [C loss: 1.268769] [G loss: 0.662662]\n",
      "1273 [C loss: 1.269691] [G loss: 0.784180]\n",
      "1274 [C loss: 1.254819] [G loss: 0.947910]\n",
      "1275 [C loss: 1.275050] [G loss: 1.143142]\n",
      "1276 [C loss: 1.257852] [G loss: 1.137295]\n",
      "1277 [C loss: 1.281403] [G loss: 0.732158]\n",
      "1278 [C loss: 1.288474] [G loss: 0.162065]\n",
      "1279 [C loss: 1.304542] [G loss: 0.306595]\n",
      "1280 [C loss: 1.288796] [G loss: 0.538973]\n",
      "1281 [C loss: 1.269330] [G loss: 0.652884]\n",
      "1282 [C loss: 1.308811] [G loss: 0.980264]\n",
      "1283 [C loss: 1.284525] [G loss: 1.273601]\n",
      "1284 [C loss: 1.265346] [G loss: 0.713791]\n",
      "1285 [C loss: 1.232832] [G loss: 0.569498]\n",
      "1286 [C loss: 1.239887] [G loss: 0.916690]\n",
      "1287 [C loss: 1.273903] [G loss: 1.428140]\n",
      "1288 [C loss: 1.253887] [G loss: 1.285553]\n",
      "1289 [C loss: 1.273209] [G loss: 1.179982]\n",
      "1290 [C loss: 1.290460] [G loss: 1.092389]\n",
      "1291 [C loss: 1.273125] [G loss: 1.079155]\n",
      "1292 [C loss: 1.282425] [G loss: 0.783827]\n",
      "1293 [C loss: 1.274115] [G loss: 0.638608]\n",
      "1294 [C loss: 1.263007] [G loss: 0.683600]\n",
      "1295 [C loss: 1.267421] [G loss: 1.019086]\n",
      "1296 [C loss: 1.272261] [G loss: 1.112079]\n",
      "1297 [C loss: 1.269737] [G loss: 0.881601]\n",
      "1298 [C loss: 1.263144] [G loss: 0.973623]\n",
      "1299 [C loss: 1.261118] [G loss: 0.944488]\n",
      "1300 [C loss: 1.253685] [G loss: 0.585304]\n",
      "1301 [C loss: 1.292943] [G loss: 0.825765]\n",
      "1302 [C loss: 1.245120] [G loss: 1.163500]\n",
      "1303 [C loss: 1.289852] [G loss: 0.884695]\n",
      "1304 [C loss: 1.278894] [G loss: 0.173643]\n",
      "1305 [C loss: 1.264281] [G loss: -0.090731]\n",
      "1306 [C loss: 1.278434] [G loss: 0.279283]\n",
      "1307 [C loss: 1.275544] [G loss: 0.846054]\n",
      "1308 [C loss: 1.260647] [G loss: 1.070669]\n",
      "1309 [C loss: 1.224465] [G loss: 0.733911]\n",
      "1310 [C loss: 1.270155] [G loss: 0.224148]\n",
      "1311 [C loss: 1.259192] [G loss: 0.240458]\n",
      "1312 [C loss: 1.254705] [G loss: 0.552603]\n",
      "1313 [C loss: 1.269947] [G loss: 1.008538]\n",
      "1314 [C loss: 1.269846] [G loss: 1.394340]\n",
      "1315 [C loss: 1.267894] [G loss: 1.511935]\n",
      "1316 [C loss: 1.242111] [G loss: 1.152605]\n",
      "1317 [C loss: 1.271346] [G loss: 0.442549]\n",
      "1318 [C loss: 1.267740] [G loss: -0.021712]\n",
      "1319 [C loss: 1.272235] [G loss: 0.020332]\n",
      "1320 [C loss: 1.278171] [G loss: 0.482252]\n",
      "1321 [C loss: 1.269513] [G loss: 0.607338]\n",
      "1322 [C loss: 1.276269] [G loss: 0.164444]\n",
      "1323 [C loss: 1.280245] [G loss: -0.200534]\n",
      "1324 [C loss: 1.257509] [G loss: 0.050495]\n",
      "1325 [C loss: 1.269113] [G loss: 0.757178]\n",
      "1326 [C loss: 1.281927] [G loss: 1.297502]\n",
      "1327 [C loss: 1.252867] [G loss: 1.504575]\n",
      "1328 [C loss: 1.265146] [G loss: 1.405645]\n",
      "1329 [C loss: 1.276230] [G loss: 1.026020]\n",
      "1330 [C loss: 1.281185] [G loss: 0.481677]\n",
      "1331 [C loss: 1.281019] [G loss: 0.293973]\n",
      "1332 [C loss: 1.272855] [G loss: 0.219518]\n",
      "1333 [C loss: 1.255294] [G loss: 0.422900]\n",
      "1334 [C loss: 1.264528] [G loss: 0.417912]\n",
      "1335 [C loss: 1.269473] [G loss: 0.751100]\n",
      "1336 [C loss: 1.266503] [G loss: 0.511188]\n",
      "1337 [C loss: 1.242456] [G loss: 0.695042]\n",
      "1338 [C loss: 1.259493] [G loss: 0.820517]\n",
      "1339 [C loss: 1.259768] [G loss: 1.095460]\n",
      "1340 [C loss: 1.267831] [G loss: 1.573438]\n",
      "1341 [C loss: 1.274879] [G loss: 1.465992]\n",
      "1342 [C loss: 1.273937] [G loss: 1.502759]\n",
      "1343 [C loss: 1.260807] [G loss: 1.099753]\n",
      "1344 [C loss: 1.245733] [G loss: 0.721633]\n",
      "1345 [C loss: 1.264431] [G loss: 0.912296]\n",
      "1346 [C loss: 1.263258] [G loss: 1.085901]\n",
      "1347 [C loss: 1.292199] [G loss: 1.380149]\n",
      "1348 [C loss: 1.281437] [G loss: 0.918274]\n",
      "1349 [C loss: 1.279014] [G loss: 0.562084]\n",
      "1350 [C loss: 1.262906] [G loss: 0.286170]\n",
      "1351 [C loss: 1.290052] [G loss: 0.637140]\n",
      "1352 [C loss: 1.270795] [G loss: 0.726318]\n",
      "1353 [C loss: 1.265562] [G loss: 0.289918]\n",
      "1354 [C loss: 1.267196] [G loss: -0.157009]\n",
      "1355 [C loss: 1.274887] [G loss: 0.128421]\n",
      "1356 [C loss: 1.266088] [G loss: 0.844773]\n",
      "1357 [C loss: 1.259865] [G loss: 1.456122]\n",
      "1358 [C loss: 1.257546] [G loss: 1.546465]\n",
      "1359 [C loss: 1.254678] [G loss: 0.906217]\n",
      "1360 [C loss: 1.273846] [G loss: 0.495432]\n",
      "1361 [C loss: 1.256063] [G loss: 0.391290]\n",
      "1362 [C loss: 1.257257] [G loss: 0.559689]\n",
      "1363 [C loss: 1.275695] [G loss: 0.906184]\n",
      "1364 [C loss: 1.259384] [G loss: 1.234180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 [C loss: 1.249857] [G loss: 1.067124]\n",
      "1366 [C loss: 1.278222] [G loss: 0.638967]\n",
      "1367 [C loss: 1.282566] [G loss: 0.367042]\n",
      "1368 [C loss: 1.255862] [G loss: 0.559400]\n",
      "1369 [C loss: 1.251823] [G loss: 0.823879]\n",
      "1370 [C loss: 1.268703] [G loss: 1.206750]\n",
      "1371 [C loss: 1.265669] [G loss: 1.338829]\n",
      "1372 [C loss: 1.233851] [G loss: 1.086924]\n",
      "1373 [C loss: 1.272264] [G loss: 0.767697]\n",
      "1374 [C loss: 1.255864] [G loss: 0.719273]\n",
      "1375 [C loss: 1.280577] [G loss: 0.705366]\n",
      "1376 [C loss: 1.268217] [G loss: 0.837807]\n",
      "1377 [C loss: 1.242329] [G loss: 0.556298]\n",
      "1378 [C loss: 1.240272] [G loss: 0.473217]\n",
      "1379 [C loss: 1.277221] [G loss: 0.409638]\n",
      "1380 [C loss: 1.271544] [G loss: 0.333308]\n",
      "1381 [C loss: 1.270775] [G loss: 0.366484]\n",
      "1382 [C loss: 1.269527] [G loss: 0.227499]\n",
      "1383 [C loss: 1.261486] [G loss: 0.209400]\n",
      "1384 [C loss: 1.271037] [G loss: 0.376904]\n",
      "1385 [C loss: 1.243034] [G loss: 0.546865]\n",
      "1386 [C loss: 1.263528] [G loss: 0.517000]\n",
      "1387 [C loss: 1.253702] [G loss: 0.589859]\n",
      "1388 [C loss: 1.263511] [G loss: 0.955098]\n",
      "1389 [C loss: 1.251524] [G loss: 0.860400]\n",
      "1390 [C loss: 1.256770] [G loss: 0.887744]\n",
      "1391 [C loss: 1.280297] [G loss: 0.724412]\n",
      "1392 [C loss: 1.253155] [G loss: 0.468766]\n",
      "1393 [C loss: 1.291042] [G loss: 0.667098]\n",
      "1394 [C loss: 1.278814] [G loss: 0.880423]\n",
      "1395 [C loss: 1.243582] [G loss: 0.519932]\n",
      "1396 [C loss: 1.222485] [G loss: 0.281790]\n",
      "1397 [C loss: 1.251837] [G loss: 0.411157]\n",
      "1398 [C loss: 1.255510] [G loss: 0.875936]\n",
      "1399 [C loss: 1.269679] [G loss: 0.620458]\n",
      "1400 [C loss: 1.288290] [G loss: 0.236251]\n",
      "1401 [C loss: 1.299557] [G loss: 0.141285]\n",
      "1402 [C loss: 1.272642] [G loss: -0.105489]\n",
      "1403 [C loss: 1.262105] [G loss: 0.059502]\n",
      "1404 [C loss: 1.253855] [G loss: 0.207016]\n",
      "1405 [C loss: 1.255197] [G loss: 0.239310]\n",
      "1406 [C loss: 1.242406] [G loss: 0.665491]\n",
      "1407 [C loss: 1.221868] [G loss: 0.536474]\n",
      "1408 [C loss: 1.275589] [G loss: 0.677329]\n",
      "1409 [C loss: 1.258164] [G loss: 0.582968]\n",
      "1410 [C loss: 1.270734] [G loss: 0.572013]\n",
      "1411 [C loss: 1.281666] [G loss: 0.620123]\n",
      "1412 [C loss: 1.254304] [G loss: 0.712267]\n",
      "1413 [C loss: 1.246379] [G loss: 0.584280]\n",
      "1414 [C loss: 1.269149] [G loss: 0.092927]\n",
      "1415 [C loss: 1.248170] [G loss: 0.277890]\n",
      "1416 [C loss: 1.241525] [G loss: 0.691379]\n",
      "1417 [C loss: 1.240134] [G loss: 1.099745]\n",
      "1418 [C loss: 1.269299] [G loss: 1.194567]\n",
      "1419 [C loss: 1.251648] [G loss: 0.881154]\n",
      "1420 [C loss: 1.271232] [G loss: 0.310352]\n",
      "1421 [C loss: 1.257594] [G loss: -0.155874]\n",
      "1422 [C loss: 1.267570] [G loss: -0.148410]\n",
      "1423 [C loss: 1.262880] [G loss: 0.482567]\n",
      "1424 [C loss: 1.252805] [G loss: 0.663073]\n",
      "1425 [C loss: 1.242577] [G loss: 0.726570]\n",
      "1426 [C loss: 1.249380] [G loss: 0.537263]\n",
      "1427 [C loss: 1.254252] [G loss: 0.430050]\n",
      "1428 [C loss: 1.253836] [G loss: 0.749714]\n",
      "1429 [C loss: 1.256584] [G loss: 1.124034]\n",
      "1430 [C loss: 1.252403] [G loss: 1.095646]\n",
      "1431 [C loss: 1.249751] [G loss: 0.871014]\n",
      "1432 [C loss: 1.268686] [G loss: 0.882548]\n",
      "1433 [C loss: 1.260543] [G loss: 0.609428]\n",
      "1434 [C loss: 1.257333] [G loss: 0.324894]\n",
      "1435 [C loss: 1.261078] [G loss: 0.437796]\n",
      "1436 [C loss: 1.233946] [G loss: 0.504053]\n",
      "1437 [C loss: 1.228186] [G loss: 0.799109]\n",
      "1438 [C loss: 1.247033] [G loss: 0.731529]\n",
      "1439 [C loss: 1.272448] [G loss: 0.260165]\n",
      "1440 [C loss: 1.267912] [G loss: -0.110767]\n",
      "1441 [C loss: 1.268201] [G loss: -0.199029]\n",
      "1442 [C loss: 1.257423] [G loss: 0.011899]\n",
      "1443 [C loss: 1.254681] [G loss: 0.131635]\n",
      "1444 [C loss: 1.239800] [G loss: 0.097579]\n",
      "1445 [C loss: 1.238091] [G loss: 0.574854]\n",
      "1446 [C loss: 1.246136] [G loss: 0.586371]\n",
      "1447 [C loss: 1.254812] [G loss: 0.735148]\n",
      "1448 [C loss: 1.258612] [G loss: 0.812067]\n",
      "1449 [C loss: 1.275146] [G loss: 0.790673]\n",
      "1450 [C loss: 1.291545] [G loss: 0.855031]\n",
      "1451 [C loss: 1.264250] [G loss: 0.835135]\n",
      "1452 [C loss: 1.245863] [G loss: 0.733146]\n",
      "1453 [C loss: 1.247984] [G loss: 0.373030]\n",
      "1454 [C loss: 1.254688] [G loss: -0.082406]\n",
      "1455 [C loss: 1.250244] [G loss: -0.353021]\n",
      "1456 [C loss: 1.251477] [G loss: -0.241268]\n",
      "1457 [C loss: 1.275974] [G loss: -0.213365]\n",
      "1458 [C loss: 1.263998] [G loss: -0.494113]\n",
      "1459 [C loss: 1.258440] [G loss: -0.436985]\n",
      "1460 [C loss: 1.254770] [G loss: -0.355358]\n",
      "1461 [C loss: 1.251195] [G loss: -0.169744]\n",
      "1462 [C loss: 1.263144] [G loss: 0.318682]\n",
      "1463 [C loss: 1.241044] [G loss: 0.305217]\n",
      "1464 [C loss: 1.255660] [G loss: 0.119459]\n",
      "1465 [C loss: 1.239439] [G loss: 0.110032]\n",
      "1466 [C loss: 1.242717] [G loss: 0.111459]\n",
      "1467 [C loss: 1.269845] [G loss: 0.490992]\n",
      "1468 [C loss: 1.293195] [G loss: 0.879550]\n",
      "1469 [C loss: 1.240947] [G loss: 0.931497]\n",
      "1470 [C loss: 1.256023] [G loss: 0.708449]\n",
      "1471 [C loss: 1.265885] [G loss: 0.109217]\n",
      "1472 [C loss: 1.285897] [G loss: -0.183107]\n",
      "1473 [C loss: 1.250100] [G loss: -0.203239]\n",
      "1474 [C loss: 1.243069] [G loss: 0.098197]\n",
      "1475 [C loss: 1.257977] [G loss: 0.335962]\n",
      "1476 [C loss: 1.249692] [G loss: 0.629620]\n",
      "1477 [C loss: 1.254064] [G loss: 0.684123]\n",
      "1478 [C loss: 1.246858] [G loss: 0.305716]\n",
      "1479 [C loss: 1.259940] [G loss: 0.010069]\n",
      "1480 [C loss: 1.285668] [G loss: -0.490055]\n",
      "1481 [C loss: 1.256790] [G loss: -0.160084]\n",
      "1482 [C loss: 1.278831] [G loss: 0.508102]\n",
      "1483 [C loss: 1.281935] [G loss: 1.189224]\n",
      "1484 [C loss: 1.239172] [G loss: 1.078164]\n",
      "1485 [C loss: 1.257172] [G loss: 0.303338]\n",
      "1486 [C loss: 1.246398] [G loss: -0.204609]\n",
      "1487 [C loss: 1.236244] [G loss: -0.467458]\n",
      "1488 [C loss: 1.242786] [G loss: -0.076576]\n",
      "1489 [C loss: 1.260858] [G loss: 0.530150]\n",
      "1490 [C loss: 1.255546] [G loss: 0.598283]\n",
      "1491 [C loss: 1.240913] [G loss: 0.415282]\n",
      "1492 [C loss: 1.261186] [G loss: -0.106254]\n",
      "1493 [C loss: 1.268675] [G loss: -0.412753]\n",
      "1494 [C loss: 1.236782] [G loss: -0.011662]\n",
      "1495 [C loss: 1.262332] [G loss: 0.601723]\n",
      "1496 [C loss: 1.270854] [G loss: 1.376029]\n",
      "1497 [C loss: 1.226634] [G loss: 1.601780]\n",
      "1498 [C loss: 1.251928] [G loss: 1.057856]\n",
      "1499 [C loss: 1.268975] [G loss: 0.524728]\n",
      "1500 [C loss: 1.260051] [G loss: 0.430140]\n",
      "1501 [C loss: 1.259367] [G loss: 0.740667]\n",
      "1502 [C loss: 1.251364] [G loss: 1.139395]\n",
      "1503 [C loss: 1.254597] [G loss: 0.944597]\n",
      "1504 [C loss: 1.270560] [G loss: 0.182757]\n",
      "1505 [C loss: 1.246227] [G loss: -0.429530]\n",
      "1506 [C loss: 1.258495] [G loss: -0.251451]\n",
      "1507 [C loss: 1.259466] [G loss: 0.022615]\n",
      "1508 [C loss: 1.266898] [G loss: 0.239106]\n",
      "1509 [C loss: 1.253895] [G loss: 0.007182]\n",
      "1510 [C loss: 1.237245] [G loss: -0.102426]\n",
      "1511 [C loss: 1.251445] [G loss: 0.073142]\n",
      "1512 [C loss: 1.239882] [G loss: 0.017785]\n",
      "1513 [C loss: 1.264951] [G loss: 0.207404]\n",
      "1514 [C loss: 1.256945] [G loss: 0.357697]\n",
      "1515 [C loss: 1.244095] [G loss: 0.091981]\n",
      "1516 [C loss: 1.253936] [G loss: -0.164530]\n",
      "1517 [C loss: 1.267330] [G loss: -0.330805]\n",
      "1518 [C loss: 1.245317] [G loss: -0.474453]\n",
      "1519 [C loss: 1.262043] [G loss: -0.401573]\n",
      "1520 [C loss: 1.281002] [G loss: -0.605046]\n",
      "1521 [C loss: 1.250219] [G loss: -0.735385]\n",
      "1522 [C loss: 1.232786] [G loss: -0.449810]\n",
      "1523 [C loss: 1.241748] [G loss: -0.083099]\n",
      "1524 [C loss: 1.240217] [G loss: 0.067739]\n",
      "1525 [C loss: 1.255843] [G loss: 0.370137]\n",
      "1526 [C loss: 1.228642] [G loss: 0.430937]\n",
      "1527 [C loss: 1.250766] [G loss: 0.318068]\n",
      "1528 [C loss: 1.241135] [G loss: 0.230320]\n",
      "1529 [C loss: 1.242630] [G loss: 0.294396]\n",
      "1530 [C loss: 1.248905] [G loss: 0.608297]\n",
      "1531 [C loss: 1.243793] [G loss: 0.637603]\n",
      "1532 [C loss: 1.232635] [G loss: 0.840718]\n",
      "1533 [C loss: 1.249131] [G loss: 0.549828]\n",
      "1534 [C loss: 1.242418] [G loss: 0.086874]\n",
      "1535 [C loss: 1.240749] [G loss: -0.192414]\n",
      "1536 [C loss: 1.243693] [G loss: -0.003347]\n",
      "1537 [C loss: 1.272234] [G loss: 0.270141]\n",
      "1538 [C loss: 1.253451] [G loss: 0.362526]\n",
      "1539 [C loss: 1.217524] [G loss: 0.012794]\n",
      "1540 [C loss: 1.278821] [G loss: -0.733028]\n",
      "1541 [C loss: 1.259116] [G loss: -1.009176]\n",
      "1542 [C loss: 1.239568] [G loss: -0.702315]\n",
      "1543 [C loss: 1.279295] [G loss: -0.376054]\n",
      "1544 [C loss: 1.259047] [G loss: 0.239280]\n",
      "1545 [C loss: 1.223829] [G loss: 0.022136]\n",
      "1546 [C loss: 1.258006] [G loss: -0.129157]\n",
      "1547 [C loss: 1.271782] [G loss: -0.106336]\n",
      "1548 [C loss: 1.264954] [G loss: 0.337565]\n",
      "1549 [C loss: 1.249180] [G loss: 0.465089]\n",
      "1550 [C loss: 1.245285] [G loss: 0.485315]\n",
      "1551 [C loss: 1.241975] [G loss: 0.042205]\n",
      "1552 [C loss: 1.246527] [G loss: -0.024635]\n",
      "1553 [C loss: 1.258064] [G loss: 0.108837]\n",
      "1554 [C loss: 1.255632] [G loss: -0.209988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1555 [C loss: 1.261433] [G loss: -0.319543]\n",
      "1556 [C loss: 1.242692] [G loss: -0.057023]\n",
      "1557 [C loss: 1.236184] [G loss: 0.299924]\n",
      "1558 [C loss: 1.259054] [G loss: 0.516121]\n",
      "1559 [C loss: 1.261185] [G loss: 0.440444]\n",
      "1560 [C loss: 1.242345] [G loss: 0.349312]\n",
      "1561 [C loss: 1.246318] [G loss: 0.371190]\n",
      "1562 [C loss: 1.240106] [G loss: 0.348854]\n",
      "1563 [C loss: 1.252866] [G loss: 0.354764]\n",
      "1564 [C loss: 1.257700] [G loss: 0.044177]\n",
      "1565 [C loss: 1.241851] [G loss: -0.355577]\n",
      "1566 [C loss: 1.257313] [G loss: -0.354984]\n",
      "1567 [C loss: 1.234036] [G loss: -0.236108]\n",
      "1568 [C loss: 1.270057] [G loss: 0.118476]\n",
      "1569 [C loss: 1.269258] [G loss: 0.319454]\n",
      "1570 [C loss: 1.280058] [G loss: -0.121546]\n",
      "1571 [C loss: 1.257972] [G loss: -0.514189]\n",
      "1572 [C loss: 1.234417] [G loss: -0.466691]\n",
      "1573 [C loss: 1.251842] [G loss: -0.273011]\n",
      "1574 [C loss: 1.259182] [G loss: 0.032568]\n",
      "1575 [C loss: 1.249970] [G loss: -0.005374]\n",
      "1576 [C loss: 1.236617] [G loss: 0.133518]\n",
      "1577 [C loss: 1.233888] [G loss: -0.178324]\n",
      "1578 [C loss: 1.227483] [G loss: -0.267172]\n",
      "1579 [C loss: 1.243359] [G loss: -0.264439]\n",
      "1580 [C loss: 1.252933] [G loss: 0.246446]\n",
      "1581 [C loss: 1.250693] [G loss: 0.490378]\n",
      "1582 [C loss: 1.244931] [G loss: 0.524207]\n",
      "1583 [C loss: 1.252670] [G loss: 0.320977]\n",
      "1584 [C loss: 1.225603] [G loss: 0.038477]\n",
      "1585 [C loss: 1.234715] [G loss: -0.177958]\n",
      "1586 [C loss: 1.231386] [G loss: 0.032810]\n",
      "1587 [C loss: 1.240454] [G loss: 0.465950]\n",
      "1588 [C loss: 1.239007] [G loss: 0.743552]\n",
      "1589 [C loss: 1.236053] [G loss: 0.688973]\n",
      "1590 [C loss: 1.237332] [G loss: 0.642111]\n",
      "1591 [C loss: 1.243311] [G loss: 0.682219]\n",
      "1592 [C loss: 1.241445] [G loss: 0.200924]\n",
      "1593 [C loss: 1.224419] [G loss: 0.171604]\n",
      "1594 [C loss: 1.230764] [G loss: 0.104956]\n",
      "1595 [C loss: 1.230261] [G loss: 0.172945]\n",
      "1596 [C loss: 1.240031] [G loss: 0.449599]\n",
      "1597 [C loss: 1.218317] [G loss: 0.620528]\n",
      "1598 [C loss: 1.244631] [G loss: 0.452753]\n",
      "1599 [C loss: 1.237918] [G loss: 0.421468]\n",
      "1600 [C loss: 1.238010] [G loss: 0.447852]\n",
      "1601 [C loss: 1.248688] [G loss: 0.468185]\n",
      "1602 [C loss: 1.241127] [G loss: 0.281816]\n",
      "1603 [C loss: 1.231370] [G loss: 0.110268]\n",
      "1604 [C loss: 1.259312] [G loss: -0.023956]\n",
      "1605 [C loss: 1.244112] [G loss: 0.018255]\n",
      "1606 [C loss: 1.249371] [G loss: 0.514900]\n",
      "1607 [C loss: 1.252431] [G loss: 0.823937]\n",
      "1608 [C loss: 1.243503] [G loss: 0.918110]\n",
      "1609 [C loss: 1.269584] [G loss: 0.448011]\n",
      "1610 [C loss: 1.233015] [G loss: 0.612113]\n",
      "1611 [C loss: 1.252407] [G loss: 1.013048]\n",
      "1612 [C loss: 1.237091] [G loss: 1.056741]\n",
      "1613 [C loss: 1.236156] [G loss: 0.954433]\n",
      "1614 [C loss: 1.241909] [G loss: 0.421458]\n",
      "1615 [C loss: 1.257358] [G loss: -0.130883]\n",
      "1616 [C loss: 1.259718] [G loss: -0.187144]\n",
      "1617 [C loss: 1.257147] [G loss: 0.048266]\n",
      "1618 [C loss: 1.247395] [G loss: 0.363142]\n",
      "1619 [C loss: 1.246434] [G loss: 0.534429]\n",
      "1620 [C loss: 1.244830] [G loss: 0.428927]\n",
      "1621 [C loss: 1.243854] [G loss: 0.429410]\n",
      "1622 [C loss: 1.236714] [G loss: 0.268494]\n",
      "1623 [C loss: 1.263401] [G loss: 0.161706]\n",
      "1624 [C loss: 1.237992] [G loss: 0.115021]\n",
      "1625 [C loss: 1.256523] [G loss: 0.113531]\n",
      "1626 [C loss: 1.246729] [G loss: -0.099003]\n",
      "1627 [C loss: 1.233659] [G loss: 0.008921]\n",
      "1628 [C loss: 1.271880] [G loss: 0.300700]\n",
      "1629 [C loss: 1.275339] [G loss: 0.175295]\n",
      "1630 [C loss: 1.241379] [G loss: -0.445001]\n",
      "1631 [C loss: 1.265891] [G loss: -0.682760]\n",
      "1632 [C loss: 1.244278] [G loss: -0.281496]\n",
      "1633 [C loss: 1.234421] [G loss: 0.011093]\n",
      "1634 [C loss: 1.258517] [G loss: -0.215906]\n",
      "1635 [C loss: 1.256285] [G loss: -0.484187]\n",
      "1636 [C loss: 1.226430] [G loss: -0.575099]\n",
      "1637 [C loss: 1.239139] [G loss: 0.011900]\n",
      "1638 [C loss: 1.248364] [G loss: 0.517359]\n",
      "1639 [C loss: 1.252964] [G loss: 0.481199]\n",
      "1640 [C loss: 1.259606] [G loss: 0.345362]\n",
      "1641 [C loss: 1.248801] [G loss: 0.161350]\n",
      "1642 [C loss: 1.223022] [G loss: 0.178204]\n",
      "1643 [C loss: 1.236644] [G loss: 0.663486]\n",
      "1644 [C loss: 1.263811] [G loss: 0.902988]\n",
      "1645 [C loss: 1.239866] [G loss: 0.855172]\n",
      "1646 [C loss: 1.230433] [G loss: 0.491109]\n",
      "1647 [C loss: 1.235491] [G loss: 0.198323]\n",
      "1648 [C loss: 1.247516] [G loss: 0.456565]\n",
      "1649 [C loss: 1.253622] [G loss: 0.724885]\n",
      "1650 [C loss: 1.262831] [G loss: 0.688776]\n",
      "1651 [C loss: 1.239862] [G loss: 0.584213]\n",
      "1652 [C loss: 1.253022] [G loss: -0.053466]\n",
      "1653 [C loss: 1.260017] [G loss: -0.165788]\n",
      "1654 [C loss: 1.248459] [G loss: 0.008738]\n",
      "1655 [C loss: 1.255076] [G loss: 0.095796]\n",
      "1656 [C loss: 1.230423] [G loss: -0.127604]\n",
      "1657 [C loss: 1.248604] [G loss: -0.275902]\n",
      "1658 [C loss: 1.241722] [G loss: -0.347704]\n",
      "1659 [C loss: 1.250491] [G loss: -0.257018]\n",
      "1660 [C loss: 1.247498] [G loss: -0.030844]\n",
      "1661 [C loss: 1.243612] [G loss: -0.178016]\n",
      "1662 [C loss: 1.246211] [G loss: -0.534039]\n",
      "1663 [C loss: 1.222076] [G loss: -0.699665]\n",
      "1664 [C loss: 1.234052] [G loss: -0.252789]\n",
      "1665 [C loss: 1.253666] [G loss: 0.126646]\n",
      "1666 [C loss: 1.253543] [G loss: 0.069983]\n",
      "1667 [C loss: 1.268555] [G loss: 0.213928]\n",
      "1668 [C loss: 1.263591] [G loss: 0.015859]\n",
      "1669 [C loss: 1.251855] [G loss: -0.252193]\n",
      "1670 [C loss: 1.256105] [G loss: -0.595850]\n",
      "1671 [C loss: 1.255723] [G loss: -0.712207]\n",
      "1672 [C loss: 1.248633] [G loss: -0.552731]\n",
      "1673 [C loss: 1.250831] [G loss: -0.505888]\n",
      "1674 [C loss: 1.246306] [G loss: -0.585925]\n",
      "1675 [C loss: 1.254219] [G loss: -0.786428]\n",
      "1676 [C loss: 1.248409] [G loss: -0.979996]\n",
      "1677 [C loss: 1.232079] [G loss: -0.247517]\n",
      "1678 [C loss: 1.253192] [G loss: 0.351235]\n",
      "1679 [C loss: 1.205026] [G loss: 0.556623]\n",
      "1680 [C loss: 1.237474] [G loss: 0.143581]\n",
      "1681 [C loss: 1.243711] [G loss: -0.099057]\n",
      "1682 [C loss: 1.246324] [G loss: 0.133408]\n",
      "1683 [C loss: 1.259341] [G loss: 0.461441]\n",
      "1684 [C loss: 1.239927] [G loss: 0.220121]\n",
      "1685 [C loss: 1.221141] [G loss: -0.254931]\n",
      "1686 [C loss: 1.258528] [G loss: -0.815412]\n",
      "1687 [C loss: 1.253986] [G loss: -0.870881]\n",
      "1688 [C loss: 1.239533] [G loss: -0.680628]\n",
      "1689 [C loss: 1.220515] [G loss: -0.348015]\n",
      "1690 [C loss: 1.244101] [G loss: -0.247126]\n",
      "1691 [C loss: 1.247081] [G loss: -0.224114]\n",
      "1692 [C loss: 1.251878] [G loss: 0.092854]\n",
      "1693 [C loss: 1.242096] [G loss: 0.379042]\n",
      "1694 [C loss: 1.230716] [G loss: 0.419759]\n",
      "1695 [C loss: 1.249565] [G loss: 0.387218]\n",
      "1696 [C loss: 1.234505] [G loss: 0.236561]\n",
      "1697 [C loss: 1.230090] [G loss: 0.255121]\n",
      "1698 [C loss: 1.244184] [G loss: 0.418219]\n",
      "1699 [C loss: 1.255266] [G loss: 0.682443]\n",
      "1700 [C loss: 1.214183] [G loss: 0.553931]\n",
      "1701 [C loss: 1.258493] [G loss: 0.082375]\n",
      "1702 [C loss: 1.251807] [G loss: -0.163364]\n",
      "1703 [C loss: 1.226122] [G loss: -0.026139]\n",
      "1704 [C loss: 1.241599] [G loss: 0.559101]\n",
      "1705 [C loss: 1.240334] [G loss: 0.896956]\n",
      "1706 [C loss: 1.239417] [G loss: 0.859481]\n",
      "1707 [C loss: 1.246617] [G loss: 0.234458]\n",
      "1708 [C loss: 1.268246] [G loss: -0.508092]\n",
      "1709 [C loss: 1.268584] [G loss: -0.977781]\n",
      "1710 [C loss: 1.241975] [G loss: -0.599409]\n",
      "1711 [C loss: 1.269038] [G loss: -0.341157]\n",
      "1712 [C loss: 1.231878] [G loss: -0.074378]\n",
      "1713 [C loss: 1.256064] [G loss: -0.417281]\n",
      "1714 [C loss: 1.249758] [G loss: -0.479184]\n",
      "1715 [C loss: 1.227369] [G loss: -0.290915]\n",
      "1716 [C loss: 1.248891] [G loss: -0.035210]\n",
      "1717 [C loss: 1.224446] [G loss: -0.082699]\n",
      "1718 [C loss: 1.235957] [G loss: -0.335254]\n",
      "1719 [C loss: 1.244209] [G loss: -0.139104]\n",
      "1720 [C loss: 1.242013] [G loss: 0.241968]\n",
      "1721 [C loss: 1.252085] [G loss: 0.379370]\n",
      "1722 [C loss: 1.245461] [G loss: 0.495626]\n",
      "1723 [C loss: 1.237443] [G loss: 0.176988]\n",
      "1724 [C loss: 1.234597] [G loss: -0.322456]\n",
      "1725 [C loss: 1.267301] [G loss: -1.004157]\n",
      "1726 [C loss: 1.251751] [G loss: -1.195953]\n",
      "1727 [C loss: 1.231861] [G loss: -1.030663]\n",
      "1728 [C loss: 1.240177] [G loss: -0.415513]\n",
      "1729 [C loss: 1.264467] [G loss: -0.133516]\n",
      "1730 [C loss: 1.227566] [G loss: -0.077029]\n",
      "1731 [C loss: 1.277712] [G loss: -0.085143]\n",
      "1732 [C loss: 1.260431] [G loss: -0.269636]\n",
      "1733 [C loss: 1.252914] [G loss: -0.353321]\n",
      "1734 [C loss: 1.264530] [G loss: -0.323260]\n",
      "1735 [C loss: 1.262195] [G loss: -0.084044]\n",
      "1736 [C loss: 1.247685] [G loss: 0.077176]\n",
      "1737 [C loss: 1.229254] [G loss: -0.034047]\n",
      "1738 [C loss: 1.277915] [G loss: -0.280942]\n",
      "1739 [C loss: 1.284047] [G loss: -0.241149]\n",
      "1740 [C loss: 1.254224] [G loss: -0.121087]\n",
      "1741 [C loss: 1.277364] [G loss: -0.028553]\n",
      "1742 [C loss: 1.247849] [G loss: 0.120288]\n",
      "1743 [C loss: 1.247780] [G loss: -0.088919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744 [C loss: 1.231117] [G loss: 0.021702]\n",
      "1745 [C loss: 1.243271] [G loss: -0.239297]\n",
      "1746 [C loss: 1.251039] [G loss: -0.231703]\n",
      "1747 [C loss: 1.257035] [G loss: 0.196096]\n",
      "1748 [C loss: 1.254519] [G loss: 0.568281]\n",
      "1749 [C loss: 1.251944] [G loss: 0.293086]\n",
      "1750 [C loss: 1.242340] [G loss: -0.215931]\n",
      "1751 [C loss: 1.255945] [G loss: -0.611361]\n",
      "1752 [C loss: 1.226277] [G loss: -0.593163]\n",
      "1753 [C loss: 1.229911] [G loss: -0.341138]\n",
      "1754 [C loss: 1.244929] [G loss: -0.131652]\n",
      "1755 [C loss: 1.257809] [G loss: -0.047685]\n",
      "1756 [C loss: 1.236246] [G loss: 0.208592]\n",
      "1757 [C loss: 1.243148] [G loss: 0.347967]\n",
      "1758 [C loss: 1.245945] [G loss: -0.050985]\n",
      "1759 [C loss: 1.241481] [G loss: -0.376003]\n",
      "1760 [C loss: 1.253974] [G loss: -0.608697]\n",
      "1761 [C loss: 1.225988] [G loss: -0.442904]\n",
      "1762 [C loss: 1.235734] [G loss: -0.415887]\n",
      "1763 [C loss: 1.258775] [G loss: -0.884742]\n",
      "1764 [C loss: 1.262113] [G loss: -1.094735]\n",
      "1765 [C loss: 1.234407] [G loss: -1.206809]\n",
      "1766 [C loss: 1.245031] [G loss: -1.163730]\n",
      "1767 [C loss: 1.244193] [G loss: -0.534407]\n",
      "1768 [C loss: 1.264266] [G loss: 0.215666]\n",
      "1769 [C loss: 1.254613] [G loss: 0.503389]\n",
      "1770 [C loss: 1.224790] [G loss: 0.009436]\n",
      "1771 [C loss: 1.255904] [G loss: -0.346230]\n",
      "1772 [C loss: 1.221127] [G loss: -0.496624]\n",
      "1773 [C loss: 1.245981] [G loss: -0.247893]\n",
      "1774 [C loss: 1.251679] [G loss: 0.198492]\n",
      "1775 [C loss: 1.259526] [G loss: 0.339415]\n",
      "1776 [C loss: 1.204746] [G loss: 0.165698]\n",
      "1777 [C loss: 1.246631] [G loss: -0.173844]\n",
      "1778 [C loss: 1.253179] [G loss: -0.458936]\n",
      "1779 [C loss: 1.247034] [G loss: -0.206876]\n",
      "1780 [C loss: 1.253372] [G loss: 0.405405]\n",
      "1781 [C loss: 1.228344] [G loss: 0.450277]\n",
      "1782 [C loss: 1.225463] [G loss: 0.466856]\n",
      "1783 [C loss: 1.253977] [G loss: 0.176002]\n",
      "1784 [C loss: 1.252929] [G loss: -0.144268]\n",
      "1785 [C loss: 1.252056] [G loss: -0.471167]\n",
      "1786 [C loss: 1.262026] [G loss: -0.690571]\n",
      "1787 [C loss: 1.227497] [G loss: -0.229172]\n",
      "1788 [C loss: 1.238960] [G loss: -0.026202]\n",
      "1789 [C loss: 1.256799] [G loss: 0.002004]\n",
      "1790 [C loss: 1.250879] [G loss: 0.128446]\n",
      "1791 [C loss: 1.246712] [G loss: 0.379134]\n",
      "1792 [C loss: 1.250043] [G loss: 0.205977]\n",
      "1793 [C loss: 1.224751] [G loss: 0.005528]\n",
      "1794 [C loss: 1.235161] [G loss: -0.134565]\n",
      "1795 [C loss: 1.236567] [G loss: 0.141566]\n",
      "1796 [C loss: 1.243738] [G loss: 0.428944]\n",
      "1797 [C loss: 1.234302] [G loss: 0.361525]\n",
      "1798 [C loss: 1.252788] [G loss: 0.081901]\n",
      "1799 [C loss: 1.257033] [G loss: -0.259192]\n",
      "1800 [C loss: 1.248144] [G loss: -0.825119]\n",
      "1801 [C loss: 1.246144] [G loss: -0.762070]\n",
      "1802 [C loss: 1.231346] [G loss: -0.612330]\n",
      "1803 [C loss: 1.250712] [G loss: -0.361542]\n",
      "1804 [C loss: 1.239975] [G loss: -0.282573]\n",
      "1805 [C loss: 1.239421] [G loss: 0.232658]\n",
      "1806 [C loss: 1.253142] [G loss: 0.564932]\n",
      "1807 [C loss: 1.209352] [G loss: 0.310512]\n",
      "1808 [C loss: 1.246926] [G loss: -0.190623]\n",
      "1809 [C loss: 1.243548] [G loss: -0.609820]\n",
      "1810 [C loss: 1.260608] [G loss: -0.529589]\n",
      "1811 [C loss: 1.248267] [G loss: -0.519222]\n",
      "1812 [C loss: 1.238738] [G loss: -0.520867]\n",
      "1813 [C loss: 1.228754] [G loss: -0.565002]\n",
      "1814 [C loss: 1.229851] [G loss: -0.168771]\n",
      "1815 [C loss: 1.229237] [G loss: 0.117134]\n",
      "1816 [C loss: 1.240937] [G loss: 0.568193]\n",
      "1817 [C loss: 1.226818] [G loss: 0.462552]\n",
      "1818 [C loss: 1.251942] [G loss: 0.153830]\n",
      "1819 [C loss: 1.239555] [G loss: -0.173419]\n",
      "1820 [C loss: 1.233666] [G loss: -0.198705]\n",
      "1821 [C loss: 1.257946] [G loss: -0.358263]\n",
      "1822 [C loss: 1.243004] [G loss: -0.390008]\n",
      "1823 [C loss: 1.236441] [G loss: -0.342207]\n",
      "1824 [C loss: 1.256521] [G loss: -0.180660]\n",
      "1825 [C loss: 1.214368] [G loss: -0.216235]\n",
      "1826 [C loss: 1.242662] [G loss: 0.104702]\n",
      "1827 [C loss: 1.249395] [G loss: 0.039261]\n",
      "1828 [C loss: 1.254971] [G loss: 0.001681]\n",
      "1829 [C loss: 1.243174] [G loss: -0.318459]\n",
      "1830 [C loss: 1.221036] [G loss: -0.553320]\n",
      "1831 [C loss: 1.251116] [G loss: -0.730283]\n",
      "1832 [C loss: 1.261941] [G loss: -0.549986]\n",
      "1833 [C loss: 1.247724] [G loss: -0.315375]\n",
      "1834 [C loss: 1.227251] [G loss: -0.373583]\n",
      "1835 [C loss: 1.229547] [G loss: -0.260888]\n",
      "1836 [C loss: 1.261015] [G loss: -0.338207]\n",
      "1837 [C loss: 1.228801] [G loss: 0.106047]\n",
      "1838 [C loss: 1.246819] [G loss: 0.314823]\n",
      "1839 [C loss: 1.264905] [G loss: 0.540434]\n",
      "1840 [C loss: 1.272461] [G loss: 0.839622]\n",
      "1841 [C loss: 1.287387] [G loss: 1.035093]\n",
      "1842 [C loss: 1.214447] [G loss: 0.612479]\n",
      "1843 [C loss: 1.274390] [G loss: -0.445674]\n",
      "1844 [C loss: 1.296194] [G loss: -1.193722]\n",
      "1845 [C loss: 1.251246] [G loss: -1.094878]\n",
      "1846 [C loss: 1.251410] [G loss: -0.747960]\n",
      "1847 [C loss: 1.254906] [G loss: -0.252134]\n",
      "1848 [C loss: 1.250845] [G loss: -0.227663]\n",
      "1849 [C loss: 1.221354] [G loss: -0.585595]\n",
      "1850 [C loss: 1.256507] [G loss: -0.861314]\n",
      "1851 [C loss: 1.225608] [G loss: -0.814188]\n",
      "1852 [C loss: 1.245213] [G loss: -0.573446]\n",
      "1853 [C loss: 1.243095] [G loss: -0.340434]\n",
      "1854 [C loss: 1.236144] [G loss: -0.183393]\n",
      "1855 [C loss: 1.242233] [G loss: 0.186900]\n",
      "1856 [C loss: 1.241237] [G loss: 0.452676]\n",
      "1857 [C loss: 1.225180] [G loss: 0.064910]\n",
      "1858 [C loss: 1.232091] [G loss: -0.649302]\n",
      "1859 [C loss: 1.264717] [G loss: -0.693778]\n",
      "1860 [C loss: 1.245689] [G loss: -0.782526]\n",
      "1861 [C loss: 1.234884] [G loss: -1.014942]\n",
      "1862 [C loss: 1.237956] [G loss: -1.234406]\n",
      "1863 [C loss: 1.228956] [G loss: -0.755311]\n",
      "1864 [C loss: 1.250478] [G loss: -0.356305]\n",
      "1865 [C loss: 1.261627] [G loss: 0.163049]\n",
      "1866 [C loss: 1.245946] [G loss: 0.371533]\n",
      "1867 [C loss: 1.249359] [G loss: 0.255833]\n",
      "1868 [C loss: 1.234610] [G loss: -0.221151]\n",
      "1869 [C loss: 1.231784] [G loss: -0.238999]\n",
      "1870 [C loss: 1.225039] [G loss: -0.426523]\n",
      "1871 [C loss: 1.269961] [G loss: -0.219494]\n",
      "1872 [C loss: 1.228244] [G loss: -0.369979]\n",
      "1873 [C loss: 1.240131] [G loss: -0.203354]\n",
      "1874 [C loss: 1.218584] [G loss: -0.794742]\n",
      "1875 [C loss: 1.219446] [G loss: -0.673431]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3487f85c5f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m losses = wgan_gp.train(epochs, n_generator, n_critic, dataset,\n\u001b[0;32m----> 5\u001b[0;31m            img_frequency, loss_frequency, latent_space_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/Master-thesis/implementation/generative_models/wgan_gp.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, n_generator, n_critic, dataset, img_frequency, loss_frequency, latent_space_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mcritic_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_transactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "wgan_gp = WGAN_GP(timesteps, latent_dim, gradient_penality_weight, batch_size, run_dir, img_dir, model_dir, generated_datesets_dir)\n",
    "wgan_gp.build_models(generator_lr, critic_lr)\n",
    "        \n",
    "losses = wgan_gp.train(epochs, n_generator, n_critic, dataset,\n",
    "           img_frequency, loss_frequency, latent_space_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
