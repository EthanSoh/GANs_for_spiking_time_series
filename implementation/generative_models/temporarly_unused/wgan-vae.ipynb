{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['subtract', 'average', 'copy', 'concatenate', 'minimum', 'poisson', 'multiply', 'maximum', 'add', 'get', 'dot', 'datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras.losses import *\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gan_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "code_folding": [
     54,
     59,
     90,
     121,
     225,
     246,
     253,
     285,
     291,
     304,
     309,
     315,
     319,
     322,
     324,
     330
    ]
   },
   "outputs": [],
   "source": [
    "class WGAN_VAE:\n",
    "    def __init__(self, timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir, batch_size):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._run_dir = run_dir\n",
    "        self._img_dir = img_dir\n",
    "        self._model_dir = model_dir\n",
    "        self._generated_datesets_dir = generated_datesets_dir\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._save_config()\n",
    "        \n",
    "        self._epoch = 0\n",
    "        self._losses = [[], []]\n",
    "\n",
    "    def build_models(self, generator_lr, critic_lr):\n",
    "        self._critic, self._critic_hidden = self._build_critic()\n",
    "        self._critic.compile(loss=self._wasserstein_loss, optimizer=RMSprop(critic_lr))\n",
    "        \n",
    "        self._real_inputs = Input((self._timesteps,))\n",
    "        self._mean_inputs = Input((self._timesteps,))\n",
    "        z_inputs = Input((self._latent_dim,))\n",
    "        \n",
    "        self._generator = self._build_generator()\n",
    "        self._encoder = self._build_encoder()\n",
    "        \n",
    "        set_model_trainable(self._critic, False)\n",
    "        set_model_trainable(self._critic_hidden, False)\n",
    "        \n",
    "        generated_inputs = self._generator(z_inputs) \n",
    "        self._discriminated_generated = self._critic(generated_inputs)\n",
    "        \n",
    "        self._encoded_mean, self._encoded_logvar = self._encoder(self._real_inputs)\n",
    "        sampled_z = Lambda(self._sampling)([self._encoded_mean, self._encoded_logvar])\n",
    "        self._decoded_inputs = self._generator(sampled_z)\n",
    "                \n",
    "        self._discriminated_hidden_decoded = self._critic_hidden(self._decoded_inputs)\n",
    "        self._discriminated_hidden_real = self._critic_hidden(self._real_inputs)\n",
    "        \n",
    "        self._discriminated_decoded = self._critic(self._decoded_inputs)\n",
    "        \n",
    "        self._gan_vae = Model([self._real_inputs, z_inputs], self._discriminated_decoded, 'GAN VAE')\n",
    "        self._gan_vae.compile(loss=self._gan_vae_custom_loss, optimizer=RMSprop(generator_lr))\n",
    "        \n",
    "        self._generator = Model(z_inputs, generated_inputs)\n",
    "        \n",
    "        return self._gan_vae, self._generator, self._critic\n",
    "\n",
    "    def _gan_vae_custom_loss(self, y_true, y_pred):\n",
    "        kl_loss = - 0.5 * K.sum(+ 1 + self._encoded_logvar - K.square(self._encoded_mean) - K.exp(self._encoded_logvar), axis=-1)\n",
    "        mse_loss = mean_squared_error(self._discriminated_hidden_decoded, self._discriminated_hidden_real)\n",
    "        gan_loss = self._wasserstein_loss(y_true, self._discriminated_generated)\n",
    "        return 0.3 * gan_loss + 0.6 * mse_loss + 0.1 * kl_loss\n",
    "    \n",
    "    def _sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(self._batch_size, self._latent_dim), mean=0., stddev=1.0)\n",
    "        return z_mean + K.exp(z_log_var/2.0) * epsilon\n",
    "    \n",
    "    def _build_generator(self):\n",
    "        generator_inputs = Input((self._latent_dim, ))\n",
    "        generated = generator_inputs\n",
    "        \n",
    "#         generated = Dense(self._latent_dim, activation='relu')(generated)\n",
    "#         generated = Dense(self._latent_dim, activation='relu')(generated)\n",
    "#         generated = Dense(self._timesteps, activation='relu')(generated)\n",
    "\n",
    "        generated = Dense(12, activation='linear')(generated)\n",
    "        \n",
    "        generated = Lambda(lambda x: K.expand_dims(x))(generated)\n",
    "        generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(64, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Conv1D(64, 3, activation='relu', padding='same')(generated)\n",
    "        \n",
    "        generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(64, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Conv1D(64, 3, activation='relu', padding='same')(generated)\n",
    "        \n",
    "        generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(32, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Conv1D(1, 3, activation='tanh', padding='same')(generated)\n",
    "        generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "        \n",
    "#         generated = Concatenate()([generated, generator_inputs])\n",
    "\n",
    "        generated = Dense(self._timesteps, activation='tanh')(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _build_critic(self):\n",
    "        critic_inputs = Input((self._timesteps, ))\n",
    "        criticized = critic_inputs\n",
    "#         criticized = Dense(self._timesteps, activation='relu')(criticized)\n",
    "#         criticized = Dense(self._timesteps, activation='relu')(criticized)\n",
    "#         criticized = Dense(self._latent_dim, activation='relu')(criticized)\n",
    "        \n",
    "        criticized = Lambda(lambda x: K.expand_dims(x))(criticized)\n",
    "        criticized = Conv1D(32, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = Conv1D(32, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        \n",
    "        criticized = Conv1D(64, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = Conv1D(64, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        \n",
    "        criticized = Conv1D(64, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = Conv1D(64, 3, activation='relu', padding='same')(criticized)\n",
    "        criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        criticized = Flatten()(criticized)\n",
    "                \n",
    "#         criticized = Concatenate()([criticized, critic_inputs])\n",
    "\n",
    "        criticized = Dense(32, activation='relu')(criticized)\n",
    "        criticized_hidden = Dense(15, activation='tanh')(criticized)\n",
    "        criticized = Dense(1)(criticized_hidden) \n",
    "\n",
    "        critic = Model(critic_inputs, criticized, 'critic')\n",
    "        critic_hidden = Model(critic_inputs, criticized_hidden, 'critic_hidden')\n",
    "        return critic, critic_hidden\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        encoder_inputs = Input((self._timesteps,))\n",
    "        encoded = encoder_inputs\n",
    "        \n",
    "#         encoded = Dense(self._timesteps, activation='relu')(encoded)\n",
    "#         encoded = Dense(self._timesteps, activation='relu')(encoded)\n",
    "#         encoded = Dense(self._latent_dim, activation='relu')(encoded)\n",
    "\n",
    "        encoded = Lambda(lambda x: K.expand_dims(x))(encoded)\n",
    "        encoded = Conv1D(32, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = Conv1D(32, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "        \n",
    "        encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "        \n",
    "        encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)\n",
    "        encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "        encoded = Flatten()(encoded)\n",
    "        \n",
    "#         encoded = Concatenate()([encoded, encoder_inputs])\n",
    "        \n",
    "        encoded = Dense(self._latent_dim, activation='relu')(encoded)    \n",
    "        encoded_mean = Dense(self._latent_dim)(encoded)\n",
    "        encoded_logvar = Dense(self._latent_dim)(encoded)\n",
    "\n",
    "        encoder = Model(encoder_inputs, [encoded_mean, encoded_logvar], 'encoder')\n",
    "        return encoder\n",
    "        \n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        \n",
    "        while self._epoch < epochs:\n",
    "            self._epoch += 1\n",
    "            critic_losses = []\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.choice(dataset.shape[0], half_batch, replace=False)\n",
    "                real_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                mixed_batch = np.vstack([real_transactions, generated_transactions])                \n",
    "                mixed_labels = np.vstack([-np.ones((half_batch, 1)), np.ones((half_batch, 1))])\n",
    "                \n",
    "                critic_losses.append(self._critic.train_on_batch(mixed_batch, mixed_labels))\n",
    "\n",
    "                self._clip_weights(clip_value)\n",
    "            critic_loss = np.mean(critic_losses)\n",
    "            \n",
    "            generator_losses = []\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "                indexes = np.random.randint(0, dataset.shape[0], batch_size)\n",
    "                \n",
    "                batch_transactions = dataset[indexes]\n",
    "                generator_losses.append(self._gan.train_on_batch([batch_transactions, noise], -np.ones((batch_size, 1))))\n",
    "            generator_loss = np.mean(generator_losses)\n",
    "            \n",
    "            generator_loss = generator_loss\n",
    "            critic_loss = critic_loss\n",
    "            \n",
    "            self._losses[0].append(generator_loss)\n",
    "            self._losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [GENERATOR loss: %f] [CRITIC loss: %f]\" % (self._epoch, generator_loss, critic_loss))\n",
    "\n",
    "            \n",
    "            if self._epoch % 250 == 0:\n",
    "                self._save_losses()\n",
    "                \n",
    "            if self._epoch % img_frequency == 0:\n",
    "                self._save_imgs()\n",
    "                self._save_latent_space()\n",
    "                \n",
    "#                 noise = np.random.normal(0, 1, (10, latent_dim))\n",
    "#                 generated = self._generator.predict(noise)\n",
    "#                 plt.subplots(2, 5)\n",
    "#                 for i in range(10):\n",
    "#                     plt.subplot(2, 5, i+1)\n",
    "#                     plt.imshow(generated[i].reshape(21, 21))\n",
    "#                     plt.xticks([])\n",
    "#                     plt.yticks([])\n",
    "#                 plt.tight_layout()\n",
    "#                 plt.show()\n",
    "\n",
    "#             if self._epoch % model_save_frequency == 0:\n",
    "#                 self._save_models()\n",
    "                \n",
    "#             if self._epoch % dataset_generation_frequency == 0:\n",
    "#                 self._generate_dataset(self._epoch, dataset_generation_size)\n",
    "          \n",
    "        self._generate_dataset(epochs, dataset_generation_size)\n",
    "        self._save_losses(self._losses)\n",
    "        self._save_models()\n",
    "        self._save_imgs()\n",
    "        self._save_latent_space()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _save_imgs(self):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('%05d.png' % self._epoch)))\n",
    "        plt.savefig(str(self._img_dir / 'last.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_latent_space(self):\n",
    "        if self._latent_dim > 2:\n",
    "            latent_vector = np.random.normal(0, 1, latent_dim)\n",
    "        plt.subplots(5, 5, figsize=(15, 5))\n",
    "\n",
    "        for i, v_i in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "            for j, v_j in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                if self._latent_dim > 2:\n",
    "                    latent_vector[-2:] = [v_i, v_j]\n",
    "                else:\n",
    "                    latent_vector = np.array([v_i, v_j])\n",
    "                    \n",
    "                plt.subplot(5, 5, i*5+j+1)\n",
    "                plt.plot(self._generator.predict(latent_vector.reshape((1, self._latent_dim))).T)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('latent_space.png')))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_losses(self):\n",
    "        plt.subplots(2, 1, figsize=(15, 9))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self._losses[0])\n",
    "        plt.plot(self._losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(self._losses[0][-1000:])\n",
    "        plt.plot(self._losses[1][-1000:])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig(str(self._img_dir / 'losses.png')) \n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        with open(str(self._run_dir / 'losses.p'), 'wb') as f:\n",
    "            pickle.dump(self._losses, f)\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        for l in self._critic.layers:\n",
    "#             if 'minibatch_discrimination' not in l.name:\n",
    "            weights = [np.clip(w, -clip_value, clip_value) for w in l.get_weights()]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            'timesteps' : self._timesteps,\n",
    "            'latent_dim' : self._latent_dim,\n",
    "            'run_dir' : self._run_dir,\n",
    "            'img_dir' : self._img_dir,\n",
    "            'model_dir' : self._model_dir,\n",
    "            'generated_datesets_dir' : self._generated_datesets_dir\n",
    "        }\n",
    "        \n",
    "        with open(str(self._run_dir / 'config.p'), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "    def _save_models(self):\n",
    "        self._gan.save(self._model_dir / 'wgan.h5')\n",
    "        self._generator.save(self._model_dir / 'generator.h5')\n",
    "        self._critic.save(self._model_dir / 'critic.h5')\n",
    "        \n",
    "    def _generate_dataset(self, epoch, dataset_generation_size):\n",
    "        z_samples = np.random.normal(0, 1, (dataset_generation_size, self._latent_dim))\n",
    "        generated_dataset = self._generator.predict(z_samples)\n",
    "        np.save(self._generated_datesets_dir / ('%d_generated_data' % epoch), generated_dataset)\n",
    "        np.save(self._generated_datesets_dir / 'last', generated_dataset)\n",
    "        \n",
    "    def get_models(self):\n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def restore_training(self):\n",
    "        self.load_models()\n",
    "        with open(str(self._run_dir / 'losses.p'), 'rb') as f:\n",
    "            self._losses = pickle.load(f)\n",
    "            self._epoch = len(self._losses[0])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    def load_models(self):\n",
    "        custom_objects = {\n",
    "            'MinibatchDiscrimination':MinibatchDiscrimination,\n",
    "            '_wasserstein_loss':self._wasserstein_loss\n",
    "        }\n",
    "        self._gan = load_model(self._model_dir / 'wgan.h5', custom_objects=custom_objects)\n",
    "        self._generator = load_model(self._model_dir / 'generator.h5')\n",
    "        self._critic = load_model(self._model_dir / 'critic.h5', custom_objects=custom_objects)\n",
    "        \n",
    "        return self._gan, self._generator, self._critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_transactions_filepath = \"../../datasets/berka_dataset/usable/normalized_transactions_months.npy\"\n",
    "\n",
    "timesteps = 90\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions = split_data(transactions, timesteps)\n",
    "np.random.shuffle(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "\n",
    "# timesteps = 21*21\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train[:, 3:-4, 3:-4]\n",
    "# x_train = x_train.reshape(60000, timesteps)\n",
    "# transactions = (x_train / 255.0) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 500000\n",
    "n_critic = 1\n",
    "n_generator = 5\n",
    "latent_dim = 2\n",
    "generator_lr = 0.0005\n",
    "critic_lr = 0.00005\n",
    "clip_value = 0.05\n",
    "img_frequency = 250\n",
    "model_save_frequency = 3000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "root_path = Path('wgan')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-89ca7c239ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n\u001b[0;32m----> 5\u001b[0;31m            img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-223-b2bc3873f96b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value, img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mcritic_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixed_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-b2bc3873f96b>\u001b[0m in \u001b[0;36m_clip_weights\u001b[0;34m(self, clip_value)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;31m#             if 'minibatch_discrimination' not in l.name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                                  'provided weight shape ' + str(w.shape))\n\u001b[1;32m   1218\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2368\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wgan = WGAN_VAE(timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir, batch_size)\n",
    "gan, generator, critic = wgan.build_models(generator_lr, critic_lr)\n",
    "        \n",
    "losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
