{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from GAN_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     1,
     230
    ]
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self, timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._run_dir = run_dir\n",
    "        self._img_dir = img_dir\n",
    "        self._model_dir = model_dir\n",
    "        self._generated_datesets_dir = generated_datesets_dir\n",
    "        \n",
    "        self._save_config()\n",
    "        \n",
    "        self._epoch = 0\n",
    "        self._losses = [[], []]\n",
    "\n",
    "    def build_models(self, generator_lr, critic_lr):        \n",
    "        self._generator = self._build_generator(self._latent_dim, self._timesteps)\n",
    "\n",
    "        self._critic = self._build_critic(self._timesteps)\n",
    "        self._critic.compile(loss=self._wasserstein_loss, optimizer=RMSprop(critic_lr))\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        set_model_trainable(self._critic, False)\n",
    "\n",
    "        valid = self._critic(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss=self._wasserstein_loss,\n",
    "            optimizer=RMSprop(generator_lr),\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "\n",
    "    def _build_generator(self, noise_dim, timesteps):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "        generated = generator_inputs\n",
    "        \n",
    "        generated = Lambda(lambda x: K.expand_dims(x))(generated)\n",
    "        while generated.shape[1] < timesteps:\n",
    "            generated = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(generated)\n",
    "            generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(\n",
    "            1, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "        generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _build_critic(self, timesteps):\n",
    "        critic_inputs = Input((timesteps, ))\n",
    "        criticized = critic_inputs\n",
    "        \n",
    "#         mbd = MinibatchDiscrimination(5, 3)(criticized)\n",
    "#         mbd = Dense(1, activation='tanh')(mbd)\n",
    "        \n",
    "        criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "            criticized)\n",
    "        while criticized.shape[1] > 1:\n",
    "            criticized = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(criticized)\n",
    "            criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        criticized = Flatten()(criticized)\n",
    "        criticized = Dense(32, activation='relu')(criticized)\n",
    "        criticized = Dense(15, activation='relu')(criticized)\n",
    "#         criticized = Dense(1, activation='tanh')(criticized) \n",
    "#         criticized = Concatenate()([criticized, mbd])\n",
    "        criticized = Dense(1)(criticized) \n",
    "\n",
    "        critic = Model(critic_inputs, criticized, 'critic')\n",
    "        return critic\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        \n",
    "        while self._epoch < epochs:\n",
    "            self._epoch += 1\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                critic_loss_real = self._critic.train_on_batch(\n",
    "                    batch_transactions, -np.ones((half_batch, 1)))\n",
    "                critic_loss_fake = self._critic.train_on_batch(\n",
    "                    generated_transactions, np.ones((half_batch, 1)))\n",
    "                critic_loss = 0.5 * np.add(critic_loss_real,\n",
    "                                                  critic_loss_fake)\n",
    "\n",
    "                self._clip_weights(clip_value)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, -np.ones((batch_size, 1)))[0]\n",
    "            \n",
    "            generator_loss = 1 - generator_loss\n",
    "            critic_loss = 1 - critic_loss\n",
    "            \n",
    "            self._losses[0].append(generator_loss)\n",
    "            self._losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [C loss: %f] [G loss: %f]\" % (self._epoch, critic_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if self._epoch % img_frequency == 0:\n",
    "                self._save_imgs()\n",
    "                self._save_latent_space()\n",
    "            \n",
    "            if self._epoch % model_save_frequency == 0:\n",
    "                self._save_models()\n",
    "                \n",
    "            if self._epoch % dataset_generation_frequency == 0:\n",
    "                self._generate_dataset(self._epoch, dataset_generation_size)\n",
    "                \n",
    "            if self._epoch % 250 == 0:\n",
    "                self._save_losses()\n",
    "          \n",
    "        self._generate_dataset(epochs, dataset_generation_size)\n",
    "        self._save_losses(self._losses)\n",
    "        self._save_models()\n",
    "        self._save_imgs()\n",
    "        self._save_latent_space()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _save_imgs(self):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('%05d.png' % self._epoch)))\n",
    "        plt.savefig(str(self._img_dir / 'last.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_latent_space(self):\n",
    "        if self._latent_dim > 2:\n",
    "            latent_vector = np.random.normal(0, 1, latent_dim)\n",
    "        plt.subplots(5, 5, figsize=(15, 5))\n",
    "\n",
    "        for i, v_i in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "            for j, v_j in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                if self._latent_dim > 2:\n",
    "                    latent_vector[-2:] = [v_i, v_j]\n",
    "                else:\n",
    "                    latent_vector = np.array([v_i, v_j])\n",
    "                    \n",
    "                plt.subplot(5, 5, i*5+j+1)\n",
    "                plt.plot(self._generator.predict(latent_vector.reshape((1, self._latent_dim))).T)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('latent_space.png')))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_losses(self):\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(self._losses[0])\n",
    "        plt.plot(self._losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig(str(self._img_dir / 'losses.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        with open(str(self._run_dir / 'losses.p'), 'wb') as f:\n",
    "            pickle.dump(self._losses, f)\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        for l in self._critic.layers:\n",
    "#             if 'minibatch_discrimination' not in l.name:\n",
    "            weights = [np.clip(w, -clip_value, clip_value) for w in l.get_weights()]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            'timesteps' : self._timesteps,\n",
    "            'latent_dim' : self._latent_dim,\n",
    "            'run_dir' : self._run_dir,\n",
    "            'img_dir' : self._img_dir,\n",
    "            'model_dir' : self._model_dir,\n",
    "            'generated_datesets_dir' : self._generated_datesets_dir\n",
    "        }\n",
    "        \n",
    "        with open(str(self._run_dir / 'config.p'), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "    def _save_models(self):\n",
    "        self._gan.save(self._model_dir / 'wgan.h5')\n",
    "        self._generator.save(self._model_dir / 'generator.h5')\n",
    "        self._critic.save(self._model_dir / 'critic.h5')\n",
    "        \n",
    "    def _generate_dataset(self, epoch, dataset_generation_size):\n",
    "        z_samples = np.random.normal(0, 1, (dataset_generation_size, self._latent_dim))\n",
    "        generated_dataset = self._generator.predict(z_samples)\n",
    "        np.save(self._generated_datesets_dir / ('%d_generated_data' % epoch), generated_dataset)\n",
    "        np.save(self._generated_datesets_dir / 'last', generated_dataset)\n",
    "        \n",
    "    def get_models(self):\n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def restore_training(self):\n",
    "        self.load_models()\n",
    "        with open(str(self._run_dir / 'losses.p'), 'rb') as f:\n",
    "            self._losses = pickle.load(f)\n",
    "            self._epoch = len(self._losses[0])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    def load_models(self):\n",
    "        custom_objects = {\n",
    "            'MinibatchDiscrimination':MinibatchDiscrimination,\n",
    "            '_wasserstein_loss':self._wasserstein_loss\n",
    "        }\n",
    "        self._gan = load_model(self._model_dir / 'wgan.h5', custom_objects=custom_objects)\n",
    "        self._generator = load_model(self._model_dir / 'generator.h5')\n",
    "        self._critic = load_model(self._model_dir / 'critic.h5', custom_objects=custom_objects)\n",
    "        \n",
    "        return self._gan, self._generator, self._critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53888, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../../datasets/berka_dataset/usable/normalized_transactions_100.npy\"\n",
    "\n",
    "timesteps = 100\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 500000\n",
    "n_critic = 5\n",
    "n_generator = 1\n",
    "latent_dim = 2\n",
    "generator_lr = 0.00005\n",
    "critic_lr = 0.00005\n",
    "clip_value = 0.05\n",
    "img_frequency = 250\n",
    "model_save_frequency = 3000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('wgan')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [C loss: 0.999962] [G loss: 1.000086]\n",
      "2 [C loss: 0.999966] [G loss: 1.000090]\n",
      "3 [C loss: 0.999967] [G loss: 1.000090]\n",
      "4 [C loss: 0.999966] [G loss: 1.000089]\n",
      "5 [C loss: 0.999966] [G loss: 1.000089]\n",
      "6 [C loss: 0.999966] [G loss: 1.000088]\n",
      "7 [C loss: 0.999966] [G loss: 1.000088]\n",
      "8 [C loss: 0.999966] [G loss: 1.000088]\n",
      "9 [C loss: 0.999966] [G loss: 1.000088]\n",
      "10 [C loss: 0.999966] [G loss: 1.000088]\n",
      "11 [C loss: 0.999966] [G loss: 1.000087]\n",
      "12 [C loss: 0.999966] [G loss: 1.000087]\n",
      "13 [C loss: 0.999966] [G loss: 1.000087]\n",
      "14 [C loss: 0.999966] [G loss: 1.000087]\n",
      "15 [C loss: 0.999966] [G loss: 1.000087]\n",
      "16 [C loss: 0.999966] [G loss: 1.000087]\n",
      "17 [C loss: 0.999967] [G loss: 1.000087]\n",
      "18 [C loss: 0.999967] [G loss: 1.000087]\n",
      "19 [C loss: 0.999967] [G loss: 1.000086]\n",
      "20 [C loss: 0.999967] [G loss: 1.000086]\n",
      "21 [C loss: 0.999967] [G loss: 1.000086]\n",
      "22 [C loss: 0.999966] [G loss: 1.000086]\n",
      "23 [C loss: 0.999967] [G loss: 1.000086]\n",
      "24 [C loss: 0.999967] [G loss: 1.000086]\n",
      "25 [C loss: 0.999967] [G loss: 1.000086]\n",
      "26 [C loss: 0.999967] [G loss: 1.000086]\n",
      "27 [C loss: 0.999968] [G loss: 1.000086]\n",
      "28 [C loss: 0.999967] [G loss: 1.000085]\n",
      "29 [C loss: 0.999968] [G loss: 1.000085]\n",
      "30 [C loss: 0.999968] [G loss: 1.000085]\n",
      "31 [C loss: 0.999968] [G loss: 1.000085]\n",
      "32 [C loss: 0.999968] [G loss: 1.000085]\n",
      "33 [C loss: 0.999968] [G loss: 1.000085]\n",
      "34 [C loss: 0.999968] [G loss: 1.000085]\n",
      "35 [C loss: 0.999969] [G loss: 1.000084]\n",
      "36 [C loss: 0.999969] [G loss: 1.000084]\n",
      "37 [C loss: 0.999969] [G loss: 1.000084]\n",
      "38 [C loss: 0.999969] [G loss: 1.000084]\n",
      "39 [C loss: 0.999969] [G loss: 1.000084]\n",
      "40 [C loss: 0.999969] [G loss: 1.000084]\n",
      "41 [C loss: 0.999970] [G loss: 1.000084]\n",
      "42 [C loss: 0.999970] [G loss: 1.000084]\n",
      "43 [C loss: 0.999970] [G loss: 1.000084]\n",
      "44 [C loss: 0.999970] [G loss: 1.000083]\n",
      "45 [C loss: 0.999970] [G loss: 1.000083]\n",
      "46 [C loss: 0.999970] [G loss: 1.000083]\n",
      "47 [C loss: 0.999972] [G loss: 1.000083]\n",
      "48 [C loss: 0.999973] [G loss: 1.000083]\n",
      "49 [C loss: 0.999974] [G loss: 1.000083]\n",
      "50 [C loss: 0.999974] [G loss: 1.000083]\n",
      "51 [C loss: 0.999974] [G loss: 1.000083]\n",
      "52 [C loss: 0.999974] [G loss: 1.000083]\n",
      "53 [C loss: 0.999976] [G loss: 1.000082]\n",
      "54 [C loss: 0.999976] [G loss: 1.000082]\n",
      "55 [C loss: 0.999977] [G loss: 1.000082]\n",
      "56 [C loss: 0.999979] [G loss: 1.000082]\n",
      "57 [C loss: 0.999979] [G loss: 1.000082]\n",
      "58 [C loss: 0.999980] [G loss: 1.000081]\n",
      "59 [C loss: 0.999981] [G loss: 1.000081]\n",
      "60 [C loss: 0.999982] [G loss: 1.000081]\n",
      "61 [C loss: 0.999984] [G loss: 1.000081]\n",
      "62 [C loss: 0.999986] [G loss: 1.000081]\n",
      "63 [C loss: 0.999987] [G loss: 1.000081]\n",
      "64 [C loss: 0.999990] [G loss: 1.000081]\n",
      "65 [C loss: 0.999990] [G loss: 1.000080]\n",
      "66 [C loss: 0.999992] [G loss: 1.000080]\n",
      "67 [C loss: 0.999995] [G loss: 1.000078]\n",
      "68 [C loss: 0.999997] [G loss: 1.000078]\n",
      "69 [C loss: 1.000001] [G loss: 1.000077]\n",
      "70 [C loss: 1.000005] [G loss: 1.000076]\n",
      "71 [C loss: 1.000007] [G loss: 1.000076]\n",
      "72 [C loss: 1.000012] [G loss: 1.000075]\n",
      "73 [C loss: 1.000013] [G loss: 1.000074]\n",
      "74 [C loss: 1.000017] [G loss: 1.000074]\n",
      "75 [C loss: 1.000024] [G loss: 1.000073]\n",
      "76 [C loss: 1.000027] [G loss: 1.000073]\n",
      "77 [C loss: 1.000030] [G loss: 1.000073]\n",
      "78 [C loss: 1.000039] [G loss: 1.000073]\n",
      "79 [C loss: 1.000041] [G loss: 1.000072]\n",
      "80 [C loss: 1.000050] [G loss: 1.000072]\n",
      "81 [C loss: 1.000058] [G loss: 1.000072]\n",
      "82 [C loss: 1.000057] [G loss: 1.000072]\n",
      "83 [C loss: 1.000067] [G loss: 1.000071]\n",
      "84 [C loss: 1.000076] [G loss: 1.000071]\n",
      "85 [C loss: 1.000081] [G loss: 1.000071]\n",
      "86 [C loss: 1.000096] [G loss: 1.000070]\n",
      "87 [C loss: 1.000096] [G loss: 1.000070]\n",
      "88 [C loss: 1.000113] [G loss: 1.000070]\n",
      "89 [C loss: 1.000122] [G loss: 1.000070]\n",
      "90 [C loss: 1.000127] [G loss: 1.000070]\n",
      "91 [C loss: 1.000142] [G loss: 1.000070]\n",
      "92 [C loss: 1.000157] [G loss: 1.000070]\n",
      "93 [C loss: 1.000161] [G loss: 1.000070]\n",
      "94 [C loss: 1.000177] [G loss: 1.000070]\n",
      "95 [C loss: 1.000198] [G loss: 1.000070]\n",
      "96 [C loss: 1.000203] [G loss: 1.000070]\n",
      "97 [C loss: 1.000227] [G loss: 1.000070]\n",
      "98 [C loss: 1.000242] [G loss: 1.000070]\n",
      "99 [C loss: 1.000268] [G loss: 1.000070]\n",
      "100 [C loss: 1.000278] [G loss: 1.000070]\n",
      "101 [C loss: 1.000312] [G loss: 1.000070]\n",
      "102 [C loss: 1.000311] [G loss: 1.000070]\n",
      "103 [C loss: 1.000348] [G loss: 1.000070]\n",
      "104 [C loss: 1.000364] [G loss: 1.000070]\n",
      "105 [C loss: 1.000394] [G loss: 1.000070]\n",
      "106 [C loss: 1.000408] [G loss: 1.000061]\n",
      "107 [C loss: 1.000448] [G loss: 1.000046]\n",
      "108 [C loss: 1.000485] [G loss: 1.000041]\n",
      "109 [C loss: 1.000501] [G loss: 1.000036]\n",
      "110 [C loss: 1.000566] [G loss: 1.000032]\n",
      "111 [C loss: 1.000567] [G loss: 1.000027]\n",
      "112 [C loss: 1.000615] [G loss: 1.000022]\n",
      "113 [C loss: 1.000663] [G loss: 1.000017]\n",
      "114 [C loss: 1.000693] [G loss: 1.000011]\n",
      "115 [C loss: 1.000736] [G loss: 1.000006]\n",
      "116 [C loss: 1.000766] [G loss: 0.999999]\n",
      "117 [C loss: 1.000840] [G loss: 0.999992]\n",
      "118 [C loss: 1.000895] [G loss: 0.999985]\n",
      "119 [C loss: 1.000936] [G loss: 0.999977]\n",
      "120 [C loss: 1.000964] [G loss: 0.999971]\n",
      "121 [C loss: 1.001041] [G loss: 0.999962]\n",
      "122 [C loss: 1.001129] [G loss: 0.999954]\n",
      "123 [C loss: 1.001137] [G loss: 0.999946]\n",
      "124 [C loss: 1.001235] [G loss: 0.999937]\n",
      "125 [C loss: 1.001303] [G loss: 0.999929]\n",
      "126 [C loss: 1.001368] [G loss: 0.999919]\n",
      "127 [C loss: 1.001414] [G loss: 0.999909]\n",
      "128 [C loss: 1.001517] [G loss: 0.999900]\n",
      "129 [C loss: 1.001592] [G loss: 0.999889]\n",
      "130 [C loss: 1.001651] [G loss: 0.999879]\n",
      "131 [C loss: 1.001704] [G loss: 0.999869]\n",
      "132 [C loss: 1.001815] [G loss: 0.999857]\n",
      "133 [C loss: 1.001886] [G loss: 0.999845]\n",
      "134 [C loss: 1.002033] [G loss: 0.999832]\n",
      "135 [C loss: 1.002099] [G loss: 0.999820]\n",
      "136 [C loss: 1.002218] [G loss: 0.999807]\n",
      "137 [C loss: 1.002299] [G loss: 0.999793]\n",
      "138 [C loss: 1.002474] [G loss: 0.999779]\n",
      "139 [C loss: 1.002568] [G loss: 0.999765]\n",
      "140 [C loss: 1.002710] [G loss: 0.999750]\n",
      "141 [C loss: 1.002788] [G loss: 0.999734]\n",
      "142 [C loss: 1.002986] [G loss: 0.999717]\n",
      "143 [C loss: 1.003027] [G loss: 0.999700]\n",
      "144 [C loss: 1.003291] [G loss: 0.999682]\n",
      "145 [C loss: 1.003318] [G loss: 0.999664]\n",
      "146 [C loss: 1.003519] [G loss: 0.999644]\n",
      "147 [C loss: 1.003641] [G loss: 0.999627]\n",
      "148 [C loss: 1.003805] [G loss: 0.999607]\n",
      "149 [C loss: 1.004010] [G loss: 0.999588]\n",
      "150 [C loss: 1.004145] [G loss: 0.999568]\n",
      "151 [C loss: 1.004509] [G loss: 0.999548]\n",
      "152 [C loss: 1.004431] [G loss: 0.999525]\n",
      "153 [C loss: 1.004872] [G loss: 0.999505]\n",
      "154 [C loss: 1.004928] [G loss: 0.999483]\n",
      "155 [C loss: 1.005298] [G loss: 0.999461]\n",
      "156 [C loss: 1.005533] [G loss: 0.999439]\n",
      "157 [C loss: 1.005675] [G loss: 0.999418]\n",
      "158 [C loss: 1.005892] [G loss: 0.999395]\n",
      "159 [C loss: 1.006221] [G loss: 0.999374]\n",
      "160 [C loss: 1.006471] [G loss: 0.999352]\n",
      "161 [C loss: 1.006653] [G loss: 0.999330]\n",
      "162 [C loss: 1.007186] [G loss: 0.999307]\n",
      "163 [C loss: 1.007249] [G loss: 0.999286]\n",
      "164 [C loss: 1.007644] [G loss: 0.999263]\n",
      "165 [C loss: 1.007998] [G loss: 0.999241]\n",
      "166 [C loss: 1.008391] [G loss: 0.999217]\n",
      "167 [C loss: 1.008803] [G loss: 0.999195]\n",
      "168 [C loss: 1.009203] [G loss: 0.999175]\n",
      "169 [C loss: 1.009082] [G loss: 0.999152]\n",
      "170 [C loss: 1.009834] [G loss: 0.999128]\n",
      "171 [C loss: 1.010060] [G loss: 0.999106]\n",
      "172 [C loss: 1.010468] [G loss: 0.999082]\n",
      "173 [C loss: 1.011011] [G loss: 0.999061]\n",
      "174 [C loss: 1.011461] [G loss: 0.999037]\n",
      "175 [C loss: 1.011837] [G loss: 0.999015]\n",
      "176 [C loss: 1.012779] [G loss: 0.998993]\n",
      "177 [C loss: 1.012686] [G loss: 0.998969]\n",
      "178 [C loss: 1.013803] [G loss: 0.998948]\n",
      "179 [C loss: 1.013523] [G loss: 0.998924]\n",
      "180 [C loss: 1.014463] [G loss: 0.998901]\n",
      "181 [C loss: 1.014922] [G loss: 0.998881]\n",
      "182 [C loss: 1.015575] [G loss: 0.998854]\n",
      "183 [C loss: 1.016232] [G loss: 0.998834]\n",
      "184 [C loss: 1.017267] [G loss: 0.998808]\n",
      "185 [C loss: 1.016571] [G loss: 0.998789]\n",
      "186 [C loss: 1.017650] [G loss: 0.998765]\n",
      "187 [C loss: 1.018945] [G loss: 0.998741]\n",
      "188 [C loss: 1.019628] [G loss: 0.998719]\n",
      "189 [C loss: 1.021152] [G loss: 0.998694]\n",
      "190 [C loss: 1.021482] [G loss: 0.998671]\n",
      "191 [C loss: 1.021930] [G loss: 0.998644]\n",
      "192 [C loss: 1.022718] [G loss: 0.998626]\n",
      "193 [C loss: 1.024302] [G loss: 0.998600]\n",
      "194 [C loss: 1.024605] [G loss: 0.998575]\n",
      "195 [C loss: 1.025085] [G loss: 0.998550]\n",
      "196 [C loss: 1.026637] [G loss: 0.998530]\n",
      "197 [C loss: 1.027576] [G loss: 0.998502]\n",
      "198 [C loss: 1.028513] [G loss: 0.998484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 [C loss: 1.029191] [G loss: 0.998459]\n",
      "200 [C loss: 1.030733] [G loss: 0.998433]\n",
      "201 [C loss: 1.031896] [G loss: 0.998406]\n",
      "202 [C loss: 1.032856] [G loss: 0.998383]\n",
      "203 [C loss: 1.033467] [G loss: 0.998357]\n",
      "204 [C loss: 1.034318] [G loss: 0.998339]\n",
      "205 [C loss: 1.035921] [G loss: 0.998314]\n",
      "206 [C loss: 1.036867] [G loss: 0.998291]\n",
      "207 [C loss: 1.039741] [G loss: 0.998264]\n",
      "208 [C loss: 1.041164] [G loss: 0.998244]\n",
      "209 [C loss: 1.042682] [G loss: 0.998214]\n",
      "210 [C loss: 1.043002] [G loss: 0.998185]\n",
      "211 [C loss: 1.044512] [G loss: 0.998169]\n",
      "212 [C loss: 1.046557] [G loss: 0.998140]\n",
      "213 [C loss: 1.046929] [G loss: 0.998112]\n",
      "214 [C loss: 1.050363] [G loss: 0.998095]\n",
      "215 [C loss: 1.049861] [G loss: 0.998066]\n",
      "216 [C loss: 1.054217] [G loss: 0.998045]\n",
      "217 [C loss: 1.053543] [G loss: 0.998015]\n",
      "218 [C loss: 1.056348] [G loss: 0.997991]\n",
      "219 [C loss: 1.060454] [G loss: 0.997970]\n",
      "220 [C loss: 1.062093] [G loss: 0.997943]\n",
      "221 [C loss: 1.063095] [G loss: 0.997919]\n",
      "222 [C loss: 1.063630] [G loss: 0.997888]\n",
      "223 [C loss: 1.065889] [G loss: 0.997857]\n",
      "224 [C loss: 1.070241] [G loss: 0.997831]\n",
      "225 [C loss: 1.073128] [G loss: 0.997815]\n",
      "226 [C loss: 1.074654] [G loss: 0.997786]\n",
      "227 [C loss: 1.073693] [G loss: 0.997761]\n",
      "228 [C loss: 1.079328] [G loss: 0.997737]\n",
      "229 [C loss: 1.082507] [G loss: 0.997713]\n",
      "230 [C loss: 1.083815] [G loss: 0.997690]\n",
      "231 [C loss: 1.085079] [G loss: 0.997659]\n",
      "232 [C loss: 1.085294] [G loss: 0.997628]\n",
      "233 [C loss: 1.091164] [G loss: 0.997607]\n",
      "234 [C loss: 1.097571] [G loss: 0.997583]\n",
      "235 [C loss: 1.097050] [G loss: 0.997556]\n",
      "236 [C loss: 1.103328] [G loss: 0.997526]\n",
      "237 [C loss: 1.104504] [G loss: 0.997501]\n",
      "238 [C loss: 1.104879] [G loss: 0.997471]\n",
      "239 [C loss: 1.113084] [G loss: 0.997432]\n",
      "240 [C loss: 1.113855] [G loss: 0.997412]\n",
      "241 [C loss: 1.118367] [G loss: 0.997386]\n",
      "242 [C loss: 1.123980] [G loss: 0.997361]\n",
      "243 [C loss: 1.127918] [G loss: 0.997333]\n",
      "244 [C loss: 1.130736] [G loss: 0.997302]\n",
      "245 [C loss: 1.134064] [G loss: 0.997276]\n",
      "246 [C loss: 1.136830] [G loss: 0.997249]\n",
      "247 [C loss: 1.139405] [G loss: 0.997220]\n",
      "248 [C loss: 1.149677] [G loss: 0.997192]\n",
      "249 [C loss: 1.153162] [G loss: 0.997172]\n",
      "250 [C loss: 1.154901] [G loss: 0.997143]\n",
      "251 [C loss: 1.156517] [G loss: 0.997113]\n",
      "252 [C loss: 1.163739] [G loss: 0.997090]\n",
      "253 [C loss: 1.170263] [G loss: 0.997062]\n",
      "254 [C loss: 1.176784] [G loss: 0.997033]\n",
      "255 [C loss: 1.184355] [G loss: 0.997005]\n",
      "256 [C loss: 1.184853] [G loss: 0.996971]\n",
      "257 [C loss: 1.191328] [G loss: 0.996948]\n",
      "258 [C loss: 1.193366] [G loss: 0.996928]\n",
      "259 [C loss: 1.206786] [G loss: 0.996899]\n",
      "260 [C loss: 1.202933] [G loss: 0.996883]\n",
      "261 [C loss: 1.210656] [G loss: 0.996846]\n",
      "262 [C loss: 1.221285] [G loss: 0.996818]\n",
      "263 [C loss: 1.230310] [G loss: 0.996795]\n",
      "264 [C loss: 1.228873] [G loss: 0.996775]\n",
      "265 [C loss: 1.235026] [G loss: 0.996752]\n",
      "266 [C loss: 1.238267] [G loss: 0.996727]\n",
      "267 [C loss: 1.249053] [G loss: 0.996690]\n",
      "268 [C loss: 1.252532] [G loss: 0.996671]\n",
      "269 [C loss: 1.254547] [G loss: 0.996644]\n",
      "270 [C loss: 1.269898] [G loss: 0.996624]\n",
      "271 [C loss: 1.277243] [G loss: 0.996590]\n",
      "272 [C loss: 1.285851] [G loss: 0.996568]\n",
      "273 [C loss: 1.296512] [G loss: 0.996612]\n",
      "274 [C loss: 1.302704] [G loss: 0.996576]\n",
      "275 [C loss: 1.304957] [G loss: 0.996755]\n",
      "276 [C loss: 1.322003] [G loss: 0.996811]\n",
      "277 [C loss: 1.319712] [G loss: 0.997428]\n",
      "278 [C loss: 1.333983] [G loss: 0.998065]\n",
      "279 [C loss: 1.334704] [G loss: 0.998485]\n",
      "280 [C loss: 1.353354] [G loss: 0.999655]\n",
      "281 [C loss: 1.349659] [G loss: 1.002022]\n",
      "282 [C loss: 1.354873] [G loss: 1.002861]\n",
      "283 [C loss: 1.360210] [G loss: 1.003677]\n",
      "284 [C loss: 1.385464] [G loss: 1.005714]\n",
      "285 [C loss: 1.392046] [G loss: 1.009137]\n",
      "286 [C loss: 1.393851] [G loss: 1.009836]\n",
      "287 [C loss: 1.403527] [G loss: 1.010023]\n",
      "288 [C loss: 1.417777] [G loss: 1.010516]\n",
      "289 [C loss: 1.419873] [G loss: 1.010543]\n",
      "290 [C loss: 1.429997] [G loss: 1.015968]\n",
      "291 [C loss: 1.440368] [G loss: 1.018195]\n",
      "292 [C loss: 1.446689] [G loss: 1.018016]\n",
      "293 [C loss: 1.454690] [G loss: 1.020585]\n",
      "294 [C loss: 1.456891] [G loss: 1.021475]\n",
      "295 [C loss: 1.460764] [G loss: 1.025236]\n",
      "296 [C loss: 1.487505] [G loss: 1.029332]\n",
      "297 [C loss: 1.489242] [G loss: 1.032202]\n",
      "298 [C loss: 1.473265] [G loss: 1.036905]\n",
      "299 [C loss: 1.499551] [G loss: 1.043717]\n",
      "300 [C loss: 1.521233] [G loss: 1.046731]\n",
      "301 [C loss: 1.525285] [G loss: 1.059562]\n",
      "302 [C loss: 1.546440] [G loss: 1.059337]\n",
      "303 [C loss: 1.530706] [G loss: 1.060451]\n",
      "304 [C loss: 1.552158] [G loss: 1.063033]\n",
      "305 [C loss: 1.591012] [G loss: 1.062498]\n",
      "306 [C loss: 1.565254] [G loss: 1.077251]\n",
      "307 [C loss: 1.580421] [G loss: 1.096511]\n",
      "308 [C loss: 1.574982] [G loss: 1.095704]\n",
      "309 [C loss: 1.593406] [G loss: 1.092492]\n",
      "310 [C loss: 1.597642] [G loss: 1.087104]\n",
      "311 [C loss: 1.646447] [G loss: 1.102654]\n",
      "312 [C loss: 1.619809] [G loss: 1.118510]\n",
      "313 [C loss: 1.613886] [G loss: 1.135164]\n",
      "314 [C loss: 1.639784] [G loss: 1.128691]\n",
      "315 [C loss: 1.632913] [G loss: 1.157228]\n",
      "316 [C loss: 1.656805] [G loss: 1.154942]\n",
      "317 [C loss: 1.657252] [G loss: 1.151645]\n",
      "318 [C loss: 1.681866] [G loss: 1.191232]\n",
      "319 [C loss: 1.660000] [G loss: 1.197334]\n",
      "320 [C loss: 1.666016] [G loss: 1.190409]\n",
      "321 [C loss: 1.698698] [G loss: 1.199285]\n",
      "322 [C loss: 1.710197] [G loss: 1.235787]\n",
      "323 [C loss: 1.693286] [G loss: 1.246545]\n",
      "324 [C loss: 1.693163] [G loss: 1.248774]\n",
      "325 [C loss: 1.655937] [G loss: 1.264185]\n",
      "326 [C loss: 1.693716] [G loss: 1.262557]\n",
      "327 [C loss: 1.679847] [G loss: 1.289619]\n",
      "328 [C loss: 1.706156] [G loss: 1.335092]\n",
      "329 [C loss: 1.692947] [G loss: 1.351601]\n",
      "330 [C loss: 1.705857] [G loss: 1.338345]\n",
      "331 [C loss: 1.686121] [G loss: 1.335833]\n",
      "332 [C loss: 1.721413] [G loss: 1.377661]\n",
      "333 [C loss: 1.709579] [G loss: 1.415860]\n",
      "334 [C loss: 1.704078] [G loss: 1.371742]\n",
      "335 [C loss: 1.732530] [G loss: 1.394671]\n",
      "336 [C loss: 1.659435] [G loss: 1.418186]\n",
      "337 [C loss: 1.730804] [G loss: 1.513550]\n",
      "338 [C loss: 1.709654] [G loss: 1.522366]\n",
      "339 [C loss: 1.698001] [G loss: 1.507972]\n",
      "340 [C loss: 1.701982] [G loss: 1.509385]\n",
      "341 [C loss: 1.716669] [G loss: 1.566969]\n",
      "342 [C loss: 1.684731] [G loss: 1.615432]\n",
      "343 [C loss: 1.697868] [G loss: 1.579808]\n",
      "344 [C loss: 1.716102] [G loss: 1.629668]\n",
      "345 [C loss: 1.685630] [G loss: 1.633202]\n",
      "346 [C loss: 1.692899] [G loss: 1.614926]\n",
      "347 [C loss: 1.641872] [G loss: 1.712757]\n",
      "348 [C loss: 1.666156] [G loss: 1.687011]\n",
      "349 [C loss: 1.624704] [G loss: 1.825370]\n",
      "350 [C loss: 1.700710] [G loss: 1.803286]\n",
      "351 [C loss: 1.638801] [G loss: 1.779466]\n",
      "352 [C loss: 1.629400] [G loss: 1.864165]\n",
      "353 [C loss: 1.619421] [G loss: 1.855671]\n",
      "354 [C loss: 1.581823] [G loss: 1.931011]\n",
      "355 [C loss: 1.629945] [G loss: 1.937045]\n",
      "356 [C loss: 1.590703] [G loss: 1.981846]\n",
      "357 [C loss: 1.606424] [G loss: 1.986494]\n",
      "358 [C loss: 1.503441] [G loss: 2.008051]\n",
      "359 [C loss: 1.584507] [G loss: 2.034128]\n",
      "360 [C loss: 1.569116] [G loss: 2.087989]\n",
      "361 [C loss: 1.630635] [G loss: 2.149864]\n",
      "362 [C loss: 1.519111] [G loss: 2.242482]\n",
      "363 [C loss: 1.537338] [G loss: 2.177473]\n",
      "364 [C loss: 1.521148] [G loss: 2.293035]\n",
      "365 [C loss: 1.495648] [G loss: 2.312162]\n",
      "366 [C loss: 1.506446] [G loss: 2.286036]\n",
      "367 [C loss: 1.350512] [G loss: 2.327945]\n",
      "368 [C loss: 1.456749] [G loss: 2.381757]\n",
      "369 [C loss: 1.398891] [G loss: 2.470718]\n",
      "370 [C loss: 1.424211] [G loss: 2.441799]\n",
      "371 [C loss: 1.359643] [G loss: 2.446371]\n",
      "372 [C loss: 1.451666] [G loss: 2.417471]\n",
      "373 [C loss: 1.379249] [G loss: 2.597217]\n",
      "374 [C loss: 1.314944] [G loss: 2.723568]\n",
      "375 [C loss: 1.391844] [G loss: 2.561519]\n",
      "376 [C loss: 1.260018] [G loss: 2.703018]\n",
      "377 [C loss: 1.258485] [G loss: 2.725419]\n",
      "378 [C loss: 1.260306] [G loss: 2.856552]\n",
      "379 [C loss: 1.302091] [G loss: 2.882496]\n",
      "380 [C loss: 1.349701] [G loss: 2.742798]\n",
      "381 [C loss: 1.109000] [G loss: 2.956094]\n",
      "382 [C loss: 1.090862] [G loss: 2.983025]\n",
      "383 [C loss: 1.193980] [G loss: 3.110179]\n",
      "384 [C loss: 1.200226] [G loss: 2.955058]\n",
      "385 [C loss: 1.158026] [G loss: 3.000454]\n",
      "386 [C loss: 0.992502] [G loss: 3.080666]\n",
      "387 [C loss: 1.108778] [G loss: 3.178862]\n",
      "388 [C loss: 1.085006] [G loss: 3.132428]\n",
      "389 [C loss: 1.135069] [G loss: 3.197770]\n",
      "390 [C loss: 1.130591] [G loss: 3.178450]\n",
      "391 [C loss: 0.968320] [G loss: 3.238962]\n",
      "392 [C loss: 1.018905] [G loss: 3.317696]\n",
      "393 [C loss: 0.993044] [G loss: 3.317993]\n",
      "394 [C loss: 1.080862] [G loss: 3.461286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 [C loss: 0.941371] [G loss: 3.482216]\n",
      "396 [C loss: 1.029515] [G loss: 3.474463]\n",
      "397 [C loss: 0.815254] [G loss: 3.529808]\n",
      "398 [C loss: 0.949855] [G loss: 3.470333]\n",
      "399 [C loss: 0.823610] [G loss: 3.436219]\n",
      "400 [C loss: 0.869663] [G loss: 3.487194]\n",
      "401 [C loss: 0.877480] [G loss: 3.451404]\n",
      "402 [C loss: 0.702220] [G loss: 3.461773]\n",
      "403 [C loss: 0.720172] [G loss: 3.668455]\n",
      "404 [C loss: 0.693384] [G loss: 3.655111]\n",
      "405 [C loss: 0.731478] [G loss: 3.645340]\n",
      "406 [C loss: 0.793424] [G loss: 3.405749]\n",
      "407 [C loss: 0.695722] [G loss: 3.663854]\n",
      "408 [C loss: 0.788032] [G loss: 3.596620]\n",
      "409 [C loss: 0.730693] [G loss: 3.721852]\n",
      "410 [C loss: 0.739372] [G loss: 3.559645]\n",
      "411 [C loss: 0.703223] [G loss: 3.611493]\n",
      "412 [C loss: 0.761944] [G loss: 3.531432]\n",
      "413 [C loss: 0.764208] [G loss: 3.554677]\n",
      "414 [C loss: 0.738106] [G loss: 3.591326]\n",
      "415 [C loss: 0.598332] [G loss: 3.622283]\n",
      "416 [C loss: 0.724599] [G loss: 3.428625]\n",
      "417 [C loss: 0.634483] [G loss: 3.479530]\n",
      "418 [C loss: 0.518140] [G loss: 3.415442]\n",
      "419 [C loss: 0.641790] [G loss: 3.390419]\n",
      "420 [C loss: 0.654448] [G loss: 3.575530]\n",
      "421 [C loss: 0.624533] [G loss: 3.411283]\n",
      "422 [C loss: 0.649214] [G loss: 3.368949]\n",
      "423 [C loss: 0.569641] [G loss: 3.265717]\n",
      "424 [C loss: 0.556453] [G loss: 3.257848]\n",
      "425 [C loss: 0.591409] [G loss: 3.138649]\n",
      "426 [C loss: 0.609420] [G loss: 3.236707]\n",
      "427 [C loss: 0.635962] [G loss: 3.155181]\n",
      "428 [C loss: 0.602488] [G loss: 3.177680]\n",
      "429 [C loss: 0.682145] [G loss: 3.144645]\n",
      "430 [C loss: 0.679470] [G loss: 3.139082]\n",
      "431 [C loss: 0.561639] [G loss: 3.133214]\n",
      "432 [C loss: 0.619814] [G loss: 2.944473]\n",
      "433 [C loss: 0.604233] [G loss: 2.933943]\n",
      "434 [C loss: 0.655174] [G loss: 2.931704]\n",
      "435 [C loss: 0.589996] [G loss: 2.954980]\n",
      "436 [C loss: 0.596546] [G loss: 2.842850]\n",
      "437 [C loss: 0.611033] [G loss: 2.810619]\n",
      "438 [C loss: 0.605511] [G loss: 2.869930]\n",
      "439 [C loss: 0.617854] [G loss: 2.739135]\n",
      "440 [C loss: 0.676508] [G loss: 2.803967]\n",
      "441 [C loss: 0.602511] [G loss: 2.666890]\n",
      "442 [C loss: 0.657570] [G loss: 2.617956]\n",
      "443 [C loss: 0.650260] [G loss: 2.630724]\n",
      "444 [C loss: 0.704929] [G loss: 2.587417]\n",
      "445 [C loss: 0.678352] [G loss: 2.534224]\n",
      "446 [C loss: 0.746828] [G loss: 2.484876]\n",
      "447 [C loss: 0.655910] [G loss: 2.459912]\n",
      "448 [C loss: 0.705784] [G loss: 2.417094]\n",
      "449 [C loss: 0.733423] [G loss: 2.352127]\n",
      "450 [C loss: 0.742570] [G loss: 2.305444]\n",
      "451 [C loss: 0.718670] [G loss: 2.292078]\n",
      "452 [C loss: 0.731146] [G loss: 2.283340]\n",
      "453 [C loss: 0.728641] [G loss: 2.272288]\n",
      "454 [C loss: 0.725060] [G loss: 2.230870]\n",
      "455 [C loss: 0.713056] [G loss: 2.166991]\n",
      "456 [C loss: 0.742772] [G loss: 2.118070]\n",
      "457 [C loss: 0.760351] [G loss: 2.060614]\n",
      "458 [C loss: 0.780955] [G loss: 2.011610]\n",
      "459 [C loss: 0.786199] [G loss: 1.970987]\n",
      "460 [C loss: 0.771319] [G loss: 1.938034]\n",
      "461 [C loss: 0.767293] [G loss: 1.930964]\n",
      "462 [C loss: 0.787151] [G loss: 1.920002]\n",
      "463 [C loss: 0.795552] [G loss: 1.905489]\n",
      "464 [C loss: 0.786225] [G loss: 1.830900]\n",
      "465 [C loss: 0.818092] [G loss: 1.833249]\n",
      "466 [C loss: 0.792068] [G loss: 1.780454]\n",
      "467 [C loss: 0.806283] [G loss: 1.752183]\n",
      "468 [C loss: 0.824201] [G loss: 1.725362]\n",
      "469 [C loss: 0.826751] [G loss: 1.724155]\n",
      "470 [C loss: 0.835465] [G loss: 1.675061]\n",
      "471 [C loss: 0.827380] [G loss: 1.666049]\n",
      "472 [C loss: 0.854864] [G loss: 1.621317]\n",
      "473 [C loss: 0.852275] [G loss: 1.590513]\n",
      "474 [C loss: 0.861185] [G loss: 1.576706]\n",
      "475 [C loss: 0.850515] [G loss: 1.546237]\n",
      "476 [C loss: 0.866992] [G loss: 1.510263]\n",
      "477 [C loss: 0.883877] [G loss: 1.503579]\n",
      "478 [C loss: 0.891215] [G loss: 1.476161]\n",
      "479 [C loss: 0.878056] [G loss: 1.453456]\n",
      "480 [C loss: 0.892781] [G loss: 1.423862]\n",
      "481 [C loss: 0.890645] [G loss: 1.419890]\n",
      "482 [C loss: 0.898987] [G loss: 1.403076]\n",
      "483 [C loss: 0.901177] [G loss: 1.388268]\n",
      "484 [C loss: 0.913721] [G loss: 1.360220]\n",
      "485 [C loss: 0.906602] [G loss: 1.349645]\n",
      "486 [C loss: 0.915168] [G loss: 1.319823]\n",
      "487 [C loss: 0.923066] [G loss: 1.310435]\n",
      "488 [C loss: 0.927050] [G loss: 1.286383]\n",
      "489 [C loss: 0.924447] [G loss: 1.286482]\n",
      "490 [C loss: 0.934148] [G loss: 1.257703]\n",
      "491 [C loss: 0.938212] [G loss: 1.253204]\n",
      "492 [C loss: 0.943937] [G loss: 1.236592]\n",
      "493 [C loss: 0.941698] [G loss: 1.218964]\n",
      "494 [C loss: 0.945812] [G loss: 1.213057]\n",
      "495 [C loss: 0.951664] [G loss: 1.198787]\n",
      "496 [C loss: 0.954136] [G loss: 1.194493]\n",
      "497 [C loss: 0.956988] [G loss: 1.173648]\n",
      "498 [C loss: 0.958638] [G loss: 1.163463]\n",
      "499 [C loss: 0.962196] [G loss: 1.155502]\n",
      "500 [C loss: 0.964689] [G loss: 1.145761]\n",
      "501 [C loss: 0.966023] [G loss: 1.134691]\n",
      "502 [C loss: 0.969744] [G loss: 1.128271]\n",
      "503 [C loss: 0.967330] [G loss: 1.117001]\n",
      "504 [C loss: 0.972181] [G loss: 1.111166]\n",
      "505 [C loss: 0.976131] [G loss: 1.099299]\n",
      "506 [C loss: 0.975281] [G loss: 1.092123]\n",
      "507 [C loss: 0.977742] [G loss: 1.087679]\n",
      "508 [C loss: 0.978982] [G loss: 1.078108]\n",
      "509 [C loss: 0.980537] [G loss: 1.070545]\n",
      "510 [C loss: 0.983244] [G loss: 1.066166]\n",
      "511 [C loss: 0.984424] [G loss: 1.058663]\n",
      "512 [C loss: 0.985593] [G loss: 1.056588]\n",
      "513 [C loss: 0.987620] [G loss: 1.049754]\n",
      "514 [C loss: 0.987818] [G loss: 1.043533]\n",
      "515 [C loss: 0.989320] [G loss: 1.039181]\n",
      "516 [C loss: 0.990214] [G loss: 1.034799]\n",
      "517 [C loss: 0.991165] [G loss: 1.031450]\n",
      "518 [C loss: 0.992713] [G loss: 1.026793]\n",
      "519 [C loss: 0.993918] [G loss: 1.022964]\n",
      "520 [C loss: 0.994380] [G loss: 1.019626]\n",
      "521 [C loss: 0.995240] [G loss: 1.015982]\n",
      "522 [C loss: 0.996158] [G loss: 1.012713]\n",
      "523 [C loss: 0.996756] [G loss: 1.009876]\n",
      "524 [C loss: 0.997621] [G loss: 1.006913]\n",
      "525 [C loss: 0.998168] [G loss: 1.003820]\n",
      "526 [C loss: 0.998831] [G loss: 1.001462]\n",
      "527 [C loss: 0.999497] [G loss: 0.998707]\n",
      "528 [C loss: 1.000161] [G loss: 0.996188]\n",
      "529 [C loss: 1.000897] [G loss: 0.993372]\n",
      "530 [C loss: 1.001581] [G loss: 0.990373]\n",
      "531 [C loss: 1.002348] [G loss: 0.987187]\n",
      "532 [C loss: 1.003186] [G loss: 0.984501]\n",
      "533 [C loss: 1.003685] [G loss: 0.981677]\n",
      "534 [C loss: 1.004267] [G loss: 0.979186]\n",
      "535 [C loss: 1.005467] [G loss: 0.976375]\n",
      "536 [C loss: 1.006487] [G loss: 0.973670]\n",
      "537 [C loss: 1.006637] [G loss: 0.972744]\n",
      "538 [C loss: 1.007748] [G loss: 0.971410]\n",
      "539 [C loss: 1.007109] [G loss: 0.969964]\n",
      "540 [C loss: 1.007774] [G loss: 0.968990]\n",
      "541 [C loss: 1.008456] [G loss: 0.967371]\n",
      "542 [C loss: 1.008889] [G loss: 0.965204]\n",
      "543 [C loss: 1.007943] [G loss: 0.964511]\n",
      "544 [C loss: 1.009640] [G loss: 0.961741]\n",
      "545 [C loss: 1.009861] [G loss: 0.961544]\n",
      "546 [C loss: 1.010783] [G loss: 0.958966]\n",
      "547 [C loss: 1.010754] [G loss: 0.957155]\n",
      "548 [C loss: 1.011458] [G loss: 0.953230]\n",
      "549 [C loss: 1.011443] [G loss: 0.952375]\n",
      "550 [C loss: 1.012929] [G loss: 0.950887]\n",
      "551 [C loss: 1.012611] [G loss: 0.950083]\n",
      "552 [C loss: 1.013464] [G loss: 0.946356]\n",
      "553 [C loss: 1.013793] [G loss: 0.944642]\n",
      "554 [C loss: 1.012699] [G loss: 0.943725]\n",
      "555 [C loss: 1.014278] [G loss: 0.941678]\n",
      "556 [C loss: 1.013419] [G loss: 0.939353]\n",
      "557 [C loss: 1.016607] [G loss: 0.936592]\n",
      "558 [C loss: 1.014574] [G loss: 0.934183]\n",
      "559 [C loss: 1.016620] [G loss: 0.934212]\n",
      "560 [C loss: 1.017075] [G loss: 0.931382]\n",
      "561 [C loss: 1.016032] [G loss: 0.928023]\n",
      "562 [C loss: 1.017926] [G loss: 0.928278]\n",
      "563 [C loss: 1.019531] [G loss: 0.924340]\n",
      "564 [C loss: 1.015812] [G loss: 0.923154]\n",
      "565 [C loss: 1.019121] [G loss: 0.919530]\n",
      "566 [C loss: 1.020528] [G loss: 0.917394]\n",
      "567 [C loss: 1.018954] [G loss: 0.916213]\n",
      "568 [C loss: 1.021286] [G loss: 0.917916]\n",
      "569 [C loss: 1.020424] [G loss: 0.915581]\n",
      "570 [C loss: 1.018671] [G loss: 0.914759]\n",
      "571 [C loss: 1.021692] [G loss: 0.909812]\n",
      "572 [C loss: 1.020469] [G loss: 0.909695]\n",
      "573 [C loss: 1.020033] [G loss: 0.908046]\n",
      "574 [C loss: 1.022998] [G loss: 0.908045]\n",
      "575 [C loss: 1.020021] [G loss: 0.901056]\n",
      "576 [C loss: 1.021482] [G loss: 0.900529]\n",
      "577 [C loss: 1.020780] [G loss: 0.898774]\n",
      "578 [C loss: 1.019722] [G loss: 0.898472]\n",
      "579 [C loss: 1.021308] [G loss: 0.898749]\n",
      "580 [C loss: 1.018110] [G loss: 0.892337]\n",
      "581 [C loss: 1.023155] [G loss: 0.895532]\n",
      "582 [C loss: 1.023475] [G loss: 0.896299]\n",
      "583 [C loss: 1.019978] [G loss: 0.897222]\n",
      "584 [C loss: 1.018452] [G loss: 0.900386]\n",
      "585 [C loss: 1.019997] [G loss: 0.895571]\n",
      "586 [C loss: 1.018317] [G loss: 0.898904]\n",
      "587 [C loss: 1.019074] [G loss: 0.897466]\n",
      "588 [C loss: 1.014333] [G loss: 0.892768]\n",
      "589 [C loss: 1.016292] [G loss: 0.894658]\n",
      "590 [C loss: 1.016073] [G loss: 0.892793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [C loss: 1.014051] [G loss: 0.896692]\n",
      "592 [C loss: 1.021236] [G loss: 0.897131]\n",
      "593 [C loss: 1.013812] [G loss: 0.895096]\n",
      "594 [C loss: 1.012182] [G loss: 0.903457]\n",
      "595 [C loss: 1.013732] [G loss: 0.894433]\n",
      "596 [C loss: 1.009508] [G loss: 0.895457]\n",
      "597 [C loss: 1.010673] [G loss: 0.898152]\n",
      "598 [C loss: 1.005520] [G loss: 0.893376]\n",
      "599 [C loss: 1.003075] [G loss: 0.903993]\n",
      "600 [C loss: 1.010635] [G loss: 0.896942]\n",
      "601 [C loss: 0.997068] [G loss: 0.902793]\n",
      "602 [C loss: 1.003100] [G loss: 0.909763]\n",
      "603 [C loss: 1.002316] [G loss: 0.906233]\n",
      "604 [C loss: 1.000300] [G loss: 0.913096]\n",
      "605 [C loss: 1.000375] [G loss: 0.912717]\n",
      "606 [C loss: 1.001908] [G loss: 0.919356]\n",
      "607 [C loss: 1.001525] [G loss: 0.907248]\n",
      "608 [C loss: 1.002056] [G loss: 0.922677]\n",
      "609 [C loss: 0.997573] [G loss: 0.929490]\n",
      "610 [C loss: 0.998074] [G loss: 0.927597]\n",
      "611 [C loss: 0.997787] [G loss: 0.919768]\n",
      "612 [C loss: 0.996181] [G loss: 0.930372]\n",
      "613 [C loss: 0.999604] [G loss: 0.932534]\n",
      "614 [C loss: 0.995387] [G loss: 0.936905]\n",
      "615 [C loss: 0.996023] [G loss: 0.938525]\n",
      "616 [C loss: 0.995477] [G loss: 0.940218]\n",
      "617 [C loss: 0.995889] [G loss: 0.944014]\n",
      "618 [C loss: 0.995757] [G loss: 0.949523]\n",
      "619 [C loss: 0.993821] [G loss: 0.951426]\n",
      "620 [C loss: 0.994171] [G loss: 0.950277]\n",
      "621 [C loss: 0.995230] [G loss: 0.956159]\n",
      "622 [C loss: 0.994568] [G loss: 0.960829]\n",
      "623 [C loss: 0.993885] [G loss: 0.962151]\n",
      "624 [C loss: 0.993336] [G loss: 0.964093]\n",
      "625 [C loss: 0.994127] [G loss: 0.967641]\n",
      "626 [C loss: 0.995619] [G loss: 0.967764]\n",
      "627 [C loss: 0.995977] [G loss: 0.975223]\n",
      "628 [C loss: 0.997365] [G loss: 0.976525]\n",
      "629 [C loss: 0.996372] [G loss: 0.979928]\n",
      "630 [C loss: 0.997781] [G loss: 0.980399]\n",
      "631 [C loss: 0.998129] [G loss: 0.984806]\n",
      "632 [C loss: 0.998740] [G loss: 0.986723]\n",
      "633 [C loss: 0.999639] [G loss: 0.989977]\n",
      "634 [C loss: 1.000443] [G loss: 0.992131]\n",
      "635 [C loss: 1.000525] [G loss: 0.994432]\n",
      "636 [C loss: 1.001121] [G loss: 0.996394]\n",
      "637 [C loss: 1.001809] [G loss: 0.997763]\n",
      "638 [C loss: 1.003056] [G loss: 1.000585]\n",
      "639 [C loss: 1.003035] [G loss: 1.002463]\n",
      "640 [C loss: 1.004427] [G loss: 1.003990]\n",
      "641 [C loss: 1.003703] [G loss: 1.007558]\n",
      "642 [C loss: 1.002446] [G loss: 1.009989]\n",
      "643 [C loss: 1.002907] [G loss: 1.010309]\n",
      "644 [C loss: 1.005209] [G loss: 1.016833]\n",
      "645 [C loss: 1.002799] [G loss: 1.020497]\n",
      "646 [C loss: 1.003360] [G loss: 1.019971]\n",
      "647 [C loss: 1.004184] [G loss: 1.026437]\n",
      "648 [C loss: 1.002705] [G loss: 1.021729]\n",
      "649 [C loss: 1.000743] [G loss: 1.027762]\n",
      "650 [C loss: 1.002505] [G loss: 1.035599]\n",
      "651 [C loss: 0.999883] [G loss: 1.032687]\n",
      "652 [C loss: 1.001011] [G loss: 1.033707]\n",
      "653 [C loss: 0.999329] [G loss: 1.037389]\n",
      "654 [C loss: 0.994654] [G loss: 1.034646]\n",
      "655 [C loss: 0.997357] [G loss: 1.033759]\n",
      "656 [C loss: 0.995176] [G loss: 1.030292]\n",
      "657 [C loss: 1.001442] [G loss: 1.034790]\n",
      "658 [C loss: 0.993274] [G loss: 1.036588]\n",
      "659 [C loss: 0.999030] [G loss: 1.033907]\n",
      "660 [C loss: 0.997543] [G loss: 1.035734]\n",
      "661 [C loss: 0.998028] [G loss: 1.032045]\n",
      "662 [C loss: 0.994242] [G loss: 1.034851]\n",
      "663 [C loss: 0.994615] [G loss: 1.030669]\n",
      "664 [C loss: 0.997734] [G loss: 1.027469]\n",
      "665 [C loss: 0.995509] [G loss: 1.029032]\n",
      "666 [C loss: 0.994484] [G loss: 1.024943]\n",
      "667 [C loss: 0.996165] [G loss: 1.022328]\n",
      "668 [C loss: 0.995866] [G loss: 1.021927]\n",
      "669 [C loss: 0.995358] [G loss: 1.018392]\n",
      "670 [C loss: 0.996222] [G loss: 1.016795]\n",
      "671 [C loss: 0.995618] [G loss: 1.012744]\n",
      "672 [C loss: 0.996275] [G loss: 1.009850]\n",
      "673 [C loss: 0.997532] [G loss: 1.007423]\n",
      "674 [C loss: 0.997179] [G loss: 1.004469]\n",
      "675 [C loss: 0.998130] [G loss: 1.001126]\n",
      "676 [C loss: 0.998109] [G loss: 0.998285]\n",
      "677 [C loss: 0.998596] [G loss: 0.995852]\n",
      "678 [C loss: 0.999288] [G loss: 0.993175]\n",
      "679 [C loss: 0.999356] [G loss: 0.990906]\n",
      "680 [C loss: 0.999476] [G loss: 0.989060]\n",
      "681 [C loss: 1.000528] [G loss: 0.985273]\n",
      "682 [C loss: 1.000583] [G loss: 0.984387]\n",
      "683 [C loss: 0.999835] [G loss: 0.984566]\n",
      "684 [C loss: 0.999937] [G loss: 0.979496]\n",
      "685 [C loss: 1.001291] [G loss: 0.979123]\n",
      "686 [C loss: 1.000269] [G loss: 0.977567]\n",
      "687 [C loss: 0.999677] [G loss: 0.977444]\n",
      "688 [C loss: 1.000307] [G loss: 0.976060]\n",
      "689 [C loss: 1.001768] [G loss: 0.974185]\n",
      "690 [C loss: 0.999339] [G loss: 0.974126]\n",
      "691 [C loss: 0.999213] [G loss: 0.975777]\n",
      "692 [C loss: 0.998453] [G loss: 0.974416]\n",
      "693 [C loss: 0.999625] [G loss: 0.977396]\n",
      "694 [C loss: 1.000665] [G loss: 0.976679]\n",
      "695 [C loss: 0.999034] [G loss: 0.977462]\n",
      "696 [C loss: 0.998121] [G loss: 0.977488]\n",
      "697 [C loss: 0.998976] [G loss: 0.979391]\n",
      "698 [C loss: 0.996868] [G loss: 0.978875]\n",
      "699 [C loss: 0.997255] [G loss: 0.982175]\n",
      "700 [C loss: 0.996762] [G loss: 0.981293]\n",
      "701 [C loss: 0.998475] [G loss: 0.983302]\n",
      "702 [C loss: 0.998104] [G loss: 0.985902]\n",
      "703 [C loss: 0.997159] [G loss: 0.985239]\n",
      "704 [C loss: 0.997842] [G loss: 0.987161]\n",
      "705 [C loss: 0.998670] [G loss: 0.988461]\n",
      "706 [C loss: 0.997754] [G loss: 0.989047]\n",
      "707 [C loss: 0.997812] [G loss: 0.991528]\n",
      "708 [C loss: 0.998719] [G loss: 0.992410]\n",
      "709 [C loss: 0.998747] [G loss: 0.993522]\n",
      "710 [C loss: 0.998718] [G loss: 0.994618]\n",
      "711 [C loss: 0.998738] [G loss: 0.995363]\n",
      "712 [C loss: 0.999163] [G loss: 0.996534]\n",
      "713 [C loss: 0.999099] [G loss: 0.997078]\n",
      "714 [C loss: 0.999232] [G loss: 0.997551]\n",
      "715 [C loss: 0.999445] [G loss: 0.998189]\n",
      "716 [C loss: 1.000167] [G loss: 0.999164]\n",
      "717 [C loss: 0.999794] [G loss: 1.001264]\n",
      "718 [C loss: 1.000274] [G loss: 1.001053]\n",
      "719 [C loss: 0.999708] [G loss: 1.001967]\n",
      "720 [C loss: 0.999110] [G loss: 1.001883]\n",
      "721 [C loss: 0.999683] [G loss: 1.003135]\n",
      "722 [C loss: 0.999061] [G loss: 1.003482]\n",
      "723 [C loss: 0.999583] [G loss: 1.002975]\n",
      "724 [C loss: 0.998933] [G loss: 1.003342]\n",
      "725 [C loss: 0.999559] [G loss: 1.004416]\n",
      "726 [C loss: 0.998963] [G loss: 1.005066]\n",
      "727 [C loss: 0.999187] [G loss: 1.004253]\n",
      "728 [C loss: 0.999350] [G loss: 1.003889]\n",
      "729 [C loss: 0.999496] [G loss: 1.003707]\n",
      "730 [C loss: 0.998845] [G loss: 1.003517]\n",
      "731 [C loss: 0.999247] [G loss: 1.003086]\n",
      "732 [C loss: 0.998998] [G loss: 1.001678]\n",
      "733 [C loss: 0.999513] [G loss: 1.001299]\n",
      "734 [C loss: 0.999285] [G loss: 1.001088]\n",
      "735 [C loss: 0.999458] [G loss: 0.999605]\n",
      "736 [C loss: 0.999196] [G loss: 0.998717]\n",
      "737 [C loss: 0.999911] [G loss: 0.998249]\n",
      "738 [C loss: 0.999677] [G loss: 0.996954]\n",
      "739 [C loss: 0.999777] [G loss: 0.996240]\n",
      "740 [C loss: 0.999972] [G loss: 0.995263]\n",
      "741 [C loss: 1.000027] [G loss: 0.994531]\n",
      "742 [C loss: 1.000261] [G loss: 0.993662]\n",
      "743 [C loss: 1.000093] [G loss: 0.992605]\n",
      "744 [C loss: 1.000094] [G loss: 0.992279]\n",
      "745 [C loss: 1.000209] [G loss: 0.991399]\n",
      "746 [C loss: 1.000432] [G loss: 0.990389]\n",
      "747 [C loss: 1.000305] [G loss: 0.990436]\n",
      "748 [C loss: 1.000093] [G loss: 0.991071]\n",
      "749 [C loss: 1.000179] [G loss: 0.991370]\n",
      "750 [C loss: 1.000017] [G loss: 0.991591]\n",
      "751 [C loss: 1.000345] [G loss: 0.991422]\n",
      "752 [C loss: 1.000012] [G loss: 0.991331]\n",
      "753 [C loss: 0.999988] [G loss: 0.992070]\n",
      "754 [C loss: 0.999805] [G loss: 0.992378]\n",
      "755 [C loss: 1.000085] [G loss: 0.992946]\n",
      "756 [C loss: 0.999878] [G loss: 0.993514]\n",
      "757 [C loss: 1.000023] [G loss: 0.993902]\n",
      "758 [C loss: 1.000035] [G loss: 0.994470]\n",
      "759 [C loss: 1.000130] [G loss: 0.995201]\n",
      "760 [C loss: 1.000219] [G loss: 0.995965]\n",
      "761 [C loss: 1.000391] [G loss: 0.996429]\n",
      "762 [C loss: 1.000603] [G loss: 0.997132]\n",
      "763 [C loss: 1.000652] [G loss: 0.997428]\n",
      "764 [C loss: 1.000833] [G loss: 0.997802]\n",
      "765 [C loss: 1.000619] [G loss: 0.998108]\n",
      "766 [C loss: 1.000616] [G loss: 0.998573]\n",
      "767 [C loss: 1.000355] [G loss: 0.998923]\n",
      "768 [C loss: 1.000450] [G loss: 0.999115]\n",
      "769 [C loss: 1.000533] [G loss: 0.999965]\n",
      "770 [C loss: 1.000561] [G loss: 0.999602]\n",
      "771 [C loss: 1.000659] [G loss: 1.000158]\n",
      "772 [C loss: 1.000206] [G loss: 0.999829]\n",
      "773 [C loss: 1.000277] [G loss: 1.000509]\n",
      "774 [C loss: 0.999741] [G loss: 0.999611]\n",
      "775 [C loss: 1.000590] [G loss: 0.999304]\n",
      "776 [C loss: 1.000331] [G loss: 0.998875]\n",
      "777 [C loss: 0.999920] [G loss: 0.998012]\n",
      "778 [C loss: 1.000250] [G loss: 0.997370]\n",
      "779 [C loss: 1.000285] [G loss: 0.996923]\n",
      "780 [C loss: 1.000465] [G loss: 0.995960]\n",
      "781 [C loss: 1.000469] [G loss: 0.995384]\n",
      "782 [C loss: 1.000640] [G loss: 0.994497]\n",
      "783 [C loss: 1.000418] [G loss: 0.994501]\n",
      "784 [C loss: 1.000219] [G loss: 0.994025]\n",
      "785 [C loss: 1.000888] [G loss: 0.993800]\n",
      "786 [C loss: 1.000381] [G loss: 0.994135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787 [C loss: 1.000485] [G loss: 0.994308]\n",
      "788 [C loss: 1.000268] [G loss: 0.993652]\n",
      "789 [C loss: 1.000277] [G loss: 0.994277]\n",
      "790 [C loss: 1.000094] [G loss: 0.994925]\n",
      "791 [C loss: 1.000154] [G loss: 0.995266]\n",
      "792 [C loss: 1.000197] [G loss: 0.995917]\n",
      "793 [C loss: 1.000235] [G loss: 0.996142]\n",
      "794 [C loss: 1.000418] [G loss: 0.996072]\n",
      "795 [C loss: 1.000391] [G loss: 0.996664]\n",
      "796 [C loss: 1.000227] [G loss: 0.997085]\n",
      "797 [C loss: 1.000375] [G loss: 0.997405]\n",
      "798 [C loss: 1.000312] [G loss: 0.997681]\n",
      "799 [C loss: 1.000343] [G loss: 0.998216]\n",
      "800 [C loss: 1.000310] [G loss: 0.998034]\n",
      "801 [C loss: 1.000204] [G loss: 0.997778]\n",
      "802 [C loss: 1.000166] [G loss: 0.997561]\n",
      "803 [C loss: 1.000422] [G loss: 0.997866]\n",
      "804 [C loss: 1.000468] [G loss: 0.998074]\n",
      "805 [C loss: 1.000203] [G loss: 0.997979]\n",
      "806 [C loss: 1.000049] [G loss: 0.997889]\n",
      "807 [C loss: 1.000517] [G loss: 0.997406]\n",
      "808 [C loss: 1.000135] [G loss: 0.997291]\n",
      "809 [C loss: 1.000331] [G loss: 0.997129]\n",
      "810 [C loss: 1.000135] [G loss: 0.996437]\n",
      "811 [C loss: 1.000312] [G loss: 0.996185]\n",
      "812 [C loss: 1.000469] [G loss: 0.996361]\n",
      "813 [C loss: 1.000313] [G loss: 0.996400]\n",
      "814 [C loss: 1.000192] [G loss: 0.996119]\n",
      "815 [C loss: 1.000207] [G loss: 0.995940]\n",
      "816 [C loss: 1.000099] [G loss: 0.996320]\n",
      "817 [C loss: 1.000118] [G loss: 0.996074]\n",
      "818 [C loss: 1.000294] [G loss: 0.996328]\n",
      "819 [C loss: 1.000409] [G loss: 0.996248]\n",
      "820 [C loss: 1.000386] [G loss: 0.996548]\n",
      "821 [C loss: 1.000196] [G loss: 0.996784]\n",
      "822 [C loss: 1.000244] [G loss: 0.997172]\n",
      "823 [C loss: 1.000325] [G loss: 0.996838]\n",
      "824 [C loss: 1.000229] [G loss: 0.996227]\n",
      "825 [C loss: 1.000209] [G loss: 0.996766]\n",
      "826 [C loss: 1.000357] [G loss: 0.996416]\n",
      "827 [C loss: 1.000079] [G loss: 0.996750]\n",
      "828 [C loss: 1.000382] [G loss: 0.996810]\n",
      "829 [C loss: 1.000333] [G loss: 0.996696]\n",
      "830 [C loss: 1.000603] [G loss: 0.996840]\n",
      "831 [C loss: 1.000238] [G loss: 0.996520]\n",
      "832 [C loss: 1.000765] [G loss: 0.996673]\n",
      "833 [C loss: 1.000176] [G loss: 0.996594]\n",
      "834 [C loss: 1.000367] [G loss: 0.997096]\n",
      "835 [C loss: 1.000090] [G loss: 0.996989]\n",
      "836 [C loss: 1.000401] [G loss: 0.997048]\n",
      "837 [C loss: 1.000490] [G loss: 0.997298]\n",
      "838 [C loss: 1.000273] [G loss: 0.997613]\n",
      "839 [C loss: 1.000414] [G loss: 0.997179]\n",
      "840 [C loss: 1.000384] [G loss: 0.997435]\n",
      "841 [C loss: 1.000318] [G loss: 0.997366]\n",
      "842 [C loss: 1.000199] [G loss: 0.996798]\n",
      "843 [C loss: 1.000089] [G loss: 0.996762]\n",
      "844 [C loss: 1.000102] [G loss: 0.997025]\n",
      "845 [C loss: 1.000034] [G loss: 0.997304]\n",
      "846 [C loss: 1.000101] [G loss: 0.997165]\n",
      "847 [C loss: 1.000148] [G loss: 0.996917]\n",
      "848 [C loss: 1.000277] [G loss: 0.997326]\n",
      "849 [C loss: 1.000169] [G loss: 0.997354]\n",
      "850 [C loss: 1.000202] [G loss: 0.997647]\n",
      "851 [C loss: 1.000228] [G loss: 0.997485]\n",
      "852 [C loss: 1.000021] [G loss: 0.997396]\n",
      "853 [C loss: 1.000013] [G loss: 0.997286]\n",
      "854 [C loss: 1.000362] [G loss: 0.996975]\n",
      "855 [C loss: 1.000259] [G loss: 0.996560]\n",
      "856 [C loss: 1.000244] [G loss: 0.996718]\n",
      "857 [C loss: 1.000136] [G loss: 0.997120]\n",
      "858 [C loss: 1.000245] [G loss: 0.997268]\n",
      "859 [C loss: 1.000337] [G loss: 0.997140]\n",
      "860 [C loss: 1.000067] [G loss: 0.998179]\n",
      "861 [C loss: 1.000295] [G loss: 0.998001]\n",
      "862 [C loss: 1.000083] [G loss: 0.998027]\n",
      "863 [C loss: 1.000092] [G loss: 0.997786]\n",
      "864 [C loss: 1.000119] [G loss: 0.997403]\n",
      "865 [C loss: 1.000225] [G loss: 0.997342]\n",
      "866 [C loss: 1.000059] [G loss: 0.997459]\n",
      "867 [C loss: 1.000303] [G loss: 0.997260]\n",
      "868 [C loss: 1.000334] [G loss: 0.997290]\n",
      "869 [C loss: 1.000348] [G loss: 0.997060]\n",
      "870 [C loss: 1.000297] [G loss: 0.996831]\n",
      "871 [C loss: 1.000095] [G loss: 0.996804]\n",
      "872 [C loss: 0.999954] [G loss: 0.997175]\n",
      "873 [C loss: 1.000125] [G loss: 0.997433]\n",
      "874 [C loss: 1.000177] [G loss: 0.997646]\n",
      "875 [C loss: 1.000195] [G loss: 0.997584]\n",
      "876 [C loss: 1.000109] [G loss: 0.998158]\n",
      "877 [C loss: 1.000007] [G loss: 0.998312]\n",
      "878 [C loss: 1.000076] [G loss: 0.997742]\n",
      "879 [C loss: 1.000146] [G loss: 0.997523]\n",
      "880 [C loss: 1.000213] [G loss: 0.997293]\n",
      "881 [C loss: 1.000191] [G loss: 0.997583]\n",
      "882 [C loss: 0.999908] [G loss: 0.997249]\n",
      "883 [C loss: 1.000005] [G loss: 0.997182]\n",
      "884 [C loss: 1.000495] [G loss: 0.997253]\n",
      "885 [C loss: 1.000249] [G loss: 0.997159]\n",
      "886 [C loss: 1.000061] [G loss: 0.997131]\n",
      "887 [C loss: 1.000215] [G loss: 0.997111]\n",
      "888 [C loss: 1.000066] [G loss: 0.996877]\n",
      "889 [C loss: 1.000041] [G loss: 0.996910]\n",
      "890 [C loss: 1.000200] [G loss: 0.997119]\n",
      "891 [C loss: 1.000105] [G loss: 0.996656]\n",
      "892 [C loss: 1.000255] [G loss: 0.996788]\n",
      "893 [C loss: 0.999751] [G loss: 0.996933]\n",
      "894 [C loss: 1.000255] [G loss: 0.996938]\n",
      "895 [C loss: 1.000252] [G loss: 0.997362]\n",
      "896 [C loss: 1.000019] [G loss: 0.997193]\n",
      "897 [C loss: 1.000455] [G loss: 0.996809]\n",
      "898 [C loss: 0.999954] [G loss: 0.997001]\n",
      "899 [C loss: 1.000083] [G loss: 0.997341]\n",
      "900 [C loss: 1.000127] [G loss: 0.997748]\n",
      "901 [C loss: 1.000299] [G loss: 0.997311]\n",
      "902 [C loss: 1.000211] [G loss: 0.997929]\n",
      "903 [C loss: 1.000093] [G loss: 0.998339]\n",
      "904 [C loss: 0.999930] [G loss: 0.997986]\n",
      "905 [C loss: 1.000302] [G loss: 0.998153]\n",
      "906 [C loss: 1.000242] [G loss: 0.997947]\n",
      "907 [C loss: 1.000201] [G loss: 0.997564]\n",
      "908 [C loss: 1.000138] [G loss: 0.998320]\n",
      "909 [C loss: 1.000001] [G loss: 0.997829]\n",
      "910 [C loss: 1.000225] [G loss: 0.997357]\n",
      "911 [C loss: 1.000224] [G loss: 0.997179]\n",
      "912 [C loss: 1.000099] [G loss: 0.997218]\n",
      "913 [C loss: 1.000103] [G loss: 0.997615]\n",
      "914 [C loss: 1.000189] [G loss: 0.998129]\n",
      "915 [C loss: 1.000187] [G loss: 0.998224]\n",
      "916 [C loss: 1.000160] [G loss: 0.998143]\n",
      "917 [C loss: 1.000201] [G loss: 0.997753]\n",
      "918 [C loss: 1.000227] [G loss: 0.997237]\n",
      "919 [C loss: 0.999908] [G loss: 0.996836]\n",
      "920 [C loss: 1.000102] [G loss: 0.997302]\n",
      "921 [C loss: 1.000209] [G loss: 0.997054]\n",
      "922 [C loss: 1.000424] [G loss: 0.997232]\n",
      "923 [C loss: 0.999928] [G loss: 0.997332]\n",
      "924 [C loss: 1.000152] [G loss: 0.997199]\n",
      "925 [C loss: 0.999816] [G loss: 0.997282]\n",
      "926 [C loss: 0.999872] [G loss: 0.997436]\n",
      "927 [C loss: 1.000237] [G loss: 0.997323]\n",
      "928 [C loss: 1.000269] [G loss: 0.997408]\n",
      "929 [C loss: 1.000182] [G loss: 0.997230]\n",
      "930 [C loss: 1.000069] [G loss: 0.997433]\n",
      "931 [C loss: 1.000132] [G loss: 0.997272]\n",
      "932 [C loss: 1.000003] [G loss: 0.996959]\n",
      "933 [C loss: 1.000184] [G loss: 0.997323]\n",
      "934 [C loss: 1.000150] [G loss: 0.997014]\n",
      "935 [C loss: 1.000031] [G loss: 0.996933]\n",
      "936 [C loss: 0.999917] [G loss: 0.997529]\n",
      "937 [C loss: 1.000025] [G loss: 0.997339]\n",
      "938 [C loss: 1.000191] [G loss: 0.997312]\n",
      "939 [C loss: 1.000318] [G loss: 0.997384]\n",
      "940 [C loss: 1.000253] [G loss: 0.997123]\n",
      "941 [C loss: 0.999828] [G loss: 0.997152]\n",
      "942 [C loss: 1.000063] [G loss: 0.997185]\n",
      "943 [C loss: 1.000094] [G loss: 0.996917]\n",
      "944 [C loss: 0.999798] [G loss: 0.996969]\n",
      "945 [C loss: 1.000023] [G loss: 0.997057]\n",
      "946 [C loss: 0.999838] [G loss: 0.997305]\n",
      "947 [C loss: 1.000307] [G loss: 0.997215]\n",
      "948 [C loss: 1.000232] [G loss: 0.997409]\n",
      "949 [C loss: 1.000017] [G loss: 0.997367]\n",
      "950 [C loss: 0.999930] [G loss: 0.997587]\n",
      "951 [C loss: 0.999712] [G loss: 0.997104]\n",
      "952 [C loss: 0.999659] [G loss: 0.997629]\n",
      "953 [C loss: 0.999642] [G loss: 0.997661]\n",
      "954 [C loss: 0.999978] [G loss: 0.997602]\n",
      "955 [C loss: 0.999963] [G loss: 0.997719]\n",
      "956 [C loss: 1.000010] [G loss: 0.997488]\n",
      "957 [C loss: 0.999608] [G loss: 0.997453]\n",
      "958 [C loss: 0.999756] [G loss: 0.997702]\n",
      "959 [C loss: 0.999817] [G loss: 0.997576]\n",
      "960 [C loss: 0.999757] [G loss: 0.997761]\n",
      "961 [C loss: 0.999659] [G loss: 0.997904]\n",
      "962 [C loss: 0.999626] [G loss: 0.997956]\n",
      "963 [C loss: 0.999911] [G loss: 0.997677]\n",
      "964 [C loss: 1.000024] [G loss: 0.997493]\n",
      "965 [C loss: 0.999719] [G loss: 0.998069]\n",
      "966 [C loss: 0.999730] [G loss: 0.997973]\n",
      "967 [C loss: 0.999682] [G loss: 0.998061]\n",
      "968 [C loss: 0.999619] [G loss: 0.997978]\n",
      "969 [C loss: 0.999764] [G loss: 0.998241]\n",
      "970 [C loss: 1.000014] [G loss: 0.997386]\n",
      "971 [C loss: 0.999738] [G loss: 0.998392]\n",
      "972 [C loss: 0.999554] [G loss: 0.998332]\n",
      "973 [C loss: 0.999554] [G loss: 0.998228]\n",
      "974 [C loss: 0.999701] [G loss: 0.998431]\n",
      "975 [C loss: 0.999594] [G loss: 0.998507]\n",
      "976 [C loss: 0.999557] [G loss: 0.998748]\n",
      "977 [C loss: 0.999616] [G loss: 0.998911]\n",
      "978 [C loss: 0.999830] [G loss: 0.998520]\n",
      "979 [C loss: 0.999469] [G loss: 0.999037]\n",
      "980 [C loss: 0.999618] [G loss: 0.998631]\n",
      "981 [C loss: 0.999454] [G loss: 0.999221]\n",
      "982 [C loss: 0.999506] [G loss: 0.999362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 [C loss: 0.999639] [G loss: 0.999386]\n",
      "984 [C loss: 0.999545] [G loss: 0.999472]\n",
      "985 [C loss: 0.999597] [G loss: 0.999836]\n",
      "986 [C loss: 0.999696] [G loss: 0.999711]\n",
      "987 [C loss: 0.999636] [G loss: 0.999789]\n",
      "988 [C loss: 0.999569] [G loss: 1.000003]\n",
      "989 [C loss: 0.999711] [G loss: 1.000031]\n",
      "990 [C loss: 0.999726] [G loss: 0.999788]\n",
      "991 [C loss: 0.999607] [G loss: 1.000238]\n",
      "992 [C loss: 0.999618] [G loss: 1.000468]\n",
      "993 [C loss: 0.999611] [G loss: 1.000563]\n",
      "994 [C loss: 0.999685] [G loss: 1.000749]\n",
      "995 [C loss: 0.999725] [G loss: 1.000854]\n",
      "996 [C loss: 0.999647] [G loss: 1.000981]\n",
      "997 [C loss: 0.999642] [G loss: 1.001139]\n",
      "998 [C loss: 0.999669] [G loss: 1.001333]\n",
      "999 [C loss: 0.999677] [G loss: 1.001515]\n",
      "1000 [C loss: 0.999652] [G loss: 1.001692]\n",
      "1001 [C loss: 0.999701] [G loss: 1.001873]\n",
      "1002 [C loss: 0.999698] [G loss: 1.002093]\n",
      "1003 [C loss: 0.999767] [G loss: 1.002334]\n",
      "1004 [C loss: 0.999784] [G loss: 1.002571]\n",
      "1005 [C loss: 0.999804] [G loss: 1.002774]\n",
      "1006 [C loss: 0.999721] [G loss: 1.002878]\n",
      "1007 [C loss: 0.999771] [G loss: 1.002987]\n",
      "1008 [C loss: 0.999759] [G loss: 1.003111]\n",
      "1009 [C loss: 0.999759] [G loss: 1.003034]\n",
      "1010 [C loss: 0.999699] [G loss: 1.002953]\n",
      "1011 [C loss: 0.999737] [G loss: 1.002802]\n",
      "1012 [C loss: 0.999758] [G loss: 1.002741]\n",
      "1013 [C loss: 0.999773] [G loss: 1.002643]\n",
      "1014 [C loss: 0.999764] [G loss: 1.002444]\n",
      "1015 [C loss: 0.999756] [G loss: 1.002362]\n",
      "1016 [C loss: 0.999791] [G loss: 1.002325]\n",
      "1017 [C loss: 0.999793] [G loss: 1.002290]\n",
      "1018 [C loss: 0.999801] [G loss: 1.002351]\n",
      "1019 [C loss: 0.999776] [G loss: 1.002428]\n",
      "1020 [C loss: 0.999797] [G loss: 1.002486]\n",
      "1021 [C loss: 0.999794] [G loss: 1.002542]\n",
      "1022 [C loss: 0.999814] [G loss: 1.002626]\n",
      "1023 [C loss: 0.999817] [G loss: 1.002701]\n",
      "1024 [C loss: 0.999806] [G loss: 1.002722]\n",
      "1025 [C loss: 0.999803] [G loss: 1.002741]\n",
      "1026 [C loss: 0.999808] [G loss: 1.002782]\n",
      "1027 [C loss: 0.999814] [G loss: 1.002749]\n",
      "1028 [C loss: 0.999791] [G loss: 1.002687]\n",
      "1029 [C loss: 0.999820] [G loss: 1.002489]\n",
      "1030 [C loss: 0.999802] [G loss: 1.002431]\n",
      "1031 [C loss: 0.999829] [G loss: 1.002520]\n",
      "1032 [C loss: 0.999802] [G loss: 1.002572]\n",
      "1033 [C loss: 0.999832] [G loss: 1.002597]\n",
      "1034 [C loss: 0.999834] [G loss: 1.002568]\n",
      "1035 [C loss: 0.999811] [G loss: 1.002658]\n",
      "1036 [C loss: 0.999792] [G loss: 1.002655]\n",
      "1037 [C loss: 0.999802] [G loss: 1.002668]\n",
      "1038 [C loss: 0.999811] [G loss: 1.002642]\n",
      "1039 [C loss: 0.999832] [G loss: 1.002669]\n",
      "1040 [C loss: 0.999840] [G loss: 1.002679]\n",
      "1041 [C loss: 0.999816] [G loss: 1.002703]\n",
      "1042 [C loss: 0.999825] [G loss: 1.002707]\n",
      "1043 [C loss: 0.999837] [G loss: 1.002565]\n",
      "1044 [C loss: 0.999814] [G loss: 1.002500]\n",
      "1045 [C loss: 0.999830] [G loss: 1.002545]\n",
      "1046 [C loss: 0.999846] [G loss: 1.002652]\n",
      "1047 [C loss: 0.999848] [G loss: 1.002716]\n",
      "1048 [C loss: 0.999850] [G loss: 1.002806]\n",
      "1049 [C loss: 0.999841] [G loss: 1.002850]\n",
      "1050 [C loss: 0.999849] [G loss: 1.002778]\n",
      "1051 [C loss: 0.999849] [G loss: 1.002722]\n",
      "1052 [C loss: 0.999852] [G loss: 1.002614]\n",
      "1053 [C loss: 0.999857] [G loss: 1.002625]\n",
      "1054 [C loss: 0.999845] [G loss: 1.002722]\n",
      "1055 [C loss: 0.999859] [G loss: 1.002807]\n",
      "1056 [C loss: 0.999865] [G loss: 1.002868]\n",
      "1057 [C loss: 0.999860] [G loss: 1.002836]\n",
      "1058 [C loss: 0.999850] [G loss: 1.002720]\n",
      "1059 [C loss: 0.999857] [G loss: 1.002634]\n",
      "1060 [C loss: 0.999859] [G loss: 1.002598]\n",
      "1061 [C loss: 0.999880] [G loss: 1.002609]\n",
      "1062 [C loss: 0.999864] [G loss: 1.002675]\n",
      "1063 [C loss: 0.999886] [G loss: 1.002723]\n",
      "1064 [C loss: 0.999895] [G loss: 1.002772]\n",
      "1065 [C loss: 0.999865] [G loss: 1.002801]\n",
      "1066 [C loss: 0.999862] [G loss: 1.002719]\n",
      "1067 [C loss: 0.999845] [G loss: 1.002645]\n",
      "1068 [C loss: 0.999859] [G loss: 1.002533]\n",
      "1069 [C loss: 0.999860] [G loss: 1.002535]\n",
      "1070 [C loss: 0.999864] [G loss: 1.002650]\n",
      "1071 [C loss: 0.999893] [G loss: 1.002738]\n",
      "1072 [C loss: 0.999865] [G loss: 1.002725]\n",
      "1073 [C loss: 0.999854] [G loss: 1.002659]\n",
      "1074 [C loss: 0.999884] [G loss: 1.002605]\n",
      "1075 [C loss: 0.999896] [G loss: 1.002629]\n",
      "1076 [C loss: 0.999885] [G loss: 1.002643]\n",
      "1077 [C loss: 0.999895] [G loss: 1.002694]\n",
      "1078 [C loss: 0.999895] [G loss: 1.002733]\n",
      "1079 [C loss: 0.999904] [G loss: 1.002825]\n",
      "1080 [C loss: 0.999877] [G loss: 1.002944]\n",
      "1081 [C loss: 0.999884] [G loss: 1.003028]\n",
      "1082 [C loss: 0.999860] [G loss: 1.003071]\n",
      "1083 [C loss: 0.999836] [G loss: 1.002957]\n",
      "1084 [C loss: 0.999873] [G loss: 1.002828]\n",
      "1085 [C loss: 0.999873] [G loss: 1.002738]\n",
      "1086 [C loss: 0.999867] [G loss: 1.002699]\n",
      "1087 [C loss: 0.999849] [G loss: 1.002707]\n",
      "1088 [C loss: 0.999868] [G loss: 1.002823]\n",
      "1089 [C loss: 0.999880] [G loss: 1.002928]\n",
      "1090 [C loss: 0.999885] [G loss: 1.002982]\n",
      "1091 [C loss: 0.999876] [G loss: 1.002957]\n",
      "1092 [C loss: 0.999879] [G loss: 1.002950]\n",
      "1093 [C loss: 0.999882] [G loss: 1.002883]\n",
      "1094 [C loss: 0.999881] [G loss: 1.002837]\n",
      "1095 [C loss: 0.999894] [G loss: 1.002731]\n",
      "1096 [C loss: 0.999885] [G loss: 1.002754]\n",
      "1097 [C loss: 0.999903] [G loss: 1.002777]\n",
      "1098 [C loss: 0.999899] [G loss: 1.002839]\n",
      "1099 [C loss: 0.999896] [G loss: 1.002898]\n",
      "1100 [C loss: 0.999900] [G loss: 1.002916]\n",
      "1101 [C loss: 0.999907] [G loss: 1.002864]\n",
      "1102 [C loss: 0.999873] [G loss: 1.002816]\n",
      "1103 [C loss: 0.999896] [G loss: 1.002729]\n",
      "1104 [C loss: 0.999916] [G loss: 1.002615]\n",
      "1105 [C loss: 0.999907] [G loss: 1.002623]\n",
      "1106 [C loss: 0.999896] [G loss: 1.002685]\n",
      "1107 [C loss: 0.999910] [G loss: 1.002716]\n",
      "1108 [C loss: 0.999920] [G loss: 1.002739]\n",
      "1109 [C loss: 0.999906] [G loss: 1.002700]\n",
      "1110 [C loss: 0.999898] [G loss: 1.002654]\n",
      "1111 [C loss: 0.999870] [G loss: 1.002665]\n",
      "1112 [C loss: 0.999879] [G loss: 1.002675]\n",
      "1113 [C loss: 0.999891] [G loss: 1.002671]\n",
      "1114 [C loss: 0.999890] [G loss: 1.002687]\n",
      "1115 [C loss: 0.999898] [G loss: 1.002758]\n",
      "1116 [C loss: 0.999903] [G loss: 1.002774]\n",
      "1117 [C loss: 0.999889] [G loss: 1.002701]\n",
      "1118 [C loss: 0.999889] [G loss: 1.002644]\n",
      "1119 [C loss: 0.999883] [G loss: 1.002626]\n",
      "1120 [C loss: 0.999903] [G loss: 1.002658]\n",
      "1121 [C loss: 0.999913] [G loss: 1.002654]\n",
      "1122 [C loss: 0.999887] [G loss: 1.002688]\n",
      "1123 [C loss: 0.999904] [G loss: 1.002676]\n",
      "1124 [C loss: 0.999897] [G loss: 1.002681]\n",
      "1125 [C loss: 0.999892] [G loss: 1.002724]\n",
      "1126 [C loss: 0.999903] [G loss: 1.002752]\n",
      "1127 [C loss: 0.999903] [G loss: 1.002780]\n",
      "1128 [C loss: 0.999899] [G loss: 1.002778]\n",
      "1129 [C loss: 0.999899] [G loss: 1.002757]\n",
      "1130 [C loss: 0.999912] [G loss: 1.002788]\n",
      "1131 [C loss: 0.999912] [G loss: 1.002825]\n",
      "1132 [C loss: 0.999910] [G loss: 1.002785]\n",
      "1133 [C loss: 0.999907] [G loss: 1.002818]\n",
      "1134 [C loss: 0.999898] [G loss: 1.002855]\n",
      "1135 [C loss: 0.999914] [G loss: 1.002898]\n",
      "1136 [C loss: 0.999910] [G loss: 1.002870]\n",
      "1137 [C loss: 0.999907] [G loss: 1.002831]\n",
      "1138 [C loss: 0.999919] [G loss: 1.002806]\n",
      "1139 [C loss: 0.999893] [G loss: 1.002858]\n",
      "1140 [C loss: 0.999894] [G loss: 1.002861]\n",
      "1141 [C loss: 0.999904] [G loss: 1.002877]\n",
      "1142 [C loss: 0.999905] [G loss: 1.002840]\n",
      "1143 [C loss: 0.999902] [G loss: 1.002854]\n",
      "1144 [C loss: 0.999906] [G loss: 1.002897]\n",
      "1145 [C loss: 0.999903] [G loss: 1.002888]\n",
      "1146 [C loss: 0.999913] [G loss: 1.002852]\n",
      "1147 [C loss: 0.999899] [G loss: 1.002822]\n",
      "1148 [C loss: 0.999907] [G loss: 1.002796]\n",
      "1149 [C loss: 0.999910] [G loss: 1.002871]\n",
      "1150 [C loss: 0.999906] [G loss: 1.002885]\n",
      "1151 [C loss: 0.999903] [G loss: 1.002919]\n",
      "1152 [C loss: 0.999916] [G loss: 1.002899]\n",
      "1153 [C loss: 0.999918] [G loss: 1.002827]\n",
      "1154 [C loss: 0.999920] [G loss: 1.002765]\n",
      "1155 [C loss: 0.999905] [G loss: 1.002741]\n",
      "1156 [C loss: 0.999919] [G loss: 1.002861]\n",
      "1157 [C loss: 0.999918] [G loss: 1.002892]\n",
      "1158 [C loss: 0.999903] [G loss: 1.002867]\n",
      "1159 [C loss: 0.999896] [G loss: 1.002881]\n",
      "1160 [C loss: 0.999929] [G loss: 1.002842]\n",
      "1161 [C loss: 0.999910] [G loss: 1.002770]\n",
      "1162 [C loss: 0.999905] [G loss: 1.002728]\n",
      "1163 [C loss: 0.999918] [G loss: 1.002722]\n",
      "1164 [C loss: 0.999910] [G loss: 1.002778]\n",
      "1165 [C loss: 0.999917] [G loss: 1.002796]\n",
      "1166 [C loss: 0.999906] [G loss: 1.002760]\n",
      "1167 [C loss: 0.999928] [G loss: 1.002723]\n",
      "1168 [C loss: 0.999907] [G loss: 1.002715]\n",
      "1169 [C loss: 0.999919] [G loss: 1.002735]\n",
      "1170 [C loss: 0.999915] [G loss: 1.002750]\n",
      "1171 [C loss: 0.999908] [G loss: 1.002725]\n",
      "1172 [C loss: 0.999918] [G loss: 1.002719]\n",
      "1173 [C loss: 0.999907] [G loss: 1.002738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174 [C loss: 0.999914] [G loss: 1.002734]\n",
      "1175 [C loss: 0.999922] [G loss: 1.002688]\n",
      "1176 [C loss: 0.999917] [G loss: 1.002662]\n",
      "1177 [C loss: 0.999914] [G loss: 1.002651]\n",
      "1178 [C loss: 0.999918] [G loss: 1.002577]\n",
      "1179 [C loss: 0.999921] [G loss: 1.002601]\n",
      "1180 [C loss: 0.999932] [G loss: 1.002591]\n",
      "1181 [C loss: 0.999931] [G loss: 1.002518]\n",
      "1182 [C loss: 0.999938] [G loss: 1.002519]\n",
      "1183 [C loss: 0.999926] [G loss: 1.002520]\n",
      "1184 [C loss: 0.999922] [G loss: 1.002524]\n",
      "1185 [C loss: 0.999932] [G loss: 1.002495]\n",
      "1186 [C loss: 0.999918] [G loss: 1.002526]\n",
      "1187 [C loss: 0.999908] [G loss: 1.002554]\n",
      "1188 [C loss: 0.999915] [G loss: 1.002530]\n",
      "1189 [C loss: 0.999904] [G loss: 1.002542]\n",
      "1190 [C loss: 0.999893] [G loss: 1.002542]\n",
      "1191 [C loss: 0.999913] [G loss: 1.002563]\n",
      "1192 [C loss: 0.999917] [G loss: 1.002483]\n",
      "1193 [C loss: 0.999903] [G loss: 1.002578]\n",
      "1194 [C loss: 0.999917] [G loss: 1.002581]\n",
      "1195 [C loss: 0.999887] [G loss: 1.002610]\n",
      "1196 [C loss: 0.999932] [G loss: 1.002623]\n",
      "1197 [C loss: 0.999905] [G loss: 1.002655]\n",
      "1198 [C loss: 0.999903] [G loss: 1.002686]\n",
      "1199 [C loss: 0.999906] [G loss: 1.002682]\n",
      "1200 [C loss: 0.999903] [G loss: 1.002732]\n",
      "1201 [C loss: 0.999908] [G loss: 1.002735]\n",
      "1202 [C loss: 0.999905] [G loss: 1.002732]\n",
      "1203 [C loss: 0.999919] [G loss: 1.002792]\n",
      "1204 [C loss: 0.999913] [G loss: 1.002822]\n",
      "1205 [C loss: 0.999932] [G loss: 1.002856]\n",
      "1206 [C loss: 0.999923] [G loss: 1.002887]\n",
      "1207 [C loss: 0.999913] [G loss: 1.002871]\n",
      "1208 [C loss: 0.999920] [G loss: 1.002848]\n",
      "1209 [C loss: 0.999933] [G loss: 1.002860]\n",
      "1210 [C loss: 0.999925] [G loss: 1.002802]\n",
      "1211 [C loss: 0.999919] [G loss: 1.002767]\n",
      "1212 [C loss: 0.999929] [G loss: 1.002746]\n",
      "1213 [C loss: 0.999923] [G loss: 1.002748]\n",
      "1214 [C loss: 0.999935] [G loss: 1.002745]\n",
      "1215 [C loss: 0.999940] [G loss: 1.002730]\n",
      "1216 [C loss: 0.999934] [G loss: 1.002743]\n",
      "1217 [C loss: 0.999922] [G loss: 1.002758]\n",
      "1218 [C loss: 0.999938] [G loss: 1.002806]\n",
      "1219 [C loss: 0.999935] [G loss: 1.002849]\n",
      "1220 [C loss: 0.999932] [G loss: 1.002819]\n",
      "1221 [C loss: 0.999928] [G loss: 1.002838]\n",
      "1222 [C loss: 0.999915] [G loss: 1.002827]\n",
      "1223 [C loss: 0.999923] [G loss: 1.002790]\n",
      "1224 [C loss: 0.999918] [G loss: 1.002744]\n",
      "1225 [C loss: 0.999915] [G loss: 1.002687]\n",
      "1226 [C loss: 0.999917] [G loss: 1.002698]\n",
      "1227 [C loss: 0.999930] [G loss: 1.002672]\n",
      "1228 [C loss: 0.999920] [G loss: 1.002601]\n",
      "1229 [C loss: 0.999914] [G loss: 1.002626]\n",
      "1230 [C loss: 0.999930] [G loss: 1.002513]\n",
      "1231 [C loss: 0.999967] [G loss: 1.002534]\n",
      "1232 [C loss: 0.999929] [G loss: 1.002559]\n",
      "1233 [C loss: 0.999908] [G loss: 1.002509]\n",
      "1234 [C loss: 0.999934] [G loss: 1.002458]\n",
      "1235 [C loss: 0.999944] [G loss: 1.002428]\n",
      "1236 [C loss: 0.999941] [G loss: 1.002440]\n",
      "1237 [C loss: 0.999925] [G loss: 1.002444]\n",
      "1238 [C loss: 0.999919] [G loss: 1.002395]\n",
      "1239 [C loss: 0.999946] [G loss: 1.002404]\n",
      "1240 [C loss: 0.999937] [G loss: 1.002405]\n",
      "1241 [C loss: 0.999914] [G loss: 1.002373]\n",
      "1242 [C loss: 0.999935] [G loss: 1.002375]\n",
      "1243 [C loss: 0.999925] [G loss: 1.002359]\n",
      "1244 [C loss: 0.999940] [G loss: 1.002335]\n",
      "1245 [C loss: 0.999945] [G loss: 1.002253]\n",
      "1246 [C loss: 0.999933] [G loss: 1.002307]\n",
      "1247 [C loss: 0.999951] [G loss: 1.002304]\n",
      "1248 [C loss: 0.999947] [G loss: 1.002248]\n",
      "1249 [C loss: 0.999946] [G loss: 1.002219]\n",
      "1250 [C loss: 0.999963] [G loss: 1.002213]\n",
      "1251 [C loss: 0.999965] [G loss: 1.002200]\n",
      "1252 [C loss: 0.999962] [G loss: 1.002196]\n",
      "1253 [C loss: 0.999956] [G loss: 1.002193]\n",
      "1254 [C loss: 0.999963] [G loss: 1.002191]\n",
      "1255 [C loss: 0.999966] [G loss: 1.002191]\n",
      "1256 [C loss: 0.999962] [G loss: 1.002194]\n",
      "1257 [C loss: 0.999962] [G loss: 1.002193]\n",
      "1258 [C loss: 0.999971] [G loss: 1.002204]\n",
      "1259 [C loss: 0.999957] [G loss: 1.002217]\n",
      "1260 [C loss: 0.999964] [G loss: 1.002215]\n",
      "1261 [C loss: 0.999956] [G loss: 1.002211]\n",
      "1262 [C loss: 0.999954] [G loss: 1.002214]\n",
      "1263 [C loss: 0.999960] [G loss: 1.002205]\n",
      "1264 [C loss: 0.999957] [G loss: 1.002219]\n",
      "1265 [C loss: 0.999949] [G loss: 1.002211]\n",
      "1266 [C loss: 0.999961] [G loss: 1.002212]\n",
      "1267 [C loss: 0.999959] [G loss: 1.002216]\n",
      "1268 [C loss: 0.999959] [G loss: 1.002213]\n",
      "1269 [C loss: 0.999959] [G loss: 1.002219]\n",
      "1270 [C loss: 0.999942] [G loss: 1.002204]\n",
      "1271 [C loss: 0.999954] [G loss: 1.002210]\n",
      "1272 [C loss: 0.999957] [G loss: 1.002206]\n",
      "1273 [C loss: 0.999956] [G loss: 1.002208]\n",
      "1274 [C loss: 0.999943] [G loss: 1.002219]\n",
      "1275 [C loss: 0.999946] [G loss: 1.002221]\n",
      "1276 [C loss: 0.999942] [G loss: 1.002213]\n",
      "1277 [C loss: 0.999945] [G loss: 1.002255]\n",
      "1278 [C loss: 0.999943] [G loss: 1.002244]\n",
      "1279 [C loss: 0.999939] [G loss: 1.002248]\n",
      "1280 [C loss: 0.999941] [G loss: 1.002217]\n",
      "1281 [C loss: 0.999941] [G loss: 1.002225]\n",
      "1282 [C loss: 0.999948] [G loss: 1.002226]\n",
      "1283 [C loss: 0.999941] [G loss: 1.002216]\n",
      "1284 [C loss: 0.999950] [G loss: 1.002223]\n",
      "1285 [C loss: 0.999940] [G loss: 1.002226]\n",
      "1286 [C loss: 0.999938] [G loss: 1.002230]\n",
      "1287 [C loss: 0.999922] [G loss: 1.002236]\n",
      "1288 [C loss: 0.999940] [G loss: 1.002233]\n",
      "1289 [C loss: 0.999942] [G loss: 1.002228]\n",
      "1290 [C loss: 0.999948] [G loss: 1.002222]\n",
      "1291 [C loss: 0.999952] [G loss: 1.002215]\n",
      "1292 [C loss: 0.999951] [G loss: 1.002205]\n",
      "1293 [C loss: 0.999953] [G loss: 1.002201]\n",
      "1294 [C loss: 0.999957] [G loss: 1.002198]\n",
      "1295 [C loss: 0.999951] [G loss: 1.002199]\n",
      "1296 [C loss: 0.999952] [G loss: 1.002191]\n",
      "1297 [C loss: 0.999952] [G loss: 1.002193]\n",
      "1298 [C loss: 0.999952] [G loss: 1.002190]\n",
      "1299 [C loss: 0.999950] [G loss: 1.002188]\n",
      "1300 [C loss: 0.999952] [G loss: 1.002181]\n",
      "1301 [C loss: 0.999948] [G loss: 1.002187]\n",
      "1302 [C loss: 0.999947] [G loss: 1.002180]\n",
      "1303 [C loss: 0.999950] [G loss: 1.002181]\n",
      "1304 [C loss: 0.999951] [G loss: 1.002180]\n",
      "1305 [C loss: 0.999950] [G loss: 1.002180]\n",
      "1306 [C loss: 0.999953] [G loss: 1.002178]\n",
      "1307 [C loss: 0.999953] [G loss: 1.002176]\n",
      "1308 [C loss: 0.999955] [G loss: 1.002175]\n",
      "1309 [C loss: 0.999957] [G loss: 1.002170]\n",
      "1310 [C loss: 0.999957] [G loss: 1.002170]\n",
      "1311 [C loss: 0.999953] [G loss: 1.002166]\n",
      "1312 [C loss: 0.999956] [G loss: 1.002168]\n",
      "1313 [C loss: 0.999956] [G loss: 1.002164]\n",
      "1314 [C loss: 0.999955] [G loss: 1.002164]\n",
      "1315 [C loss: 0.999956] [G loss: 1.002157]\n",
      "1316 [C loss: 0.999956] [G loss: 1.002158]\n",
      "1317 [C loss: 0.999958] [G loss: 1.002155]\n",
      "1318 [C loss: 0.999957] [G loss: 1.002149]\n",
      "1319 [C loss: 0.999957] [G loss: 1.002147]\n",
      "1320 [C loss: 0.999959] [G loss: 1.002139]\n",
      "1321 [C loss: 0.999959] [G loss: 1.002138]\n",
      "1322 [C loss: 0.999961] [G loss: 1.002132]\n",
      "1323 [C loss: 0.999960] [G loss: 1.002134]\n",
      "1324 [C loss: 0.999960] [G loss: 1.002135]\n",
      "1325 [C loss: 0.999961] [G loss: 1.002135]\n",
      "1326 [C loss: 0.999957] [G loss: 1.002133]\n",
      "1327 [C loss: 0.999957] [G loss: 1.002132]\n",
      "1328 [C loss: 0.999953] [G loss: 1.002125]\n",
      "1329 [C loss: 0.999963] [G loss: 1.002124]\n",
      "1330 [C loss: 0.999965] [G loss: 1.002114]\n",
      "1331 [C loss: 0.999956] [G loss: 1.002120]\n",
      "1332 [C loss: 0.999955] [G loss: 1.002124]\n",
      "1333 [C loss: 0.999962] [G loss: 1.002127]\n",
      "1334 [C loss: 0.999952] [G loss: 1.002116]\n",
      "1335 [C loss: 0.999953] [G loss: 1.002116]\n",
      "1336 [C loss: 0.999963] [G loss: 1.002118]\n",
      "1337 [C loss: 0.999961] [G loss: 1.002123]\n",
      "1338 [C loss: 0.999950] [G loss: 1.002126]\n",
      "1339 [C loss: 0.999953] [G loss: 1.002116]\n",
      "1340 [C loss: 0.999944] [G loss: 1.002123]\n",
      "1341 [C loss: 0.999949] [G loss: 1.002117]\n",
      "1342 [C loss: 0.999951] [G loss: 1.002111]\n",
      "1343 [C loss: 0.999947] [G loss: 1.002126]\n",
      "1344 [C loss: 0.999950] [G loss: 1.002128]\n",
      "1345 [C loss: 0.999946] [G loss: 1.002115]\n",
      "1346 [C loss: 0.999948] [G loss: 1.002126]\n",
      "1347 [C loss: 0.999942] [G loss: 1.002127]\n",
      "1348 [C loss: 0.999947] [G loss: 1.002121]\n",
      "1349 [C loss: 0.999948] [G loss: 1.002116]\n",
      "1350 [C loss: 0.999946] [G loss: 1.002123]\n",
      "1351 [C loss: 0.999940] [G loss: 1.002120]\n",
      "1352 [C loss: 0.999946] [G loss: 1.002118]\n",
      "1353 [C loss: 0.999950] [G loss: 1.002115]\n",
      "1354 [C loss: 0.999949] [G loss: 1.002115]\n",
      "1355 [C loss: 0.999951] [G loss: 1.002110]\n",
      "1356 [C loss: 0.999950] [G loss: 1.002109]\n",
      "1357 [C loss: 0.999950] [G loss: 1.002109]\n",
      "1358 [C loss: 0.999951] [G loss: 1.002105]\n",
      "1359 [C loss: 0.999950] [G loss: 1.002105]\n",
      "1360 [C loss: 0.999951] [G loss: 1.002103]\n",
      "1361 [C loss: 0.999951] [G loss: 1.002102]\n",
      "1362 [C loss: 0.999952] [G loss: 1.002098]\n",
      "1363 [C loss: 0.999952] [G loss: 1.002102]\n",
      "1364 [C loss: 0.999949] [G loss: 1.002101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 [C loss: 0.999952] [G loss: 1.002098]\n",
      "1366 [C loss: 0.999950] [G loss: 1.002098]\n",
      "1367 [C loss: 0.999952] [G loss: 1.002095]\n",
      "1368 [C loss: 0.999952] [G loss: 1.002095]\n",
      "1369 [C loss: 0.999952] [G loss: 1.002092]\n",
      "1370 [C loss: 0.999952] [G loss: 1.002091]\n",
      "1371 [C loss: 0.999954] [G loss: 1.002091]\n",
      "1372 [C loss: 0.999951] [G loss: 1.002088]\n",
      "1373 [C loss: 0.999951] [G loss: 1.002088]\n",
      "1374 [C loss: 0.999952] [G loss: 1.002088]\n",
      "1375 [C loss: 0.999950] [G loss: 1.002086]\n",
      "1376 [C loss: 0.999952] [G loss: 1.002087]\n",
      "1377 [C loss: 0.999952] [G loss: 1.002086]\n",
      "1378 [C loss: 0.999951] [G loss: 1.002086]\n",
      "1379 [C loss: 0.999952] [G loss: 1.002085]\n",
      "1380 [C loss: 0.999952] [G loss: 1.002082]\n",
      "1381 [C loss: 0.999953] [G loss: 1.002083]\n",
      "1382 [C loss: 0.999951] [G loss: 1.002083]\n",
      "1383 [C loss: 0.999952] [G loss: 1.002082]\n",
      "1384 [C loss: 0.999953] [G loss: 1.002081]\n",
      "1385 [C loss: 0.999953] [G loss: 1.002079]\n",
      "1386 [C loss: 0.999951] [G loss: 1.002079]\n",
      "1387 [C loss: 0.999954] [G loss: 1.002078]\n",
      "1388 [C loss: 0.999951] [G loss: 1.002075]\n",
      "1389 [C loss: 0.999953] [G loss: 1.002075]\n",
      "1390 [C loss: 0.999954] [G loss: 1.002072]\n",
      "1391 [C loss: 0.999953] [G loss: 1.002072]\n",
      "1392 [C loss: 0.999953] [G loss: 1.002071]\n",
      "1393 [C loss: 0.999955] [G loss: 1.002070]\n",
      "1394 [C loss: 0.999954] [G loss: 1.002069]\n",
      "1395 [C loss: 0.999953] [G loss: 1.002068]\n",
      "1396 [C loss: 0.999953] [G loss: 1.002067]\n",
      "1397 [C loss: 0.999954] [G loss: 1.002067]\n",
      "1398 [C loss: 0.999955] [G loss: 1.002066]\n",
      "1399 [C loss: 0.999954] [G loss: 1.002067]\n",
      "1400 [C loss: 0.999957] [G loss: 1.002066]\n",
      "1401 [C loss: 0.999963] [G loss: 1.002073]\n",
      "1402 [C loss: 0.999956] [G loss: 1.002071]\n",
      "1403 [C loss: 0.999958] [G loss: 1.002068]\n",
      "1404 [C loss: 0.999955] [G loss: 1.002066]\n",
      "1405 [C loss: 0.999956] [G loss: 1.002061]\n",
      "1406 [C loss: 0.999955] [G loss: 1.002064]\n",
      "1407 [C loss: 0.999954] [G loss: 1.002065]\n",
      "1408 [C loss: 0.999954] [G loss: 1.002070]\n",
      "1409 [C loss: 0.999952] [G loss: 1.002070]\n",
      "1410 [C loss: 0.999952] [G loss: 1.002069]\n",
      "1411 [C loss: 0.999955] [G loss: 1.002067]\n",
      "1412 [C loss: 0.999964] [G loss: 1.002074]\n",
      "1413 [C loss: 0.999958] [G loss: 1.002081]\n",
      "1414 [C loss: 0.999942] [G loss: 1.002091]\n",
      "1415 [C loss: 0.999939] [G loss: 1.002091]\n",
      "1416 [C loss: 0.999950] [G loss: 1.002068]\n",
      "1417 [C loss: 0.999955] [G loss: 1.002062]\n",
      "1418 [C loss: 0.999961] [G loss: 1.002050]\n",
      "1419 [C loss: 0.999960] [G loss: 1.002049]\n",
      "1420 [C loss: 0.999963] [G loss: 1.002046]\n",
      "1421 [C loss: 0.999962] [G loss: 1.002050]\n",
      "1422 [C loss: 0.999962] [G loss: 1.002049]\n",
      "1423 [C loss: 0.999960] [G loss: 1.002052]\n",
      "1424 [C loss: 0.999951] [G loss: 1.002050]\n",
      "1425 [C loss: 0.999953] [G loss: 1.002053]\n",
      "1426 [C loss: 0.999953] [G loss: 1.002056]\n",
      "1427 [C loss: 0.999953] [G loss: 1.002057]\n",
      "1428 [C loss: 0.999949] [G loss: 1.002055]\n",
      "1429 [C loss: 0.999953] [G loss: 1.002049]\n",
      "1430 [C loss: 0.999972] [G loss: 1.002050]\n",
      "1431 [C loss: 1.000006] [G loss: 1.002088]\n",
      "1432 [C loss: 0.999994] [G loss: 1.002131]\n",
      "1433 [C loss: 0.999992] [G loss: 1.002157]\n",
      "1434 [C loss: 0.999997] [G loss: 1.002187]\n",
      "1435 [C loss: 0.999981] [G loss: 1.002186]\n",
      "1436 [C loss: 0.999991] [G loss: 1.002243]\n",
      "1437 [C loss: 0.999985] [G loss: 1.002179]\n",
      "1438 [C loss: 0.999984] [G loss: 1.002203]\n",
      "1439 [C loss: 0.999956] [G loss: 1.002237]\n",
      "1440 [C loss: 0.999965] [G loss: 1.002222]\n",
      "1441 [C loss: 0.999950] [G loss: 1.002266]\n",
      "1442 [C loss: 0.999976] [G loss: 1.002218]\n",
      "1443 [C loss: 0.999962] [G loss: 1.002228]\n",
      "1444 [C loss: 0.999931] [G loss: 1.002239]\n",
      "1445 [C loss: 0.999936] [G loss: 1.002254]\n",
      "1446 [C loss: 0.999950] [G loss: 1.002236]\n",
      "1447 [C loss: 0.999925] [G loss: 1.002200]\n",
      "1448 [C loss: 0.999905] [G loss: 1.002238]\n",
      "1449 [C loss: 0.999930] [G loss: 1.002213]\n",
      "1450 [C loss: 0.999916] [G loss: 1.002193]\n",
      "1451 [C loss: 0.999906] [G loss: 1.002156]\n",
      "1452 [C loss: 0.999933] [G loss: 1.002123]\n",
      "1453 [C loss: 0.999949] [G loss: 1.002084]\n",
      "1454 [C loss: 0.999953] [G loss: 1.002078]\n",
      "1455 [C loss: 0.999958] [G loss: 1.002073]\n",
      "1456 [C loss: 0.999959] [G loss: 1.002070]\n",
      "1457 [C loss: 0.999961] [G loss: 1.002071]\n",
      "1458 [C loss: 0.999961] [G loss: 1.002071]\n",
      "1459 [C loss: 0.999958] [G loss: 1.002072]\n",
      "1460 [C loss: 0.999958] [G loss: 1.002073]\n",
      "1461 [C loss: 0.999958] [G loss: 1.002076]\n",
      "1462 [C loss: 0.999955] [G loss: 1.002080]\n",
      "1463 [C loss: 0.999958] [G loss: 1.002084]\n",
      "1464 [C loss: 0.999953] [G loss: 1.002081]\n",
      "1465 [C loss: 0.999953] [G loss: 1.002084]\n",
      "1466 [C loss: 0.999954] [G loss: 1.002086]\n",
      "1467 [C loss: 0.999950] [G loss: 1.002088]\n",
      "1468 [C loss: 0.999950] [G loss: 1.002090]\n",
      "1469 [C loss: 0.999949] [G loss: 1.002090]\n",
      "1470 [C loss: 0.999954] [G loss: 1.002089]\n",
      "1471 [C loss: 0.999950] [G loss: 1.002082]\n",
      "1472 [C loss: 0.999958] [G loss: 1.002080]\n",
      "1473 [C loss: 0.999975] [G loss: 1.002099]\n",
      "1474 [C loss: 0.999975] [G loss: 1.002124]\n",
      "1475 [C loss: 0.999949] [G loss: 1.002143]\n",
      "1476 [C loss: 0.999939] [G loss: 1.002161]\n",
      "1477 [C loss: 0.999950] [G loss: 1.002153]\n",
      "1478 [C loss: 0.999926] [G loss: 1.002120]\n",
      "1479 [C loss: 0.999951] [G loss: 1.002128]\n",
      "1480 [C loss: 0.999945] [G loss: 1.002085]\n",
      "1481 [C loss: 0.999968] [G loss: 1.002093]\n",
      "1482 [C loss: 0.999966] [G loss: 1.002078]\n",
      "1483 [C loss: 0.999967] [G loss: 1.002070]\n",
      "1484 [C loss: 0.999977] [G loss: 1.002074]\n",
      "1485 [C loss: 0.999972] [G loss: 1.002059]\n",
      "1486 [C loss: 0.999977] [G loss: 1.002061]\n",
      "1487 [C loss: 0.999975] [G loss: 1.002058]\n",
      "1488 [C loss: 0.999978] [G loss: 1.002062]\n",
      "1489 [C loss: 0.999972] [G loss: 1.002058]\n",
      "1490 [C loss: 0.999976] [G loss: 1.002065]\n",
      "1491 [C loss: 0.999964] [G loss: 1.002074]\n",
      "1492 [C loss: 0.999955] [G loss: 1.002069]\n",
      "1493 [C loss: 0.999953] [G loss: 1.002081]\n",
      "1494 [C loss: 0.999958] [G loss: 1.002087]\n",
      "1495 [C loss: 0.999946] [G loss: 1.002091]\n",
      "1496 [C loss: 0.999946] [G loss: 1.002099]\n",
      "1497 [C loss: 0.999946] [G loss: 1.002109]\n",
      "1498 [C loss: 0.999948] [G loss: 1.002114]\n",
      "1499 [C loss: 0.999942] [G loss: 1.002117]\n",
      "1500 [C loss: 0.999945] [G loss: 1.002111]\n",
      "1501 [C loss: 0.999949] [G loss: 1.002114]\n",
      "1502 [C loss: 0.999953] [G loss: 1.002113]\n",
      "1503 [C loss: 0.999951] [G loss: 1.002112]\n",
      "1504 [C loss: 0.999954] [G loss: 1.002110]\n",
      "1505 [C loss: 0.999955] [G loss: 1.002109]\n",
      "1506 [C loss: 0.999957] [G loss: 1.002107]\n",
      "1507 [C loss: 0.999957] [G loss: 1.002105]\n",
      "1508 [C loss: 0.999957] [G loss: 1.002104]\n",
      "1509 [C loss: 0.999960] [G loss: 1.002103]\n",
      "1510 [C loss: 0.999963] [G loss: 1.002103]\n",
      "1511 [C loss: 0.999963] [G loss: 1.002103]\n",
      "1512 [C loss: 0.999965] [G loss: 1.002105]\n",
      "1513 [C loss: 0.999965] [G loss: 1.002107]\n",
      "1514 [C loss: 0.999968] [G loss: 1.002114]\n",
      "1515 [C loss: 0.999966] [G loss: 1.002120]\n",
      "1516 [C loss: 0.999959] [G loss: 1.002124]\n",
      "1517 [C loss: 0.999959] [G loss: 1.002135]\n",
      "1518 [C loss: 0.999963] [G loss: 1.002158]\n",
      "1519 [C loss: 0.999934] [G loss: 1.002149]\n",
      "1520 [C loss: 0.999946] [G loss: 1.002139]\n",
      "1521 [C loss: 0.999953] [G loss: 1.002131]\n",
      "1522 [C loss: 0.999946] [G loss: 1.002120]\n",
      "1523 [C loss: 0.999948] [G loss: 1.002119]\n",
      "1524 [C loss: 0.999949] [G loss: 1.002110]\n",
      "1525 [C loss: 0.999951] [G loss: 1.002103]\n",
      "1526 [C loss: 0.999953] [G loss: 1.002101]\n",
      "1527 [C loss: 0.999955] [G loss: 1.002091]\n",
      "1528 [C loss: 0.999958] [G loss: 1.002082]\n",
      "1529 [C loss: 0.999958] [G loss: 1.002075]\n",
      "1530 [C loss: 0.999959] [G loss: 1.002068]\n",
      "1531 [C loss: 0.999957] [G loss: 1.002069]\n",
      "1532 [C loss: 0.999953] [G loss: 1.002067]\n",
      "1533 [C loss: 0.999956] [G loss: 1.002081]\n",
      "1534 [C loss: 0.999947] [G loss: 1.002076]\n",
      "1535 [C loss: 0.999943] [G loss: 1.002084]\n",
      "1536 [C loss: 0.999945] [G loss: 1.002085]\n",
      "1537 [C loss: 0.999947] [G loss: 1.002086]\n",
      "1538 [C loss: 0.999947] [G loss: 1.002087]\n",
      "1539 [C loss: 0.999950] [G loss: 1.002083]\n",
      "1540 [C loss: 0.999950] [G loss: 1.002081]\n",
      "1541 [C loss: 0.999951] [G loss: 1.002077]\n",
      "1542 [C loss: 0.999951] [G loss: 1.002074]\n",
      "1543 [C loss: 0.999952] [G loss: 1.002074]\n",
      "1544 [C loss: 0.999952] [G loss: 1.002073]\n",
      "1545 [C loss: 0.999953] [G loss: 1.002069]\n",
      "1546 [C loss: 0.999953] [G loss: 1.002068]\n",
      "1547 [C loss: 0.999953] [G loss: 1.002066]\n",
      "1548 [C loss: 0.999953] [G loss: 1.002065]\n",
      "1549 [C loss: 0.999953] [G loss: 1.002063]\n",
      "1550 [C loss: 0.999953] [G loss: 1.002062]\n",
      "1551 [C loss: 0.999953] [G loss: 1.002061]\n",
      "1552 [C loss: 0.999953] [G loss: 1.002059]\n",
      "1553 [C loss: 0.999952] [G loss: 1.002058]\n",
      "1554 [C loss: 0.999953] [G loss: 1.002056]\n",
      "1555 [C loss: 0.999953] [G loss: 1.002055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556 [C loss: 0.999953] [G loss: 1.002054]\n",
      "1557 [C loss: 0.999954] [G loss: 1.002052]\n",
      "1558 [C loss: 0.999953] [G loss: 1.002051]\n",
      "1559 [C loss: 0.999953] [G loss: 1.002050]\n",
      "1560 [C loss: 0.999953] [G loss: 1.002049]\n",
      "1561 [C loss: 0.999953] [G loss: 1.002047]\n",
      "1562 [C loss: 0.999954] [G loss: 1.002046]\n",
      "1563 [C loss: 0.999953] [G loss: 1.002045]\n",
      "1564 [C loss: 0.999954] [G loss: 1.002043]\n",
      "1565 [C loss: 0.999953] [G loss: 1.002042]\n",
      "1566 [C loss: 0.999954] [G loss: 1.002041]\n",
      "1567 [C loss: 0.999954] [G loss: 1.002040]\n",
      "1568 [C loss: 0.999954] [G loss: 1.002038]\n",
      "1569 [C loss: 0.999954] [G loss: 1.002037]\n",
      "1570 [C loss: 0.999954] [G loss: 1.002036]\n",
      "1571 [C loss: 0.999954] [G loss: 1.002035]\n",
      "1572 [C loss: 0.999954] [G loss: 1.002034]\n",
      "1573 [C loss: 0.999954] [G loss: 1.002032]\n",
      "1574 [C loss: 0.999954] [G loss: 1.002031]\n",
      "1575 [C loss: 0.999954] [G loss: 1.002030]\n",
      "1576 [C loss: 0.999954] [G loss: 1.002029]\n",
      "1577 [C loss: 0.999954] [G loss: 1.002028]\n",
      "1578 [C loss: 0.999954] [G loss: 1.002027]\n",
      "1579 [C loss: 0.999955] [G loss: 1.002026]\n",
      "1580 [C loss: 0.999955] [G loss: 1.002025]\n",
      "1581 [C loss: 0.999954] [G loss: 1.002024]\n",
      "1582 [C loss: 0.999954] [G loss: 1.002023]\n",
      "1583 [C loss: 0.999954] [G loss: 1.002022]\n",
      "1584 [C loss: 0.999954] [G loss: 1.002021]\n",
      "1585 [C loss: 0.999955] [G loss: 1.002020]\n",
      "1586 [C loss: 0.999955] [G loss: 1.002019]\n",
      "1587 [C loss: 0.999954] [G loss: 1.002018]\n",
      "1588 [C loss: 0.999954] [G loss: 1.002017]\n",
      "1589 [C loss: 0.999954] [G loss: 1.002016]\n",
      "1590 [C loss: 0.999954] [G loss: 1.002015]\n",
      "1591 [C loss: 0.999954] [G loss: 1.002014]\n",
      "1592 [C loss: 0.999954] [G loss: 1.002013]\n",
      "1593 [C loss: 0.999955] [G loss: 1.002012]\n",
      "1594 [C loss: 0.999955] [G loss: 1.002011]\n",
      "1595 [C loss: 0.999955] [G loss: 1.002010]\n",
      "1596 [C loss: 0.999955] [G loss: 1.002008]\n",
      "1597 [C loss: 0.999957] [G loss: 1.002008]\n",
      "1598 [C loss: 0.999954] [G loss: 1.002007]\n",
      "1599 [C loss: 0.999955] [G loss: 1.002007]\n",
      "1600 [C loss: 0.999954] [G loss: 1.002006]\n",
      "1601 [C loss: 0.999954] [G loss: 1.002007]\n",
      "1602 [C loss: 0.999953] [G loss: 1.002006]\n",
      "1603 [C loss: 0.999952] [G loss: 1.002004]\n",
      "1604 [C loss: 0.999954] [G loss: 1.002002]\n",
      "1605 [C loss: 0.999954] [G loss: 1.002001]\n",
      "1606 [C loss: 0.999955] [G loss: 1.002000]\n",
      "1607 [C loss: 0.999954] [G loss: 1.001998]\n",
      "1608 [C loss: 0.999955] [G loss: 1.001997]\n",
      "1609 [C loss: 0.999955] [G loss: 1.001996]\n",
      "1610 [C loss: 0.999955] [G loss: 1.001995]\n",
      "1611 [C loss: 0.999955] [G loss: 1.001994]\n",
      "1612 [C loss: 0.999956] [G loss: 1.001993]\n",
      "1613 [C loss: 0.999956] [G loss: 1.001992]\n",
      "1614 [C loss: 0.999956] [G loss: 1.001991]\n",
      "1615 [C loss: 0.999955] [G loss: 1.001990]\n",
      "1616 [C loss: 0.999956] [G loss: 1.001989]\n",
      "1617 [C loss: 0.999956] [G loss: 1.001989]\n",
      "1618 [C loss: 0.999956] [G loss: 1.001988]\n",
      "1619 [C loss: 0.999955] [G loss: 1.001988]\n",
      "1620 [C loss: 0.999955] [G loss: 1.001987]\n",
      "1621 [C loss: 0.999955] [G loss: 1.001987]\n",
      "1622 [C loss: 0.999955] [G loss: 1.001987]\n",
      "1623 [C loss: 0.999956] [G loss: 1.001986]\n",
      "1624 [C loss: 0.999955] [G loss: 1.001985]\n",
      "1625 [C loss: 0.999954] [G loss: 1.001985]\n",
      "1626 [C loss: 0.999954] [G loss: 1.001985]\n",
      "1627 [C loss: 0.999955] [G loss: 1.001984]\n",
      "1628 [C loss: 0.999954] [G loss: 1.001983]\n",
      "1629 [C loss: 0.999954] [G loss: 1.001982]\n",
      "1630 [C loss: 0.999954] [G loss: 1.001982]\n",
      "1631 [C loss: 0.999954] [G loss: 1.001981]\n",
      "1632 [C loss: 0.999955] [G loss: 1.001981]\n",
      "1633 [C loss: 0.999954] [G loss: 1.001980]\n",
      "1634 [C loss: 0.999955] [G loss: 1.001979]\n",
      "1635 [C loss: 0.999955] [G loss: 1.001978]\n",
      "1636 [C loss: 0.999955] [G loss: 1.001977]\n",
      "1637 [C loss: 0.999955] [G loss: 1.001975]\n",
      "1638 [C loss: 0.999956] [G loss: 1.001974]\n",
      "1639 [C loss: 0.999957] [G loss: 1.001973]\n",
      "1640 [C loss: 0.999958] [G loss: 1.001972]\n",
      "1641 [C loss: 0.999959] [G loss: 1.001973]\n",
      "1642 [C loss: 0.999960] [G loss: 1.001976]\n",
      "1643 [C loss: 0.999957] [G loss: 1.001979]\n",
      "1644 [C loss: 0.999952] [G loss: 1.001983]\n",
      "1645 [C loss: 0.999949] [G loss: 1.001984]\n",
      "1646 [C loss: 0.999951] [G loss: 1.001984]\n",
      "1647 [C loss: 0.999950] [G loss: 1.001982]\n",
      "1648 [C loss: 0.999951] [G loss: 1.001975]\n",
      "1649 [C loss: 0.999956] [G loss: 1.001967]\n",
      "1650 [C loss: 0.999955] [G loss: 1.001967]\n",
      "1651 [C loss: 0.999956] [G loss: 1.001963]\n",
      "1652 [C loss: 0.999951] [G loss: 1.001966]\n",
      "1653 [C loss: 0.999950] [G loss: 1.001969]\n",
      "1654 [C loss: 0.999952] [G loss: 1.001967]\n",
      "1655 [C loss: 0.999953] [G loss: 1.001966]\n",
      "1656 [C loss: 0.999953] [G loss: 1.001963]\n",
      "1657 [C loss: 0.999954] [G loss: 1.001961]\n",
      "1658 [C loss: 0.999954] [G loss: 1.001960]\n",
      "1659 [C loss: 0.999955] [G loss: 1.001959]\n",
      "1660 [C loss: 0.999953] [G loss: 1.001958]\n",
      "1661 [C loss: 0.999954] [G loss: 1.001956]\n",
      "1662 [C loss: 0.999954] [G loss: 1.001954]\n",
      "1663 [C loss: 0.999954] [G loss: 1.001952]\n",
      "1664 [C loss: 0.999955] [G loss: 1.001950]\n",
      "1665 [C loss: 0.999955] [G loss: 1.001950]\n",
      "1666 [C loss: 0.999955] [G loss: 1.001948]\n",
      "1667 [C loss: 0.999955] [G loss: 1.001947]\n",
      "1668 [C loss: 0.999955] [G loss: 1.001946]\n",
      "1669 [C loss: 0.999955] [G loss: 1.001945]\n",
      "1670 [C loss: 0.999956] [G loss: 1.001944]\n",
      "1671 [C loss: 0.999956] [G loss: 1.001943]\n",
      "1672 [C loss: 0.999956] [G loss: 1.001942]\n",
      "1673 [C loss: 0.999956] [G loss: 1.001941]\n",
      "1674 [C loss: 0.999956] [G loss: 1.001940]\n",
      "1675 [C loss: 0.999956] [G loss: 1.001939]\n",
      "1676 [C loss: 0.999956] [G loss: 1.001938]\n",
      "1677 [C loss: 0.999956] [G loss: 1.001938]\n",
      "1678 [C loss: 0.999957] [G loss: 1.001937]\n",
      "1679 [C loss: 0.999956] [G loss: 1.001936]\n",
      "1680 [C loss: 0.999956] [G loss: 1.001936]\n",
      "1681 [C loss: 0.999956] [G loss: 1.001935]\n",
      "1682 [C loss: 0.999956] [G loss: 1.001935]\n",
      "1683 [C loss: 0.999956] [G loss: 1.001934]\n",
      "1684 [C loss: 0.999955] [G loss: 1.001934]\n",
      "1685 [C loss: 0.999955] [G loss: 1.001933]\n",
      "1686 [C loss: 0.999956] [G loss: 1.001932]\n",
      "1687 [C loss: 0.999956] [G loss: 1.001931]\n",
      "1688 [C loss: 0.999956] [G loss: 1.001931]\n",
      "1689 [C loss: 0.999956] [G loss: 1.001930]\n",
      "1690 [C loss: 0.999956] [G loss: 1.001930]\n",
      "1691 [C loss: 0.999957] [G loss: 1.001929]\n",
      "1692 [C loss: 0.999957] [G loss: 1.001927]\n",
      "1693 [C loss: 0.999956] [G loss: 1.001927]\n",
      "1694 [C loss: 0.999958] [G loss: 1.001926]\n",
      "1695 [C loss: 0.999957] [G loss: 1.001925]\n",
      "1696 [C loss: 0.999956] [G loss: 1.001925]\n",
      "1697 [C loss: 0.999957] [G loss: 1.001925]\n",
      "1698 [C loss: 0.999955] [G loss: 1.001925]\n",
      "1699 [C loss: 0.999956] [G loss: 1.001924]\n",
      "1700 [C loss: 0.999956] [G loss: 1.001923]\n",
      "1701 [C loss: 0.999955] [G loss: 1.001922]\n",
      "1702 [C loss: 0.999956] [G loss: 1.001922]\n",
      "1703 [C loss: 0.999956] [G loss: 1.001921]\n",
      "1704 [C loss: 0.999956] [G loss: 1.001920]\n",
      "1705 [C loss: 0.999956] [G loss: 1.001919]\n",
      "1706 [C loss: 0.999957] [G loss: 1.001919]\n",
      "1707 [C loss: 0.999957] [G loss: 1.001918]\n",
      "1708 [C loss: 0.999956] [G loss: 1.001917]\n",
      "1709 [C loss: 0.999957] [G loss: 1.001916]\n",
      "1710 [C loss: 0.999957] [G loss: 1.001916]\n",
      "1711 [C loss: 0.999956] [G loss: 1.001916]\n",
      "1712 [C loss: 0.999956] [G loss: 1.001916]\n",
      "1713 [C loss: 0.999956] [G loss: 1.001916]\n",
      "1714 [C loss: 0.999956] [G loss: 1.001915]\n",
      "1715 [C loss: 0.999956] [G loss: 1.001915]\n",
      "1716 [C loss: 0.999956] [G loss: 1.001914]\n",
      "1717 [C loss: 0.999955] [G loss: 1.001914]\n",
      "1718 [C loss: 0.999956] [G loss: 1.001913]\n",
      "1719 [C loss: 0.999955] [G loss: 1.001913]\n",
      "1720 [C loss: 0.999956] [G loss: 1.001912]\n",
      "1721 [C loss: 0.999956] [G loss: 1.001911]\n",
      "1722 [C loss: 0.999956] [G loss: 1.001910]\n",
      "1723 [C loss: 0.999957] [G loss: 1.001909]\n",
      "1724 [C loss: 0.999960] [G loss: 1.001908]\n",
      "1725 [C loss: 0.999958] [G loss: 1.001908]\n",
      "1726 [C loss: 0.999956] [G loss: 1.001910]\n",
      "1727 [C loss: 0.999955] [G loss: 1.001910]\n",
      "1728 [C loss: 0.999955] [G loss: 1.001909]\n",
      "1729 [C loss: 0.999955] [G loss: 1.001909]\n",
      "1730 [C loss: 0.999956] [G loss: 1.001909]\n",
      "1731 [C loss: 0.999955] [G loss: 1.001907]\n",
      "1732 [C loss: 0.999956] [G loss: 1.001905]\n",
      "1733 [C loss: 0.999956] [G loss: 1.001904]\n",
      "1734 [C loss: 0.999957] [G loss: 1.001901]\n",
      "1735 [C loss: 0.999958] [G loss: 1.001899]\n",
      "1736 [C loss: 0.999959] [G loss: 1.001897]\n",
      "1737 [C loss: 0.999959] [G loss: 1.001894]\n",
      "1738 [C loss: 0.999959] [G loss: 1.001895]\n",
      "1739 [C loss: 0.999957] [G loss: 1.001896]\n",
      "1740 [C loss: 0.999958] [G loss: 1.001895]\n",
      "1741 [C loss: 0.999957] [G loss: 1.001897]\n",
      "1742 [C loss: 0.999957] [G loss: 1.001898]\n",
      "1743 [C loss: 0.999957] [G loss: 1.001897]\n",
      "1744 [C loss: 0.999958] [G loss: 1.001897]\n",
      "1745 [C loss: 0.999961] [G loss: 1.001897]\n",
      "1746 [C loss: 0.999962] [G loss: 1.001897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747 [C loss: 0.999961] [G loss: 1.001897]\n",
      "1748 [C loss: 0.999959] [G loss: 1.001899]\n",
      "1749 [C loss: 0.999964] [G loss: 1.001900]\n",
      "1750 [C loss: 0.999963] [G loss: 1.001900]\n",
      "1751 [C loss: 0.999960] [G loss: 1.001904]\n",
      "1752 [C loss: 0.999958] [G loss: 1.001904]\n",
      "1753 [C loss: 0.999957] [G loss: 1.001893]\n",
      "1754 [C loss: 0.999960] [G loss: 1.001894]\n",
      "1755 [C loss: 0.999965] [G loss: 1.001886]\n",
      "1756 [C loss: 0.999963] [G loss: 1.001882]\n",
      "1757 [C loss: 0.999962] [G loss: 1.001881]\n",
      "1758 [C loss: 0.999962] [G loss: 1.001881]\n",
      "1759 [C loss: 0.999963] [G loss: 1.001882]\n",
      "1760 [C loss: 0.999961] [G loss: 1.001885]\n",
      "1761 [C loss: 0.999961] [G loss: 1.001886]\n",
      "1762 [C loss: 0.999959] [G loss: 1.001890]\n",
      "1763 [C loss: 0.999956] [G loss: 1.001893]\n",
      "1764 [C loss: 0.999959] [G loss: 1.001893]\n",
      "1765 [C loss: 0.999966] [G loss: 1.001894]\n",
      "1766 [C loss: 0.999965] [G loss: 1.001899]\n",
      "1767 [C loss: 0.999972] [G loss: 1.001909]\n",
      "1768 [C loss: 0.999976] [G loss: 1.001916]\n",
      "1769 [C loss: 0.999972] [G loss: 1.001950]\n",
      "1770 [C loss: 0.999942] [G loss: 1.001953]\n",
      "1771 [C loss: 0.999954] [G loss: 1.001878]\n",
      "1772 [C loss: 0.999962] [G loss: 1.001853]\n",
      "1773 [C loss: 0.999934] [G loss: 1.001906]\n",
      "1774 [C loss: 0.999960] [G loss: 1.001893]\n",
      "1775 [C loss: 0.999956] [G loss: 1.001889]\n",
      "1776 [C loss: 0.999950] [G loss: 1.001873]\n",
      "1777 [C loss: 0.999957] [G loss: 1.001886]\n",
      "1778 [C loss: 0.999958] [G loss: 1.001866]\n",
      "1779 [C loss: 0.999962] [G loss: 1.001878]\n",
      "1780 [C loss: 0.999964] [G loss: 1.001877]\n",
      "1781 [C loss: 0.999971] [G loss: 1.001877]\n",
      "1782 [C loss: 0.999966] [G loss: 1.001869]\n",
      "1783 [C loss: 0.999975] [G loss: 1.001857]\n",
      "1784 [C loss: 0.999962] [G loss: 1.001866]\n",
      "1785 [C loss: 0.999962] [G loss: 1.001861]\n",
      "1786 [C loss: 0.999964] [G loss: 1.001852]\n",
      "1787 [C loss: 0.999978] [G loss: 1.001848]\n",
      "1788 [C loss: 0.999978] [G loss: 1.001852]\n",
      "1789 [C loss: 0.999960] [G loss: 1.001848]\n",
      "1790 [C loss: 0.999960] [G loss: 1.001852]\n",
      "1791 [C loss: 0.999965] [G loss: 1.001861]\n",
      "1792 [C loss: 0.999958] [G loss: 1.001871]\n",
      "1793 [C loss: 0.999948] [G loss: 1.001873]\n",
      "1794 [C loss: 0.999965] [G loss: 1.001876]\n",
      "1795 [C loss: 0.999972] [G loss: 1.001881]\n",
      "1796 [C loss: 0.999969] [G loss: 1.001866]\n",
      "1797 [C loss: 0.999966] [G loss: 1.001876]\n",
      "1798 [C loss: 0.999954] [G loss: 1.001874]\n",
      "1799 [C loss: 0.999965] [G loss: 1.001862]\n",
      "1800 [C loss: 0.999970] [G loss: 1.001854]\n",
      "1801 [C loss: 0.999964] [G loss: 1.001863]\n",
      "1802 [C loss: 0.999959] [G loss: 1.001853]\n",
      "1803 [C loss: 0.999964] [G loss: 1.001859]\n",
      "1804 [C loss: 0.999965] [G loss: 1.001863]\n",
      "1805 [C loss: 0.999960] [G loss: 1.001872]\n",
      "1806 [C loss: 0.999955] [G loss: 1.001866]\n",
      "1807 [C loss: 0.999958] [G loss: 1.001867]\n",
      "1808 [C loss: 0.999955] [G loss: 1.001864]\n",
      "1809 [C loss: 0.999963] [G loss: 1.001868]\n",
      "1810 [C loss: 0.999981] [G loss: 1.001887]\n",
      "1811 [C loss: 0.999981] [G loss: 1.001912]\n",
      "1812 [C loss: 0.999953] [G loss: 1.001966]\n",
      "1813 [C loss: 0.999960] [G loss: 1.002024]\n",
      "1814 [C loss: 0.999939] [G loss: 1.001914]\n",
      "1815 [C loss: 0.999983] [G loss: 1.001802]\n",
      "1816 [C loss: 1.000010] [G loss: 1.001755]\n",
      "1817 [C loss: 0.999938] [G loss: 1.001838]\n",
      "1818 [C loss: 0.999963] [G loss: 1.001857]\n",
      "1819 [C loss: 0.999957] [G loss: 1.001843]\n",
      "1820 [C loss: 0.999976] [G loss: 1.001856]\n",
      "1821 [C loss: 0.999948] [G loss: 1.001839]\n",
      "1822 [C loss: 0.999954] [G loss: 1.001843]\n",
      "1823 [C loss: 0.999961] [G loss: 1.001817]\n",
      "1824 [C loss: 0.999953] [G loss: 1.001805]\n",
      "1825 [C loss: 0.999965] [G loss: 1.001830]\n",
      "1826 [C loss: 0.999962] [G loss: 1.001813]\n",
      "1827 [C loss: 0.999958] [G loss: 1.001825]\n",
      "1828 [C loss: 0.999967] [G loss: 1.001823]\n",
      "1829 [C loss: 0.999961] [G loss: 1.001816]\n",
      "1830 [C loss: 0.999968] [G loss: 1.001808]\n",
      "1831 [C loss: 0.999959] [G loss: 1.001810]\n",
      "1832 [C loss: 0.999960] [G loss: 1.001810]\n",
      "1833 [C loss: 0.999961] [G loss: 1.001808]\n",
      "1834 [C loss: 0.999966] [G loss: 1.001816]\n",
      "1835 [C loss: 0.999954] [G loss: 1.001813]\n",
      "1836 [C loss: 0.999956] [G loss: 1.001817]\n",
      "1837 [C loss: 0.999962] [G loss: 1.001808]\n",
      "1838 [C loss: 0.999955] [G loss: 1.001812]\n",
      "1839 [C loss: 0.999956] [G loss: 1.001821]\n",
      "1840 [C loss: 0.999954] [G loss: 1.001811]\n",
      "1841 [C loss: 0.999959] [G loss: 1.001818]\n",
      "1842 [C loss: 0.999964] [G loss: 1.001818]\n",
      "1843 [C loss: 0.999956] [G loss: 1.001818]\n",
      "1844 [C loss: 0.999968] [G loss: 1.001814]\n",
      "1845 [C loss: 0.999965] [G loss: 1.001822]\n",
      "1846 [C loss: 0.999965] [G loss: 1.001825]\n",
      "1847 [C loss: 0.999964] [G loss: 1.001829]\n",
      "1848 [C loss: 0.999969] [G loss: 1.001831]\n",
      "1849 [C loss: 0.999961] [G loss: 1.001844]\n",
      "1850 [C loss: 0.999948] [G loss: 1.001829]\n",
      "1851 [C loss: 0.999957] [G loss: 1.001829]\n",
      "1852 [C loss: 0.999961] [G loss: 1.001819]\n",
      "1853 [C loss: 0.999958] [G loss: 1.001835]\n",
      "1854 [C loss: 0.999961] [G loss: 1.001809]\n",
      "1855 [C loss: 0.999951] [G loss: 1.001815]\n",
      "1856 [C loss: 0.999963] [G loss: 1.001828]\n",
      "1857 [C loss: 0.999963] [G loss: 1.001809]\n",
      "1858 [C loss: 0.999972] [G loss: 1.001744]\n",
      "1859 [C loss: 0.999966] [G loss: 1.001789]\n",
      "1860 [C loss: 0.999961] [G loss: 1.001808]\n",
      "1861 [C loss: 0.999962] [G loss: 1.001803]\n",
      "1862 [C loss: 0.999957] [G loss: 1.001803]\n",
      "1863 [C loss: 0.999956] [G loss: 1.001797]\n",
      "1864 [C loss: 0.999961] [G loss: 1.001805]\n",
      "1865 [C loss: 0.999963] [G loss: 1.001788]\n",
      "1866 [C loss: 0.999958] [G loss: 1.001794]\n",
      "1867 [C loss: 0.999961] [G loss: 1.001787]\n",
      "1868 [C loss: 0.999976] [G loss: 1.001791]\n",
      "1869 [C loss: 0.999971] [G loss: 1.001787]\n",
      "1870 [C loss: 0.999962] [G loss: 1.001782]\n",
      "1871 [C loss: 0.999964] [G loss: 1.001771]\n",
      "1872 [C loss: 0.999968] [G loss: 1.001770]\n",
      "1873 [C loss: 0.999960] [G loss: 1.001774]\n",
      "1874 [C loss: 0.999964] [G loss: 1.001774]\n",
      "1875 [C loss: 0.999958] [G loss: 1.001770]\n",
      "1876 [C loss: 0.999967] [G loss: 1.001766]\n",
      "1877 [C loss: 0.999961] [G loss: 1.001767]\n",
      "1878 [C loss: 0.999987] [G loss: 1.001771]\n",
      "1879 [C loss: 0.999971] [G loss: 1.001785]\n",
      "1880 [C loss: 0.999960] [G loss: 1.001781]\n",
      "1881 [C loss: 0.999971] [G loss: 1.001754]\n",
      "1882 [C loss: 0.999976] [G loss: 1.001775]\n",
      "1883 [C loss: 0.999978] [G loss: 1.001758]\n",
      "1884 [C loss: 0.999968] [G loss: 1.001738]\n",
      "1885 [C loss: 0.999979] [G loss: 1.001757]\n",
      "1886 [C loss: 0.999982] [G loss: 1.001764]\n",
      "1887 [C loss: 0.999975] [G loss: 1.001766]\n",
      "1888 [C loss: 0.999990] [G loss: 1.001786]\n",
      "1889 [C loss: 0.999956] [G loss: 1.001770]\n",
      "1890 [C loss: 0.999966] [G loss: 1.001777]\n",
      "1891 [C loss: 0.999970] [G loss: 1.001771]\n",
      "1892 [C loss: 0.999980] [G loss: 1.001724]\n",
      "1893 [C loss: 0.999955] [G loss: 1.001765]\n",
      "1894 [C loss: 0.999971] [G loss: 1.001753]\n",
      "1895 [C loss: 0.999964] [G loss: 1.001756]\n",
      "1896 [C loss: 0.999961] [G loss: 1.001762]\n",
      "1897 [C loss: 0.999963] [G loss: 1.001764]\n",
      "1898 [C loss: 0.999958] [G loss: 1.001766]\n",
      "1899 [C loss: 0.999959] [G loss: 1.001738]\n",
      "1900 [C loss: 0.999973] [G loss: 1.001738]\n",
      "1901 [C loss: 0.999952] [G loss: 1.001748]\n",
      "1902 [C loss: 0.999961] [G loss: 1.001747]\n",
      "1903 [C loss: 0.999981] [G loss: 1.001757]\n",
      "1904 [C loss: 0.999961] [G loss: 1.001772]\n",
      "1905 [C loss: 0.999969] [G loss: 1.001784]\n",
      "1906 [C loss: 0.999969] [G loss: 1.001758]\n",
      "1907 [C loss: 0.999956] [G loss: 1.001745]\n",
      "1908 [C loss: 0.999953] [G loss: 1.001739]\n",
      "1909 [C loss: 0.999963] [G loss: 1.001731]\n",
      "1910 [C loss: 0.999959] [G loss: 1.001738]\n",
      "1911 [C loss: 0.999975] [G loss: 1.001749]\n",
      "1912 [C loss: 0.999965] [G loss: 1.001744]\n",
      "1913 [C loss: 0.999972] [G loss: 1.001725]\n",
      "1914 [C loss: 0.999993] [G loss: 1.001702]\n",
      "1915 [C loss: 0.999973] [G loss: 1.001750]\n",
      "1916 [C loss: 0.999983] [G loss: 1.001744]\n",
      "1917 [C loss: 0.999970] [G loss: 1.001739]\n",
      "1918 [C loss: 0.999966] [G loss: 1.001723]\n",
      "1919 [C loss: 0.999980] [G loss: 1.001706]\n",
      "1920 [C loss: 0.999981] [G loss: 1.001692]\n",
      "1921 [C loss: 0.999996] [G loss: 1.001715]\n",
      "1922 [C loss: 0.999970] [G loss: 1.001717]\n",
      "1923 [C loss: 0.999986] [G loss: 1.001721]\n",
      "1924 [C loss: 0.999995] [G loss: 1.001735]\n",
      "1925 [C loss: 0.999976] [G loss: 1.001747]\n",
      "1926 [C loss: 0.999975] [G loss: 1.001754]\n",
      "1927 [C loss: 0.999983] [G loss: 1.001738]\n",
      "1928 [C loss: 0.999963] [G loss: 1.001749]\n",
      "1929 [C loss: 0.999980] [G loss: 1.001761]\n",
      "1930 [C loss: 0.999989] [G loss: 1.001745]\n",
      "1931 [C loss: 0.999977] [G loss: 1.001776]\n",
      "1932 [C loss: 0.999989] [G loss: 1.001773]\n",
      "1933 [C loss: 0.999987] [G loss: 1.001797]\n",
      "1934 [C loss: 0.999954] [G loss: 1.001785]\n",
      "1935 [C loss: 0.999986] [G loss: 1.001766]\n",
      "1936 [C loss: 0.999983] [G loss: 1.001622]\n",
      "1937 [C loss: 0.999968] [G loss: 1.001794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938 [C loss: 0.999970] [G loss: 1.001812]\n",
      "1939 [C loss: 0.999963] [G loss: 1.001829]\n",
      "1940 [C loss: 0.999999] [G loss: 1.001794]\n",
      "1941 [C loss: 0.999961] [G loss: 1.001829]\n",
      "1942 [C loss: 0.999950] [G loss: 1.001787]\n",
      "1943 [C loss: 0.999958] [G loss: 1.001770]\n",
      "1944 [C loss: 0.999969] [G loss: 1.001766]\n",
      "1945 [C loss: 0.999971] [G loss: 1.001749]\n",
      "1946 [C loss: 0.999979] [G loss: 1.001688]\n",
      "1947 [C loss: 0.999997] [G loss: 1.001692]\n",
      "1948 [C loss: 0.999966] [G loss: 1.001716]\n",
      "1949 [C loss: 0.999978] [G loss: 1.001709]\n",
      "1950 [C loss: 0.999971] [G loss: 1.001730]\n",
      "1951 [C loss: 0.999960] [G loss: 1.001728]\n",
      "1952 [C loss: 0.999967] [G loss: 1.001718]\n",
      "1953 [C loss: 0.999970] [G loss: 1.001716]\n",
      "1954 [C loss: 0.999959] [G loss: 1.001705]\n",
      "1955 [C loss: 0.999975] [G loss: 1.001707]\n",
      "1956 [C loss: 0.999988] [G loss: 1.001679]\n",
      "1957 [C loss: 0.999976] [G loss: 1.001677]\n",
      "1958 [C loss: 0.999984] [G loss: 1.001691]\n",
      "1959 [C loss: 0.999976] [G loss: 1.001694]\n",
      "1960 [C loss: 0.999974] [G loss: 1.001658]\n",
      "1961 [C loss: 0.999981] [G loss: 1.001699]\n",
      "1962 [C loss: 0.999975] [G loss: 1.001707]\n",
      "1963 [C loss: 0.999982] [G loss: 1.001721]\n",
      "1964 [C loss: 0.999971] [G loss: 1.001735]\n",
      "1965 [C loss: 0.999952] [G loss: 1.001724]\n",
      "1966 [C loss: 0.999970] [G loss: 1.001716]\n",
      "1967 [C loss: 0.999966] [G loss: 1.001697]\n",
      "1968 [C loss: 0.999976] [G loss: 1.001697]\n",
      "1969 [C loss: 0.999986] [G loss: 1.001718]\n",
      "1970 [C loss: 0.999963] [G loss: 1.001704]\n",
      "1971 [C loss: 0.999973] [G loss: 1.001724]\n",
      "1972 [C loss: 0.999980] [G loss: 1.001735]\n",
      "1973 [C loss: 0.999973] [G loss: 1.001731]\n",
      "1974 [C loss: 0.999963] [G loss: 1.001720]\n",
      "1975 [C loss: 0.999954] [G loss: 1.001716]\n",
      "1976 [C loss: 0.999972] [G loss: 1.001715]\n",
      "1977 [C loss: 0.999970] [G loss: 1.001715]\n",
      "1978 [C loss: 0.999974] [G loss: 1.001708]\n",
      "1979 [C loss: 0.999967] [G loss: 1.001710]\n",
      "1980 [C loss: 0.999975] [G loss: 1.001749]\n",
      "1981 [C loss: 0.999953] [G loss: 1.001722]\n",
      "1982 [C loss: 0.999960] [G loss: 1.001770]\n",
      "1983 [C loss: 0.999987] [G loss: 1.001764]\n",
      "1984 [C loss: 0.999964] [G loss: 1.001761]\n",
      "1985 [C loss: 0.999965] [G loss: 1.001754]\n",
      "1986 [C loss: 0.999958] [G loss: 1.001700]\n",
      "1987 [C loss: 0.999948] [G loss: 1.001705]\n",
      "1988 [C loss: 0.999973] [G loss: 1.001650]\n",
      "1989 [C loss: 0.999965] [G loss: 1.001675]\n",
      "1990 [C loss: 0.999975] [G loss: 1.001658]\n",
      "1991 [C loss: 0.999987] [G loss: 1.001672]\n",
      "1992 [C loss: 0.999966] [G loss: 1.001675]\n",
      "1993 [C loss: 0.999971] [G loss: 1.001693]\n",
      "1994 [C loss: 0.999980] [G loss: 1.001686]\n",
      "1995 [C loss: 0.999968] [G loss: 1.001695]\n",
      "1996 [C loss: 0.999987] [G loss: 1.001720]\n",
      "1997 [C loss: 0.999958] [G loss: 1.001721]\n",
      "1998 [C loss: 0.999983] [G loss: 1.001729]\n",
      "1999 [C loss: 0.999963] [G loss: 1.001709]\n",
      "2000 [C loss: 0.999988] [G loss: 1.001723]\n",
      "2001 [C loss: 0.999979] [G loss: 1.001699]\n",
      "2002 [C loss: 0.999974] [G loss: 1.001679]\n",
      "2003 [C loss: 0.999982] [G loss: 1.001683]\n",
      "2004 [C loss: 0.999979] [G loss: 1.001658]\n",
      "2005 [C loss: 0.999968] [G loss: 1.001655]\n",
      "2006 [C loss: 0.999986] [G loss: 1.001656]\n",
      "2007 [C loss: 0.999992] [G loss: 1.001653]\n",
      "2008 [C loss: 0.999988] [G loss: 1.001657]\n",
      "2009 [C loss: 0.999986] [G loss: 1.001650]\n",
      "2010 [C loss: 1.000001] [G loss: 1.001684]\n",
      "2011 [C loss: 0.999983] [G loss: 1.001654]\n",
      "2012 [C loss: 0.999994] [G loss: 1.001645]\n",
      "2013 [C loss: 0.999984] [G loss: 1.001615]\n",
      "2014 [C loss: 0.999993] [G loss: 1.001597]\n",
      "2015 [C loss: 0.999996] [G loss: 1.001591]\n",
      "2016 [C loss: 0.999983] [G loss: 1.001615]\n",
      "2017 [C loss: 0.999996] [G loss: 1.001616]\n",
      "2018 [C loss: 0.999998] [G loss: 1.001622]\n",
      "2019 [C loss: 1.000002] [G loss: 1.001632]\n",
      "2020 [C loss: 0.999990] [G loss: 1.001636]\n",
      "2021 [C loss: 0.999992] [G loss: 1.001639]\n",
      "2022 [C loss: 0.999980] [G loss: 1.001635]\n",
      "2023 [C loss: 0.999980] [G loss: 1.001616]\n",
      "2024 [C loss: 0.999985] [G loss: 1.001623]\n",
      "2025 [C loss: 0.999978] [G loss: 1.001634]\n",
      "2026 [C loss: 0.999980] [G loss: 1.001636]\n",
      "2027 [C loss: 0.999997] [G loss: 1.001658]\n",
      "2028 [C loss: 0.999980] [G loss: 1.001630]\n",
      "2029 [C loss: 0.999988] [G loss: 1.001595]\n",
      "2030 [C loss: 0.999991] [G loss: 1.001601]\n",
      "2031 [C loss: 0.999984] [G loss: 1.001595]\n",
      "2032 [C loss: 0.999987] [G loss: 1.001613]\n",
      "2033 [C loss: 0.999981] [G loss: 1.001618]\n",
      "2034 [C loss: 0.999978] [G loss: 1.001632]\n",
      "2035 [C loss: 0.999992] [G loss: 1.001647]\n",
      "2036 [C loss: 0.999985] [G loss: 1.001647]\n",
      "2037 [C loss: 0.999993] [G loss: 1.001652]\n",
      "2038 [C loss: 0.999991] [G loss: 1.001647]\n",
      "2039 [C loss: 0.999987] [G loss: 1.001654]\n",
      "2040 [C loss: 0.999982] [G loss: 1.001656]\n",
      "2041 [C loss: 0.999980] [G loss: 1.001637]\n",
      "2042 [C loss: 0.999987] [G loss: 1.001623]\n",
      "2043 [C loss: 0.999981] [G loss: 1.001633]\n",
      "2044 [C loss: 0.999977] [G loss: 1.001614]\n",
      "2045 [C loss: 0.999993] [G loss: 1.001628]\n",
      "2046 [C loss: 0.999986] [G loss: 1.001639]\n",
      "2047 [C loss: 0.999987] [G loss: 1.001642]\n",
      "2048 [C loss: 0.999984] [G loss: 1.001634]\n",
      "2049 [C loss: 0.999983] [G loss: 1.001644]\n",
      "2050 [C loss: 0.999992] [G loss: 1.001645]\n",
      "2051 [C loss: 0.999983] [G loss: 1.001641]\n",
      "2052 [C loss: 0.999989] [G loss: 1.001661]\n",
      "2053 [C loss: 0.999990] [G loss: 1.001680]\n",
      "2054 [C loss: 0.999990] [G loss: 1.001679]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c7bd58dacbbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n\u001b[0;32m----> 6\u001b[0;31m            img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-795212c142af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value, img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                                   critic_loss_fake)\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-795212c142af>\u001b[0m in \u001b[0;36m_clip_weights\u001b[0;34m(self, clip_value)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m#             if 'minibatch_discrimination' not in l.name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \"\"\"\n\u001b[1;32m   1227\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \"\"\"\n\u001b[1;32m   2319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wgan = WGAN(timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir)\n",
    "# gan, generator, critic = wgan.restore_training()\n",
    "gan, generator, critic = wgan.build_models(generator_lr, critic_lr)\n",
    "        \n",
    "losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
