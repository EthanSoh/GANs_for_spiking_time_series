{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import Lambda, LSTM, RepeatVector, Dense, TimeDistributed, Bidirectional, concatenate,\\\n",
    "Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, Activation, Flatten, Reshape\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self, timesteps, latent_dim, generator_type,\n",
    "                 critic_type):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._generator_type = generator_type\n",
    "        self._critic_type = critic_type\n",
    "\n",
    "    def build_model(self, lr):\n",
    "        optimizer = RMSprop(lr)\n",
    "        \n",
    "        self._generator = self._get_generator(\n",
    "            self._latent_dim, self._timesteps, self._generator_type)\n",
    "        self._generator.compile(\n",
    "            loss=self._wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "        self._critic = self._get_critic(self._timesteps,\n",
    "                                                      self._critic_type)\n",
    "        self._critic.compile(\n",
    "            loss=self._wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        real = Input(shape=[\n",
    "            self._timesteps,\n",
    "        ])\n",
    "\n",
    "        self._critic.trainable = False\n",
    "\n",
    "        valid = self._critic(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss=self._wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "#         self._gan.summary()\n",
    "#         self._generator.summary()\n",
    "#         self._critic.summary()\n",
    "        return self._gan, self._generator, self._critic\n",
    "\n",
    "    def _get_generator(self, noise_dim, timesteps, generator_type):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "\n",
    "        if generator_type == 'dense':\n",
    "            generated = Dense(timesteps, activation='tanh')(generator_inputs)\n",
    "            generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        elif generator_type == 'conv':\n",
    "            generated = Lambda(lambda x: K.expand_dims(x))(generator_inputs)\n",
    "            while generated.shape[1] < timesteps:\n",
    "                generated = Conv1D(\n",
    "                    32, 3, activation='tanh', padding='same')(generated)\n",
    "                generated = UpSampling1D(2)(generated)\n",
    "            generated = Conv1D(\n",
    "                1, 3, activation='tanh', padding='same')(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "            generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        elif generator_type == 'lstm':\n",
    "            generated = RepeatVector(timesteps)(generator_inputs)\n",
    "            generated = LSTM(32, return_sequences=True)(generated)\n",
    "            generated = TimeDistributed(Dense(1, activation='tanh'))(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "\n",
    "        elif generator_type == 'blstm':\n",
    "            generated = RepeatVector(timesteps)(generator_inputs)\n",
    "            generated = Bidirectional(LSTM(32,\n",
    "                                           return_sequences=True))(generated)\n",
    "            generated = TimeDistributed(Dense(1, activation='tanh'))(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _get_critic(self, timesteps, critic_type):\n",
    "        critic_inputs = Input((timesteps, ))\n",
    "\n",
    "        if critic_type == 'dense':\n",
    "            criticized = Dense(\n",
    "                timesteps, activation='tanh')(critic_inputs)\n",
    "            criticized = Dense(timesteps, activation='tanh')(criticized)\n",
    "            criticized = Dense(1)(criticized)\n",
    "\n",
    "        elif critic_type == 'conv':\n",
    "            criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "                critic_inputs)\n",
    "            while criticized.shape[1] > 1:\n",
    "                criticized = Conv1D(\n",
    "                    32, 3, activation='tanh', padding='same')(criticized)\n",
    "                criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "            criticized = Flatten()(criticized)\n",
    "            criticized = Dense(1)(criticized) \n",
    "\n",
    "        elif critic_type == 'lstm':\n",
    "            criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "                critic_inputs)\n",
    "            criticized = LSTM(32, return_sequences=False)(criticized)\n",
    "            criticized = Dense(1)(criticized)\n",
    "\n",
    "        elif critic_type == 'blstm':\n",
    "            criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "                critic_inputs)\n",
    "            criticized = Bidirectional(LSTM(\n",
    "                32, return_sequences=False))(criticized)\n",
    "            criticized = Dense(1)(criticized)\n",
    "\n",
    "        critic = Model(critic_inputs, criticized,\n",
    "                              'critic')\n",
    "        return critic\n",
    "\n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset,\n",
    "              img_frequency, clip_value):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        losses = [[], []]\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                critic_loss_real = self._critic.train_on_batch(\n",
    "                    batch_transactions, -np.ones((half_batch, 1)))\n",
    "                critic_loss_fake = self._critic.train_on_batch(\n",
    "                    generated_transactions, np.ones((half_batch, 1)))\n",
    "                critic_loss = 0.5 * np.add(critic_loss_real,\n",
    "                                                  critic_loss_fake)\n",
    "\n",
    "                self._clip_weights(clip_value)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, -np.ones((batch_size, 1)))[0]\n",
    "            \n",
    "            generator_loss = 1 - generator_loss\n",
    "            critic_loss = 1 - critic_loss\n",
    "            \n",
    "            losses[0].append(generator_loss)\n",
    "            losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, critic_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if epoch % img_frequency == 0:\n",
    "                self._save_imgs(epoch)\n",
    "                self._save_losses(losses)\n",
    "\n",
    "    def _save_imgs(self, epoch):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wgan/%05d.png' % epoch)\n",
    "        plt.savefig('wgan/last.png')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_losses(losses):\n",
    "        plt.plot(losses[0])\n",
    "        plt.plot(losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig('wgan/losses.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        weights = [np.clip(w, -clip_value, clip_value) for w in self._critic.get_weights()]\n",
    "        self._critic.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94500 100\n"
     ]
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../../datasets/berka_dataset/usable/normalized_transactions.npy\"\n",
    "\n",
    "timesteps = 100\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "\n",
    "transactions = split_data(transactions, timesteps)\n",
    "np.random.shuffle(transactions)\n",
    "\n",
    "N, D = transactions.shape\n",
    "print(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = int(1e5)\n",
    "n_critic = 5\n",
    "n_generator = 1\n",
    "latent_dim = 10\n",
    "lr = 0.00005\n",
    "clip_value = 0.025\n",
    "img_frequency = 100\n",
    "generator_type = 'conv'\n",
    "critic_type = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.999957] [G loss: 1.000095]\n",
      "1 [D loss: 0.999959] [G loss: 1.000096]\n",
      "2 [D loss: 0.999958] [G loss: 1.000095]\n",
      "3 [D loss: 0.999956] [G loss: 1.000093]\n",
      "4 [D loss: 0.999954] [G loss: 1.000092]\n",
      "5 [D loss: 0.999954] [G loss: 1.000091]\n",
      "6 [D loss: 0.999953] [G loss: 1.000091]\n",
      "7 [D loss: 0.999953] [G loss: 1.000090]\n",
      "8 [D loss: 0.999953] [G loss: 1.000090]\n",
      "9 [D loss: 0.999953] [G loss: 1.000090]\n",
      "10 [D loss: 0.999953] [G loss: 1.000089]\n",
      "11 [D loss: 0.999954] [G loss: 1.000089]\n",
      "12 [D loss: 0.999954] [G loss: 1.000089]\n",
      "13 [D loss: 0.999954] [G loss: 1.000089]\n",
      "14 [D loss: 0.999955] [G loss: 1.000089]\n",
      "15 [D loss: 0.999955] [G loss: 1.000088]\n",
      "16 [D loss: 0.999955] [G loss: 1.000087]\n",
      "17 [D loss: 0.999956] [G loss: 1.000087]\n",
      "18 [D loss: 0.999956] [G loss: 1.000087]\n",
      "19 [D loss: 0.999956] [G loss: 1.000086]\n",
      "20 [D loss: 0.999957] [G loss: 1.000086]\n",
      "21 [D loss: 0.999957] [G loss: 1.000085]\n",
      "22 [D loss: 0.999957] [G loss: 1.000085]\n",
      "23 [D loss: 0.999957] [G loss: 1.000085]\n",
      "24 [D loss: 0.999958] [G loss: 1.000085]\n",
      "25 [D loss: 0.999958] [G loss: 1.000084]\n",
      "26 [D loss: 0.999958] [G loss: 1.000083]\n",
      "27 [D loss: 0.999959] [G loss: 1.000083]\n",
      "28 [D loss: 0.999958] [G loss: 1.000083]\n",
      "29 [D loss: 0.999959] [G loss: 1.000082]\n",
      "30 [D loss: 0.999959] [G loss: 1.000082]\n",
      "31 [D loss: 0.999960] [G loss: 1.000081]\n",
      "32 [D loss: 0.999960] [G loss: 1.000081]\n",
      "33 [D loss: 0.999960] [G loss: 1.000081]\n",
      "34 [D loss: 0.999961] [G loss: 1.000080]\n",
      "35 [D loss: 0.999961] [G loss: 1.000080]\n",
      "36 [D loss: 0.999961] [G loss: 1.000079]\n",
      "37 [D loss: 0.999962] [G loss: 1.000079]\n",
      "38 [D loss: 0.999962] [G loss: 1.000078]\n",
      "39 [D loss: 0.999962] [G loss: 1.000077]\n",
      "40 [D loss: 0.999962] [G loss: 1.000077]\n",
      "41 [D loss: 0.999963] [G loss: 1.000076]\n",
      "42 [D loss: 0.999963] [G loss: 1.000076]\n",
      "43 [D loss: 0.999964] [G loss: 1.000076]\n",
      "44 [D loss: 0.999964] [G loss: 1.000075]\n",
      "45 [D loss: 0.999964] [G loss: 1.000074]\n",
      "46 [D loss: 0.999965] [G loss: 1.000073]\n",
      "47 [D loss: 0.999966] [G loss: 1.000073]\n",
      "48 [D loss: 0.999966] [G loss: 1.000072]\n",
      "49 [D loss: 0.999967] [G loss: 1.000071]\n",
      "50 [D loss: 0.999968] [G loss: 1.000070]\n",
      "51 [D loss: 0.999968] [G loss: 1.000069]\n",
      "52 [D loss: 0.999969] [G loss: 1.000068]\n",
      "53 [D loss: 0.999970] [G loss: 1.000066]\n",
      "54 [D loss: 0.999971] [G loss: 1.000064]\n",
      "55 [D loss: 0.999973] [G loss: 1.000062]\n",
      "56 [D loss: 0.999975] [G loss: 1.000060]\n",
      "57 [D loss: 0.999977] [G loss: 1.000057]\n",
      "58 [D loss: 0.999978] [G loss: 1.000054]\n",
      "59 [D loss: 0.999980] [G loss: 1.000049]\n",
      "60 [D loss: 0.999984] [G loss: 1.000044]\n",
      "61 [D loss: 0.999988] [G loss: 1.000040]\n",
      "62 [D loss: 0.999993] [G loss: 1.000034]\n",
      "63 [D loss: 1.000000] [G loss: 1.000024]\n",
      "64 [D loss: 1.000005] [G loss: 1.000019]\n",
      "65 [D loss: 1.000011] [G loss: 1.000004]\n",
      "66 [D loss: 1.000022] [G loss: 0.999994]\n",
      "67 [D loss: 1.000033] [G loss: 0.999981]\n",
      "68 [D loss: 1.000047] [G loss: 0.999964]\n",
      "69 [D loss: 1.000059] [G loss: 0.999944]\n",
      "70 [D loss: 1.000079] [G loss: 0.999924]\n",
      "71 [D loss: 1.000093] [G loss: 0.999896]\n",
      "72 [D loss: 1.000124] [G loss: 0.999871]\n",
      "73 [D loss: 1.000134] [G loss: 0.999845]\n",
      "74 [D loss: 1.000167] [G loss: 0.999813]\n",
      "75 [D loss: 1.000200] [G loss: 0.999772]\n",
      "76 [D loss: 1.000233] [G loss: 0.999731]\n",
      "77 [D loss: 1.000269] [G loss: 0.999685]\n",
      "78 [D loss: 1.000323] [G loss: 0.999628]\n",
      "79 [D loss: 1.000383] [G loss: 0.999558]\n",
      "80 [D loss: 1.000434] [G loss: 0.999487]\n",
      "81 [D loss: 1.000466] [G loss: 0.999412]\n",
      "82 [D loss: 1.000571] [G loss: 0.999335]\n",
      "83 [D loss: 1.000610] [G loss: 0.999234]\n",
      "84 [D loss: 1.000704] [G loss: 0.999162]\n",
      "85 [D loss: 1.000811] [G loss: 0.999057]\n",
      "86 [D loss: 1.000902] [G loss: 0.998957]\n",
      "87 [D loss: 1.001008] [G loss: 0.998831]\n",
      "88 [D loss: 1.001106] [G loss: 0.998728]\n",
      "89 [D loss: 1.001187] [G loss: 0.998596]\n",
      "90 [D loss: 1.001362] [G loss: 0.998456]\n",
      "91 [D loss: 1.001479] [G loss: 0.998346]\n",
      "92 [D loss: 1.001582] [G loss: 0.998208]\n",
      "93 [D loss: 1.001799] [G loss: 0.998073]\n",
      "94 [D loss: 1.001987] [G loss: 0.997819]\n",
      "95 [D loss: 1.002103] [G loss: 0.997683]\n",
      "96 [D loss: 1.002326] [G loss: 0.997478]\n",
      "97 [D loss: 1.002542] [G loss: 0.997346]\n",
      "98 [D loss: 1.002648] [G loss: 0.997089]\n",
      "99 [D loss: 1.002887] [G loss: 0.997089]\n",
      "100 [D loss: 1.002982] [G loss: 0.996838]\n",
      "101 [D loss: 1.003170] [G loss: 0.996607]\n",
      "102 [D loss: 1.003196] [G loss: 0.996580]\n",
      "103 [D loss: 1.003519] [G loss: 0.996297]\n",
      "104 [D loss: 1.003692] [G loss: 0.996243]\n",
      "105 [D loss: 1.003807] [G loss: 0.995882]\n",
      "106 [D loss: 1.004063] [G loss: 0.995894]\n",
      "107 [D loss: 1.004510] [G loss: 0.995961]\n",
      "108 [D loss: 1.004240] [G loss: 0.995293]\n",
      "109 [D loss: 1.004531] [G loss: 0.995976]\n",
      "110 [D loss: 1.004885] [G loss: 0.996181]\n",
      "111 [D loss: 1.004831] [G loss: 0.995910]\n",
      "112 [D loss: 1.005067] [G loss: 0.995857]\n",
      "113 [D loss: 1.005123] [G loss: 0.995525]\n",
      "114 [D loss: 1.005780] [G loss: 0.996143]\n",
      "115 [D loss: 1.004812] [G loss: 0.996195]\n",
      "116 [D loss: 1.005834] [G loss: 0.995988]\n",
      "117 [D loss: 1.005660] [G loss: 0.996166]\n",
      "118 [D loss: 1.005451] [G loss: 0.996829]\n",
      "119 [D loss: 1.005370] [G loss: 0.996533]\n",
      "120 [D loss: 1.004650] [G loss: 0.998021]\n",
      "121 [D loss: 1.006563] [G loss: 0.997068]\n",
      "122 [D loss: 1.004742] [G loss: 0.998567]\n",
      "123 [D loss: 1.005008] [G loss: 0.997897]\n",
      "124 [D loss: 1.005558] [G loss: 1.000035]\n",
      "125 [D loss: 1.004892] [G loss: 1.000766]\n",
      "126 [D loss: 1.003703] [G loss: 1.000269]\n",
      "127 [D loss: 1.004225] [G loss: 1.001652]\n",
      "128 [D loss: 1.003109] [G loss: 1.001712]\n",
      "129 [D loss: 1.004015] [G loss: 1.002642]\n",
      "130 [D loss: 1.001937] [G loss: 1.004216]\n",
      "131 [D loss: 1.002591] [G loss: 1.004756]\n",
      "132 [D loss: 1.001996] [G loss: 1.005323]\n",
      "133 [D loss: 1.000175] [G loss: 1.008353]\n",
      "134 [D loss: 1.000433] [G loss: 1.006914]\n",
      "135 [D loss: 0.999636] [G loss: 1.008114]\n",
      "136 [D loss: 0.999947] [G loss: 1.007500]\n",
      "137 [D loss: 1.000216] [G loss: 1.009780]\n",
      "138 [D loss: 0.999983] [G loss: 1.009894]\n",
      "139 [D loss: 0.999318] [G loss: 1.010535]\n",
      "140 [D loss: 0.997336] [G loss: 1.010527]\n",
      "141 [D loss: 0.997111] [G loss: 1.010555]\n",
      "142 [D loss: 0.997849] [G loss: 1.010933]\n",
      "143 [D loss: 0.997475] [G loss: 1.011113]\n",
      "144 [D loss: 0.998214] [G loss: 1.011586]\n",
      "145 [D loss: 0.997536] [G loss: 1.010864]\n",
      "146 [D loss: 0.997941] [G loss: 1.010928]\n",
      "147 [D loss: 0.997921] [G loss: 1.010843]\n",
      "148 [D loss: 0.996770] [G loss: 1.009502]\n",
      "149 [D loss: 0.996958] [G loss: 1.010216]\n",
      "150 [D loss: 0.996682] [G loss: 1.008925]\n",
      "151 [D loss: 0.997370] [G loss: 1.009474]\n",
      "152 [D loss: 0.996501] [G loss: 1.008813]\n",
      "153 [D loss: 0.998038] [G loss: 1.008220]\n",
      "154 [D loss: 0.996592] [G loss: 1.007709]\n",
      "155 [D loss: 0.997452] [G loss: 1.007406]\n",
      "156 [D loss: 0.997189] [G loss: 1.007014]\n",
      "157 [D loss: 0.997831] [G loss: 1.006923]\n",
      "158 [D loss: 0.997882] [G loss: 1.006193]\n",
      "159 [D loss: 0.997915] [G loss: 1.005716]\n",
      "160 [D loss: 0.997978] [G loss: 1.005389]\n",
      "161 [D loss: 0.998180] [G loss: 1.005239]\n",
      "162 [D loss: 0.998631] [G loss: 1.004793]\n",
      "163 [D loss: 0.998505] [G loss: 1.004380]\n",
      "164 [D loss: 0.998615] [G loss: 1.004097]\n",
      "165 [D loss: 0.998679] [G loss: 1.003925]\n",
      "166 [D loss: 0.998775] [G loss: 1.003546]\n",
      "167 [D loss: 0.998930] [G loss: 1.003391]\n",
      "168 [D loss: 0.998583] [G loss: 1.003149]\n",
      "169 [D loss: 0.999029] [G loss: 1.002865]\n",
      "170 [D loss: 0.999117] [G loss: 1.002650]\n",
      "171 [D loss: 0.999131] [G loss: 1.002507]\n",
      "172 [D loss: 0.999378] [G loss: 1.002278]\n",
      "173 [D loss: 0.999287] [G loss: 1.002124]\n",
      "174 [D loss: 0.999355] [G loss: 1.001965]\n",
      "175 [D loss: 0.999443] [G loss: 1.001897]\n",
      "176 [D loss: 0.999491] [G loss: 1.001740]\n",
      "177 [D loss: 0.999575] [G loss: 1.001649]\n",
      "178 [D loss: 0.999630] [G loss: 1.001559]\n",
      "179 [D loss: 0.999678] [G loss: 1.001479]\n",
      "180 [D loss: 0.999697] [G loss: 1.001397]\n",
      "181 [D loss: 0.999753] [G loss: 1.001314]\n",
      "182 [D loss: 0.999813] [G loss: 1.001273]\n",
      "183 [D loss: 0.999825] [G loss: 1.001216]\n",
      "184 [D loss: 0.999876] [G loss: 1.001155]\n",
      "185 [D loss: 0.999898] [G loss: 1.001119]\n",
      "186 [D loss: 0.999928] [G loss: 1.001069]\n",
      "187 [D loss: 0.999944] [G loss: 1.001015]\n",
      "188 [D loss: 0.999971] [G loss: 1.000957]\n",
      "189 [D loss: 0.999991] [G loss: 1.000898]\n",
      "190 [D loss: 1.000005] [G loss: 1.000830]\n",
      "191 [D loss: 1.000022] [G loss: 1.000770]\n",
      "192 [D loss: 1.000061] [G loss: 1.000699]\n",
      "193 [D loss: 1.000103] [G loss: 1.000616]\n",
      "194 [D loss: 1.000110] [G loss: 1.000523]\n",
      "195 [D loss: 1.000166] [G loss: 1.000447]\n",
      "196 [D loss: 1.000147] [G loss: 1.000358]\n",
      "197 [D loss: 1.000159] [G loss: 1.000276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 [D loss: 1.000184] [G loss: 1.000155]\n",
      "199 [D loss: 1.000289] [G loss: 1.000066]\n",
      "200 [D loss: 1.000276] [G loss: 0.999931]\n",
      "201 [D loss: 1.000266] [G loss: 0.999816]\n",
      "202 [D loss: 1.000309] [G loss: 0.999743]\n",
      "203 [D loss: 1.000367] [G loss: 0.999646]\n",
      "204 [D loss: 1.000340] [G loss: 0.999476]\n",
      "205 [D loss: 1.000388] [G loss: 0.999392]\n",
      "206 [D loss: 1.000422] [G loss: 0.999338]\n",
      "207 [D loss: 1.000437] [G loss: 0.999228]\n",
      "208 [D loss: 1.000401] [G loss: 0.999295]\n",
      "209 [D loss: 1.000336] [G loss: 0.999094]\n",
      "210 [D loss: 1.000432] [G loss: 0.999191]\n",
      "211 [D loss: 1.000276] [G loss: 0.999141]\n",
      "212 [D loss: 1.000353] [G loss: 0.999093]\n",
      "213 [D loss: 1.000285] [G loss: 0.999212]\n",
      "214 [D loss: 1.000172] [G loss: 0.999195]\n",
      "215 [D loss: 1.000284] [G loss: 0.999451]\n",
      "216 [D loss: 1.000132] [G loss: 0.999771]\n",
      "217 [D loss: 1.000183] [G loss: 0.999698]\n",
      "218 [D loss: 0.999736] [G loss: 0.999895]\n",
      "219 [D loss: 0.999860] [G loss: 0.999677]\n",
      "220 [D loss: 0.999661] [G loss: 1.000245]\n",
      "221 [D loss: 1.000118] [G loss: 1.000138]\n",
      "222 [D loss: 0.999938] [G loss: 1.000215]\n",
      "223 [D loss: 0.999794] [G loss: 1.000539]\n",
      "224 [D loss: 0.999825] [G loss: 1.000727]\n",
      "225 [D loss: 0.999947] [G loss: 1.000590]\n",
      "226 [D loss: 1.000067] [G loss: 1.000761]\n",
      "227 [D loss: 1.000042] [G loss: 1.000834]\n",
      "228 [D loss: 1.000018] [G loss: 1.000801]\n",
      "229 [D loss: 1.000113] [G loss: 1.000818]\n",
      "230 [D loss: 1.000172] [G loss: 1.000707]\n",
      "231 [D loss: 1.000258] [G loss: 1.000747]\n",
      "232 [D loss: 1.000281] [G loss: 1.000681]\n",
      "233 [D loss: 1.000176] [G loss: 1.000593]\n",
      "234 [D loss: 1.000250] [G loss: 1.000658]\n",
      "235 [D loss: 1.000174] [G loss: 1.000757]\n",
      "236 [D loss: 1.000125] [G loss: 1.000697]\n",
      "237 [D loss: 1.000185] [G loss: 1.000776]\n",
      "238 [D loss: 1.000134] [G loss: 1.000836]\n",
      "239 [D loss: 1.000099] [G loss: 1.000913]\n",
      "240 [D loss: 1.000098] [G loss: 1.001000]\n",
      "241 [D loss: 1.000033] [G loss: 1.001106]\n",
      "242 [D loss: 1.000029] [G loss: 1.001075]\n",
      "243 [D loss: 0.999988] [G loss: 1.000970]\n",
      "244 [D loss: 1.000033] [G loss: 1.001001]\n",
      "245 [D loss: 1.000027] [G loss: 1.001108]\n",
      "246 [D loss: 1.000012] [G loss: 1.001149]\n",
      "247 [D loss: 0.999993] [G loss: 1.001122]\n",
      "248 [D loss: 1.000015] [G loss: 1.001116]\n",
      "249 [D loss: 1.000018] [G loss: 1.001141]\n",
      "250 [D loss: 1.000024] [G loss: 1.001118]\n",
      "251 [D loss: 1.000040] [G loss: 1.001127]\n",
      "252 [D loss: 1.000002] [G loss: 1.001091]\n",
      "253 [D loss: 1.000004] [G loss: 1.001110]\n",
      "254 [D loss: 1.000061] [G loss: 1.001074]\n",
      "255 [D loss: 1.000043] [G loss: 1.001070]\n",
      "256 [D loss: 1.000095] [G loss: 1.001127]\n",
      "257 [D loss: 1.000058] [G loss: 1.001102]\n",
      "258 [D loss: 1.000046] [G loss: 1.001077]\n",
      "259 [D loss: 1.000070] [G loss: 1.001019]\n",
      "260 [D loss: 1.000085] [G loss: 1.001048]\n",
      "261 [D loss: 1.000113] [G loss: 1.001019]\n",
      "262 [D loss: 1.000052] [G loss: 1.000960]\n",
      "263 [D loss: 1.000052] [G loss: 1.001049]\n",
      "264 [D loss: 1.000056] [G loss: 1.001054]\n",
      "265 [D loss: 1.000110] [G loss: 1.001018]\n",
      "266 [D loss: 1.000036] [G loss: 1.001039]\n",
      "267 [D loss: 1.000102] [G loss: 1.001127]\n",
      "268 [D loss: 1.000030] [G loss: 1.001053]\n",
      "269 [D loss: 1.000085] [G loss: 1.000997]\n",
      "270 [D loss: 0.999965] [G loss: 1.001079]\n",
      "271 [D loss: 1.000076] [G loss: 1.001067]\n",
      "272 [D loss: 0.999970] [G loss: 1.001058]\n",
      "273 [D loss: 1.000028] [G loss: 1.001127]\n",
      "274 [D loss: 1.000059] [G loss: 1.001047]\n",
      "275 [D loss: 1.000061] [G loss: 1.001079]\n",
      "276 [D loss: 1.000042] [G loss: 1.001103]\n",
      "277 [D loss: 1.000005] [G loss: 1.001080]\n",
      "278 [D loss: 0.999925] [G loss: 1.001121]\n",
      "279 [D loss: 0.999956] [G loss: 1.001127]\n",
      "280 [D loss: 1.000037] [G loss: 1.001098]\n",
      "281 [D loss: 1.000013] [G loss: 1.001071]\n",
      "282 [D loss: 1.000074] [G loss: 1.001038]\n",
      "283 [D loss: 1.000017] [G loss: 1.001072]\n",
      "284 [D loss: 0.999992] [G loss: 1.001036]\n",
      "285 [D loss: 1.000067] [G loss: 1.001059]\n",
      "286 [D loss: 1.000056] [G loss: 1.001079]\n",
      "287 [D loss: 1.000094] [G loss: 1.001030]\n",
      "288 [D loss: 1.000045] [G loss: 1.001015]\n",
      "289 [D loss: 1.000058] [G loss: 1.001014]\n",
      "290 [D loss: 1.000011] [G loss: 1.001018]\n",
      "291 [D loss: 1.000047] [G loss: 1.001032]\n",
      "292 [D loss: 1.000095] [G loss: 1.001027]\n",
      "293 [D loss: 1.000114] [G loss: 1.000898]\n",
      "294 [D loss: 1.000088] [G loss: 1.000998]\n",
      "295 [D loss: 1.000060] [G loss: 1.000985]\n",
      "296 [D loss: 1.000111] [G loss: 1.001030]\n",
      "297 [D loss: 1.000090] [G loss: 1.001001]\n",
      "298 [D loss: 1.000093] [G loss: 1.001006]\n",
      "299 [D loss: 1.000074] [G loss: 1.001033]\n",
      "300 [D loss: 1.000055] [G loss: 1.001009]\n",
      "301 [D loss: 1.000031] [G loss: 1.001034]\n",
      "302 [D loss: 1.000057] [G loss: 1.000953]\n",
      "303 [D loss: 1.000055] [G loss: 1.001086]\n",
      "304 [D loss: 1.000029] [G loss: 1.000948]\n",
      "305 [D loss: 1.000082] [G loss: 1.001121]\n",
      "306 [D loss: 0.999999] [G loss: 1.001066]\n",
      "307 [D loss: 1.000015] [G loss: 1.001075]\n",
      "308 [D loss: 1.000023] [G loss: 1.001051]\n",
      "309 [D loss: 1.000059] [G loss: 1.001106]\n",
      "310 [D loss: 1.000130] [G loss: 1.001160]\n",
      "311 [D loss: 1.000080] [G loss: 1.001168]\n",
      "312 [D loss: 1.000015] [G loss: 1.001117]\n",
      "313 [D loss: 1.000066] [G loss: 1.001101]\n",
      "314 [D loss: 1.000090] [G loss: 1.001069]\n",
      "315 [D loss: 1.000044] [G loss: 1.001138]\n",
      "316 [D loss: 0.999943] [G loss: 1.001117]\n",
      "317 [D loss: 1.000008] [G loss: 1.001160]\n",
      "318 [D loss: 1.000005] [G loss: 1.001209]\n",
      "319 [D loss: 1.000046] [G loss: 1.001163]\n",
      "320 [D loss: 1.000017] [G loss: 1.001213]\n",
      "321 [D loss: 1.000045] [G loss: 1.001199]\n",
      "322 [D loss: 1.000040] [G loss: 1.001183]\n",
      "323 [D loss: 0.999990] [G loss: 1.001160]\n",
      "324 [D loss: 0.999993] [G loss: 1.001199]\n",
      "325 [D loss: 1.000026] [G loss: 1.001175]\n",
      "326 [D loss: 1.000019] [G loss: 1.001265]\n",
      "327 [D loss: 0.999973] [G loss: 1.001236]\n",
      "328 [D loss: 0.999946] [G loss: 1.001230]\n",
      "329 [D loss: 0.999977] [G loss: 1.001176]\n",
      "330 [D loss: 0.999965] [G loss: 1.001230]\n",
      "331 [D loss: 0.999981] [G loss: 1.001238]\n",
      "332 [D loss: 0.999943] [G loss: 1.001246]\n",
      "333 [D loss: 0.999951] [G loss: 1.001173]\n",
      "334 [D loss: 0.999942] [G loss: 1.001260]\n",
      "335 [D loss: 0.999901] [G loss: 1.001223]\n",
      "336 [D loss: 0.999964] [G loss: 1.001222]\n",
      "337 [D loss: 0.999976] [G loss: 1.001269]\n",
      "338 [D loss: 0.999971] [G loss: 1.001234]\n",
      "339 [D loss: 0.999982] [G loss: 1.001213]\n",
      "340 [D loss: 0.999988] [G loss: 1.001282]\n",
      "341 [D loss: 0.999925] [G loss: 1.001262]\n",
      "342 [D loss: 0.999891] [G loss: 1.001280]\n",
      "343 [D loss: 0.999918] [G loss: 1.001202]\n",
      "344 [D loss: 0.999907] [G loss: 1.001293]\n",
      "345 [D loss: 0.999910] [G loss: 1.001266]\n",
      "346 [D loss: 0.999928] [G loss: 1.001250]\n",
      "347 [D loss: 0.999978] [G loss: 1.001181]\n",
      "348 [D loss: 0.999964] [G loss: 1.001212]\n",
      "349 [D loss: 0.999984] [G loss: 1.001268]\n",
      "350 [D loss: 0.999904] [G loss: 1.001260]\n",
      "351 [D loss: 0.999938] [G loss: 1.001247]\n",
      "352 [D loss: 0.999937] [G loss: 1.001184]\n",
      "353 [D loss: 0.999944] [G loss: 1.001231]\n",
      "354 [D loss: 0.999928] [G loss: 1.001214]\n",
      "355 [D loss: 0.999932] [G loss: 1.001242]\n",
      "356 [D loss: 0.999910] [G loss: 1.001198]\n",
      "357 [D loss: 0.999927] [G loss: 1.001179]\n",
      "358 [D loss: 0.999954] [G loss: 1.001202]\n",
      "359 [D loss: 0.999950] [G loss: 1.001176]\n",
      "360 [D loss: 0.999961] [G loss: 1.001180]\n",
      "361 [D loss: 0.999951] [G loss: 1.001204]\n",
      "362 [D loss: 0.999950] [G loss: 1.001215]\n",
      "363 [D loss: 0.999953] [G loss: 1.001180]\n",
      "364 [D loss: 0.999932] [G loss: 1.001180]\n",
      "365 [D loss: 0.999945] [G loss: 1.001212]\n",
      "366 [D loss: 0.999976] [G loss: 1.001219]\n",
      "367 [D loss: 0.999905] [G loss: 1.001214]\n",
      "368 [D loss: 0.999977] [G loss: 1.001204]\n",
      "369 [D loss: 0.999958] [G loss: 1.001194]\n",
      "370 [D loss: 0.999970] [G loss: 1.001193]\n",
      "371 [D loss: 0.999970] [G loss: 1.001202]\n",
      "372 [D loss: 0.999968] [G loss: 1.001206]\n",
      "373 [D loss: 0.999954] [G loss: 1.001173]\n",
      "374 [D loss: 0.999945] [G loss: 1.001154]\n",
      "375 [D loss: 0.999933] [G loss: 1.001197]\n",
      "376 [D loss: 0.999960] [G loss: 1.001176]\n",
      "377 [D loss: 0.999995] [G loss: 1.001102]\n",
      "378 [D loss: 0.999928] [G loss: 1.001162]\n",
      "379 [D loss: 1.000006] [G loss: 1.001131]\n",
      "380 [D loss: 0.999972] [G loss: 1.001180]\n",
      "381 [D loss: 0.999963] [G loss: 1.001181]\n",
      "382 [D loss: 0.999947] [G loss: 1.001187]\n",
      "383 [D loss: 0.999974] [G loss: 1.001203]\n",
      "384 [D loss: 0.999928] [G loss: 1.001159]\n",
      "385 [D loss: 0.999961] [G loss: 1.001187]\n",
      "386 [D loss: 0.999956] [G loss: 1.001152]\n",
      "387 [D loss: 0.999974] [G loss: 1.001168]\n",
      "388 [D loss: 1.000009] [G loss: 1.001131]\n",
      "389 [D loss: 0.999981] [G loss: 1.001166]\n",
      "390 [D loss: 0.999978] [G loss: 1.001144]\n",
      "391 [D loss: 0.999993] [G loss: 1.001170]\n",
      "392 [D loss: 0.999966] [G loss: 1.001155]\n",
      "393 [D loss: 0.999984] [G loss: 1.001166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.999947] [G loss: 1.001141]\n",
      "395 [D loss: 0.999986] [G loss: 1.001176]\n",
      "396 [D loss: 0.999955] [G loss: 1.001168]\n",
      "397 [D loss: 0.999970] [G loss: 1.001171]\n",
      "398 [D loss: 0.999964] [G loss: 1.001100]\n",
      "399 [D loss: 0.999962] [G loss: 1.001163]\n",
      "400 [D loss: 0.999970] [G loss: 1.001156]\n",
      "401 [D loss: 0.999959] [G loss: 1.001159]\n",
      "402 [D loss: 0.999973] [G loss: 1.001136]\n",
      "403 [D loss: 0.999987] [G loss: 1.001154]\n",
      "404 [D loss: 1.000000] [G loss: 1.001130]\n",
      "405 [D loss: 1.000016] [G loss: 1.001134]\n",
      "406 [D loss: 0.999967] [G loss: 1.001147]\n",
      "407 [D loss: 0.999964] [G loss: 1.001162]\n",
      "408 [D loss: 0.999964] [G loss: 1.001161]\n",
      "409 [D loss: 1.000000] [G loss: 1.001174]\n",
      "410 [D loss: 0.999995] [G loss: 1.001146]\n",
      "411 [D loss: 0.999964] [G loss: 1.001141]\n",
      "412 [D loss: 1.000019] [G loss: 1.001168]\n",
      "413 [D loss: 1.000031] [G loss: 1.001160]\n",
      "414 [D loss: 1.000030] [G loss: 1.001174]\n",
      "415 [D loss: 1.000008] [G loss: 1.001162]\n",
      "416 [D loss: 0.999999] [G loss: 1.001148]\n",
      "417 [D loss: 0.999979] [G loss: 1.001192]\n",
      "418 [D loss: 1.000029] [G loss: 1.001162]\n",
      "419 [D loss: 0.999983] [G loss: 1.001200]\n",
      "420 [D loss: 0.999996] [G loss: 1.001184]\n",
      "421 [D loss: 0.999962] [G loss: 1.001185]\n",
      "422 [D loss: 1.000076] [G loss: 1.001207]\n",
      "423 [D loss: 1.000021] [G loss: 1.001177]\n",
      "424 [D loss: 0.999993] [G loss: 1.001201]\n",
      "425 [D loss: 0.999967] [G loss: 1.001167]\n",
      "426 [D loss: 0.999909] [G loss: 1.001209]\n",
      "427 [D loss: 1.000030] [G loss: 1.001207]\n",
      "428 [D loss: 1.000014] [G loss: 1.001197]\n",
      "429 [D loss: 1.000040] [G loss: 1.001176]\n",
      "430 [D loss: 0.999970] [G loss: 1.001220]\n",
      "431 [D loss: 0.999978] [G loss: 1.001168]\n",
      "432 [D loss: 1.000000] [G loss: 1.001240]\n",
      "433 [D loss: 0.999990] [G loss: 1.001250]\n",
      "434 [D loss: 1.000010] [G loss: 1.001230]\n",
      "435 [D loss: 0.999995] [G loss: 1.001125]\n",
      "436 [D loss: 1.000044] [G loss: 1.001173]\n",
      "437 [D loss: 0.999926] [G loss: 1.001236]\n",
      "438 [D loss: 1.000016] [G loss: 1.001178]\n",
      "439 [D loss: 0.999979] [G loss: 1.001210]\n",
      "440 [D loss: 1.000045] [G loss: 1.001232]\n",
      "441 [D loss: 1.000093] [G loss: 1.001209]\n",
      "442 [D loss: 0.999977] [G loss: 1.001231]\n",
      "443 [D loss: 1.000115] [G loss: 1.001226]\n",
      "444 [D loss: 1.000056] [G loss: 1.001259]\n",
      "445 [D loss: 1.000006] [G loss: 1.001270]\n",
      "446 [D loss: 1.000010] [G loss: 1.001304]\n",
      "447 [D loss: 1.000024] [G loss: 1.001299]\n",
      "448 [D loss: 1.000004] [G loss: 1.001310]\n",
      "449 [D loss: 1.000031] [G loss: 1.001271]\n",
      "450 [D loss: 1.000055] [G loss: 1.001311]\n",
      "451 [D loss: 1.000025] [G loss: 1.001269]\n",
      "452 [D loss: 0.999982] [G loss: 1.001320]\n",
      "453 [D loss: 0.999966] [G loss: 1.001315]\n",
      "454 [D loss: 0.999987] [G loss: 1.001332]\n",
      "455 [D loss: 0.999972] [G loss: 1.001361]\n",
      "456 [D loss: 0.999998] [G loss: 1.001410]\n",
      "457 [D loss: 1.000019] [G loss: 1.001331]\n",
      "458 [D loss: 1.000011] [G loss: 1.001342]\n",
      "459 [D loss: 0.999993] [G loss: 1.001333]\n",
      "460 [D loss: 1.000118] [G loss: 1.001427]\n",
      "461 [D loss: 1.000022] [G loss: 1.001373]\n",
      "462 [D loss: 1.000067] [G loss: 1.001389]\n",
      "463 [D loss: 0.999992] [G loss: 1.001410]\n",
      "464 [D loss: 0.999960] [G loss: 1.001432]\n",
      "465 [D loss: 1.000040] [G loss: 1.001457]\n",
      "466 [D loss: 0.999995] [G loss: 1.001433]\n",
      "467 [D loss: 1.000112] [G loss: 1.001436]\n",
      "468 [D loss: 1.000037] [G loss: 1.001477]\n",
      "469 [D loss: 1.000048] [G loss: 1.001411]\n",
      "470 [D loss: 1.000107] [G loss: 1.001403]\n",
      "471 [D loss: 1.000138] [G loss: 1.001350]\n",
      "472 [D loss: 1.000027] [G loss: 1.001385]\n",
      "473 [D loss: 1.000047] [G loss: 1.001411]\n",
      "474 [D loss: 1.000040] [G loss: 1.001259]\n",
      "475 [D loss: 1.000015] [G loss: 1.001404]\n",
      "476 [D loss: 0.999955] [G loss: 1.001376]\n",
      "477 [D loss: 1.000063] [G loss: 1.001585]\n",
      "478 [D loss: 1.000082] [G loss: 1.001502]\n",
      "479 [D loss: 1.000061] [G loss: 1.001497]\n",
      "480 [D loss: 1.000050] [G loss: 1.001456]\n",
      "481 [D loss: 1.000039] [G loss: 1.001336]\n",
      "482 [D loss: 1.000086] [G loss: 1.001468]\n",
      "483 [D loss: 1.000022] [G loss: 1.001476]\n",
      "484 [D loss: 1.000062] [G loss: 1.001413]\n",
      "485 [D loss: 1.000142] [G loss: 1.001449]\n",
      "486 [D loss: 1.000095] [G loss: 1.001443]\n",
      "487 [D loss: 1.000002] [G loss: 1.001401]\n",
      "488 [D loss: 1.000058] [G loss: 1.001418]\n",
      "489 [D loss: 1.000108] [G loss: 1.001481]\n",
      "490 [D loss: 1.000085] [G loss: 1.001415]\n",
      "491 [D loss: 1.000079] [G loss: 1.001476]\n",
      "492 [D loss: 1.000171] [G loss: 1.001542]\n",
      "493 [D loss: 0.999891] [G loss: 1.001729]\n",
      "494 [D loss: 1.000122] [G loss: 1.001652]\n",
      "495 [D loss: 1.000018] [G loss: 1.001632]\n",
      "496 [D loss: 1.000018] [G loss: 1.001787]\n",
      "497 [D loss: 1.000039] [G loss: 1.001719]\n",
      "498 [D loss: 1.000166] [G loss: 1.001738]\n",
      "499 [D loss: 1.000129] [G loss: 1.001820]\n",
      "500 [D loss: 1.000232] [G loss: 1.001852]\n",
      "501 [D loss: 1.000226] [G loss: 1.001905]\n",
      "502 [D loss: 1.000036] [G loss: 1.002034]\n",
      "503 [D loss: 0.999963] [G loss: 1.002014]\n",
      "504 [D loss: 0.999979] [G loss: 1.002008]\n",
      "505 [D loss: 0.999912] [G loss: 1.002151]\n",
      "506 [D loss: 1.000015] [G loss: 1.002065]\n",
      "507 [D loss: 0.999911] [G loss: 1.001850]\n",
      "508 [D loss: 0.999922] [G loss: 1.001895]\n",
      "509 [D loss: 0.999866] [G loss: 1.001916]\n",
      "510 [D loss: 0.999890] [G loss: 1.001977]\n",
      "511 [D loss: 1.000121] [G loss: 1.001913]\n",
      "512 [D loss: 0.999980] [G loss: 1.001857]\n",
      "513 [D loss: 0.999948] [G loss: 1.001918]\n",
      "514 [D loss: 0.999948] [G loss: 1.001858]\n",
      "515 [D loss: 1.000028] [G loss: 1.001882]\n",
      "516 [D loss: 1.000169] [G loss: 1.001695]\n",
      "517 [D loss: 1.000031] [G loss: 1.001728]\n",
      "518 [D loss: 1.000242] [G loss: 1.001765]\n",
      "519 [D loss: 1.000025] [G loss: 1.001654]\n",
      "520 [D loss: 1.000337] [G loss: 1.001725]\n",
      "521 [D loss: 1.000056] [G loss: 1.001780]\n",
      "522 [D loss: 0.999982] [G loss: 1.001767]\n",
      "523 [D loss: 1.000024] [G loss: 1.001860]\n",
      "524 [D loss: 1.000066] [G loss: 1.001879]\n",
      "525 [D loss: 0.999987] [G loss: 1.002054]\n",
      "526 [D loss: 0.999851] [G loss: 1.002215]\n",
      "527 [D loss: 0.999907] [G loss: 1.002196]\n",
      "528 [D loss: 1.000077] [G loss: 1.002091]\n",
      "529 [D loss: 1.000049] [G loss: 1.002154]\n",
      "530 [D loss: 1.000086] [G loss: 1.002078]\n",
      "531 [D loss: 1.000155] [G loss: 1.002201]\n",
      "532 [D loss: 1.000028] [G loss: 1.002256]\n",
      "533 [D loss: 1.000120] [G loss: 1.002206]\n",
      "534 [D loss: 0.999997] [G loss: 1.002226]\n",
      "535 [D loss: 1.000359] [G loss: 1.002134]\n",
      "536 [D loss: 1.000007] [G loss: 1.002652]\n",
      "537 [D loss: 0.999979] [G loss: 1.002390]\n",
      "538 [D loss: 1.000038] [G loss: 1.002553]\n",
      "539 [D loss: 0.999897] [G loss: 1.002610]\n",
      "540 [D loss: 0.999844] [G loss: 1.002574]\n",
      "541 [D loss: 0.999927] [G loss: 1.002580]\n",
      "542 [D loss: 0.999818] [G loss: 1.002569]\n",
      "543 [D loss: 0.999813] [G loss: 1.002603]\n",
      "544 [D loss: 0.999809] [G loss: 1.002723]\n",
      "545 [D loss: 0.999869] [G loss: 1.002729]\n",
      "546 [D loss: 0.999913] [G loss: 1.002504]\n",
      "547 [D loss: 0.999813] [G loss: 1.002554]\n",
      "548 [D loss: 0.999887] [G loss: 1.002525]\n",
      "549 [D loss: 0.999936] [G loss: 1.002409]\n",
      "550 [D loss: 0.999910] [G loss: 1.002338]\n",
      "551 [D loss: 0.999950] [G loss: 1.002361]\n",
      "552 [D loss: 0.999965] [G loss: 1.002199]\n",
      "553 [D loss: 1.000022] [G loss: 1.002169]\n",
      "554 [D loss: 1.000000] [G loss: 1.002070]\n",
      "555 [D loss: 1.000119] [G loss: 1.002020]\n",
      "556 [D loss: 1.000162] [G loss: 1.002064]\n",
      "557 [D loss: 1.000400] [G loss: 1.002071]\n",
      "558 [D loss: 1.000235] [G loss: 1.002181]\n",
      "559 [D loss: 1.000265] [G loss: 1.002140]\n",
      "560 [D loss: 1.000014] [G loss: 1.002319]\n",
      "561 [D loss: 0.999909] [G loss: 1.002416]\n",
      "562 [D loss: 0.999971] [G loss: 1.002407]\n",
      "563 [D loss: 0.999992] [G loss: 1.002546]\n",
      "564 [D loss: 1.000151] [G loss: 1.002619]\n",
      "565 [D loss: 1.000085] [G loss: 1.002715]\n",
      "566 [D loss: 1.000018] [G loss: 1.002628]\n",
      "567 [D loss: 1.000190] [G loss: 1.002711]\n",
      "568 [D loss: 1.000152] [G loss: 1.002722]\n",
      "569 [D loss: 1.000184] [G loss: 1.002652]\n",
      "570 [D loss: 1.000123] [G loss: 1.002846]\n",
      "571 [D loss: 1.000159] [G loss: 1.002691]\n",
      "572 [D loss: 0.999784] [G loss: 1.002911]\n",
      "573 [D loss: 1.000144] [G loss: 1.002875]\n",
      "574 [D loss: 1.000012] [G loss: 1.002998]\n",
      "575 [D loss: 1.000012] [G loss: 1.002984]\n",
      "576 [D loss: 1.000069] [G loss: 1.002847]\n",
      "577 [D loss: 0.999973] [G loss: 1.002696]\n",
      "578 [D loss: 0.999917] [G loss: 1.002531]\n",
      "579 [D loss: 0.999915] [G loss: 1.002429]\n",
      "580 [D loss: 0.999923] [G loss: 1.002560]\n",
      "581 [D loss: 0.999870] [G loss: 1.002770]\n",
      "582 [D loss: 0.999880] [G loss: 1.002731]\n",
      "583 [D loss: 0.999948] [G loss: 1.002868]\n",
      "584 [D loss: 1.000064] [G loss: 1.002325]\n",
      "585 [D loss: 0.999916] [G loss: 1.002721]\n",
      "586 [D loss: 0.999991] [G loss: 1.002602]\n",
      "587 [D loss: 1.000059] [G loss: 1.002510]\n",
      "588 [D loss: 1.000055] [G loss: 1.002329]\n",
      "589 [D loss: 1.000096] [G loss: 1.002469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 1.000019] [G loss: 1.002394]\n",
      "591 [D loss: 1.000147] [G loss: 1.002391]\n",
      "592 [D loss: 1.000156] [G loss: 1.002458]\n",
      "593 [D loss: 1.000272] [G loss: 1.002486]\n",
      "594 [D loss: 1.000082] [G loss: 1.002489]\n",
      "595 [D loss: 1.000017] [G loss: 1.002602]\n",
      "596 [D loss: 1.000083] [G loss: 1.002596]\n",
      "597 [D loss: 0.999998] [G loss: 1.002798]\n",
      "598 [D loss: 0.999947] [G loss: 1.002754]\n",
      "599 [D loss: 0.999966] [G loss: 1.002803]\n",
      "600 [D loss: 1.000121] [G loss: 1.002881]\n",
      "601 [D loss: 1.000277] [G loss: 1.002811]\n",
      "602 [D loss: 1.000142] [G loss: 1.002766]\n",
      "603 [D loss: 1.000174] [G loss: 1.002840]\n",
      "604 [D loss: 1.000154] [G loss: 1.002889]\n",
      "605 [D loss: 1.000215] [G loss: 1.002937]\n",
      "606 [D loss: 1.000197] [G loss: 1.003041]\n",
      "607 [D loss: 0.999961] [G loss: 1.003056]\n",
      "608 [D loss: 1.000074] [G loss: 1.003120]\n",
      "609 [D loss: 1.000004] [G loss: 1.003266]\n",
      "610 [D loss: 0.999924] [G loss: 1.003082]\n",
      "611 [D loss: 0.999922] [G loss: 1.003213]\n",
      "612 [D loss: 0.999846] [G loss: 1.003242]\n",
      "613 [D loss: 0.999825] [G loss: 1.003286]\n",
      "614 [D loss: 0.999870] [G loss: 1.003125]\n",
      "615 [D loss: 0.999907] [G loss: 1.003082]\n",
      "616 [D loss: 0.999874] [G loss: 1.003124]\n",
      "617 [D loss: 0.999985] [G loss: 1.002724]\n",
      "618 [D loss: 0.999903] [G loss: 1.003048]\n",
      "619 [D loss: 1.000058] [G loss: 1.002890]\n",
      "620 [D loss: 1.000014] [G loss: 1.002828]\n",
      "621 [D loss: 1.000162] [G loss: 1.002817]\n",
      "622 [D loss: 1.000010] [G loss: 1.002781]\n",
      "623 [D loss: 1.000115] [G loss: 1.002712]\n",
      "624 [D loss: 1.000095] [G loss: 1.002679]\n",
      "625 [D loss: 1.000070] [G loss: 1.002782]\n",
      "626 [D loss: 1.000059] [G loss: 1.002806]\n",
      "627 [D loss: 1.000224] [G loss: 1.002741]\n",
      "628 [D loss: 1.000039] [G loss: 1.002795]\n",
      "629 [D loss: 1.000184] [G loss: 1.002828]\n",
      "630 [D loss: 1.000125] [G loss: 1.002994]\n",
      "631 [D loss: 1.000313] [G loss: 1.002969]\n",
      "632 [D loss: 1.000165] [G loss: 1.003163]\n",
      "633 [D loss: 1.000063] [G loss: 1.003226]\n",
      "634 [D loss: 1.000075] [G loss: 1.003204]\n",
      "635 [D loss: 1.000045] [G loss: 1.003398]\n",
      "636 [D loss: 1.000320] [G loss: 1.003356]\n",
      "637 [D loss: 1.000199] [G loss: 1.003390]\n",
      "638 [D loss: 1.000194] [G loss: 1.003487]\n",
      "639 [D loss: 1.000214] [G loss: 1.003649]\n",
      "640 [D loss: 1.000361] [G loss: 1.004008]\n",
      "641 [D loss: 0.999954] [G loss: 1.003867]\n",
      "642 [D loss: 0.999979] [G loss: 1.003840]\n",
      "643 [D loss: 1.000002] [G loss: 1.003676]\n",
      "644 [D loss: 0.999986] [G loss: 1.003704]\n",
      "645 [D loss: 1.000006] [G loss: 1.003567]\n",
      "646 [D loss: 0.999975] [G loss: 1.003354]\n",
      "647 [D loss: 1.000033] [G loss: 1.003257]\n",
      "648 [D loss: 1.000002] [G loss: 1.003193]\n",
      "649 [D loss: 1.000069] [G loss: 1.003180]\n",
      "650 [D loss: 0.999990] [G loss: 1.003177]\n",
      "651 [D loss: 1.000039] [G loss: 1.003223]\n",
      "652 [D loss: 0.999985] [G loss: 1.003422]\n",
      "653 [D loss: 1.000006] [G loss: 1.003203]\n",
      "654 [D loss: 1.000099] [G loss: 1.003243]\n",
      "655 [D loss: 1.000145] [G loss: 1.003206]\n",
      "656 [D loss: 1.000299] [G loss: 1.003157]\n",
      "657 [D loss: 1.000210] [G loss: 1.003104]\n",
      "658 [D loss: 1.000121] [G loss: 1.003064]\n",
      "659 [D loss: 1.000292] [G loss: 1.003276]\n",
      "660 [D loss: 1.000272] [G loss: 1.003151]\n",
      "661 [D loss: 1.000160] [G loss: 1.003358]\n",
      "662 [D loss: 1.000033] [G loss: 1.003623]\n",
      "663 [D loss: 0.999795] [G loss: 1.003792]\n",
      "664 [D loss: 0.999658] [G loss: 1.003934]\n",
      "665 [D loss: 0.999862] [G loss: 1.003772]\n",
      "666 [D loss: 1.000083] [G loss: 1.003658]\n",
      "667 [D loss: 1.000292] [G loss: 1.003687]\n",
      "668 [D loss: 1.000323] [G loss: 1.003821]\n",
      "669 [D loss: 1.000229] [G loss: 1.003872]\n",
      "670 [D loss: 1.000328] [G loss: 1.004074]\n",
      "671 [D loss: 1.000109] [G loss: 1.003925]\n",
      "672 [D loss: 1.000454] [G loss: 1.003890]\n",
      "673 [D loss: 1.000336] [G loss: 1.004114]\n",
      "674 [D loss: 1.000048] [G loss: 1.003989]\n",
      "675 [D loss: 0.999936] [G loss: 1.004347]\n",
      "676 [D loss: 0.999966] [G loss: 1.004395]\n",
      "677 [D loss: 0.999899] [G loss: 1.004465]\n",
      "678 [D loss: 0.999938] [G loss: 1.004455]\n",
      "679 [D loss: 0.999932] [G loss: 1.004318]\n",
      "680 [D loss: 0.999859] [G loss: 1.004394]\n",
      "681 [D loss: 0.999860] [G loss: 1.004333]\n",
      "682 [D loss: 0.999786] [G loss: 1.004377]\n",
      "683 [D loss: 0.999826] [G loss: 1.004379]\n",
      "684 [D loss: 0.999805] [G loss: 1.004402]\n",
      "685 [D loss: 0.999759] [G loss: 1.004350]\n",
      "686 [D loss: 0.999967] [G loss: 1.004393]\n",
      "687 [D loss: 0.999888] [G loss: 1.004326]\n",
      "688 [D loss: 0.999821] [G loss: 1.004237]\n",
      "689 [D loss: 0.999805] [G loss: 1.004170]\n",
      "690 [D loss: 0.999874] [G loss: 1.003988]\n",
      "691 [D loss: 0.999918] [G loss: 1.003866]\n",
      "692 [D loss: 0.999920] [G loss: 1.003964]\n",
      "693 [D loss: 0.999974] [G loss: 1.003789]\n",
      "694 [D loss: 1.000130] [G loss: 1.003714]\n",
      "695 [D loss: 1.000067] [G loss: 1.003617]\n",
      "696 [D loss: 1.000052] [G loss: 1.003563]\n",
      "697 [D loss: 1.000249] [G loss: 1.003547]\n",
      "698 [D loss: 1.000191] [G loss: 1.003683]\n",
      "699 [D loss: 1.000266] [G loss: 1.003639]\n",
      "700 [D loss: 1.000042] [G loss: 1.003641]\n",
      "701 [D loss: 0.999959] [G loss: 1.003817]\n",
      "702 [D loss: 0.999996] [G loss: 1.003963]\n",
      "703 [D loss: 1.000043] [G loss: 1.004208]\n",
      "704 [D loss: 0.999832] [G loss: 1.004092]\n",
      "705 [D loss: 0.999989] [G loss: 1.004324]\n",
      "706 [D loss: 1.000055] [G loss: 1.004295]\n",
      "707 [D loss: 1.000135] [G loss: 1.004335]\n",
      "708 [D loss: 1.000116] [G loss: 1.004407]\n",
      "709 [D loss: 1.000085] [G loss: 1.004103]\n",
      "710 [D loss: 1.000171] [G loss: 1.004499]\n",
      "711 [D loss: 1.000106] [G loss: 1.004486]\n",
      "712 [D loss: 1.000151] [G loss: 1.004561]\n",
      "713 [D loss: 1.000112] [G loss: 1.004418]\n",
      "714 [D loss: 1.000134] [G loss: 1.004350]\n",
      "715 [D loss: 0.999938] [G loss: 1.004513]\n",
      "716 [D loss: 1.000091] [G loss: 1.004250]\n",
      "717 [D loss: 1.000053] [G loss: 1.004297]\n",
      "718 [D loss: 1.000031] [G loss: 1.004280]\n",
      "719 [D loss: 1.000132] [G loss: 1.003844]\n",
      "720 [D loss: 0.999936] [G loss: 1.003909]\n",
      "721 [D loss: 1.000018] [G loss: 1.003960]\n",
      "722 [D loss: 1.000047] [G loss: 1.004171]\n",
      "723 [D loss: 1.000140] [G loss: 1.003949]\n",
      "724 [D loss: 1.000043] [G loss: 1.003850]\n",
      "725 [D loss: 1.000167] [G loss: 1.003933]\n",
      "726 [D loss: 1.000059] [G loss: 1.003898]\n",
      "727 [D loss: 1.000518] [G loss: 1.003821]\n",
      "728 [D loss: 1.000266] [G loss: 1.003813]\n",
      "729 [D loss: 1.000174] [G loss: 1.003834]\n",
      "730 [D loss: 1.000277] [G loss: 1.003887]\n",
      "731 [D loss: 1.000186] [G loss: 1.003909]\n",
      "732 [D loss: 1.000006] [G loss: 1.004042]\n",
      "733 [D loss: 1.000269] [G loss: 1.004074]\n",
      "734 [D loss: 1.000047] [G loss: 1.003977]\n",
      "735 [D loss: 1.000114] [G loss: 1.004170]\n",
      "736 [D loss: 1.000139] [G loss: 1.004094]\n",
      "737 [D loss: 1.000169] [G loss: 1.004287]\n",
      "738 [D loss: 1.000244] [G loss: 1.004088]\n",
      "739 [D loss: 1.000149] [G loss: 1.004328]\n",
      "740 [D loss: 1.000192] [G loss: 1.004265]\n",
      "741 [D loss: 1.000288] [G loss: 1.004311]\n",
      "742 [D loss: 1.000163] [G loss: 1.004399]\n",
      "743 [D loss: 1.000286] [G loss: 1.004452]\n",
      "744 [D loss: 1.000233] [G loss: 1.004598]\n",
      "745 [D loss: 1.000275] [G loss: 1.004479]\n",
      "746 [D loss: 1.000260] [G loss: 1.004734]\n",
      "747 [D loss: 0.999991] [G loss: 1.004623]\n",
      "748 [D loss: 1.000029] [G loss: 1.004612]\n",
      "749 [D loss: 1.000244] [G loss: 1.004616]\n",
      "750 [D loss: 1.000002] [G loss: 1.004665]\n",
      "751 [D loss: 1.000022] [G loss: 1.004535]\n",
      "752 [D loss: 1.000254] [G loss: 1.004290]\n",
      "753 [D loss: 1.000504] [G loss: 1.004280]\n",
      "754 [D loss: 1.000532] [G loss: 1.004247]\n",
      "755 [D loss: 1.000523] [G loss: 1.004082]\n",
      "756 [D loss: 1.000246] [G loss: 1.004245]\n",
      "757 [D loss: 1.000544] [G loss: 1.004389]\n",
      "758 [D loss: 1.000152] [G loss: 1.004655]\n",
      "759 [D loss: 1.000173] [G loss: 1.004761]\n",
      "760 [D loss: 0.999782] [G loss: 1.005039]\n",
      "761 [D loss: 0.999892] [G loss: 1.005145]\n",
      "762 [D loss: 0.999911] [G loss: 1.004947]\n",
      "763 [D loss: 1.000038] [G loss: 1.005238]\n",
      "764 [D loss: 1.000095] [G loss: 1.005466]\n",
      "765 [D loss: 1.000138] [G loss: 1.005465]\n",
      "766 [D loss: 1.000044] [G loss: 1.004971]\n",
      "767 [D loss: 1.000313] [G loss: 1.005413]\n",
      "768 [D loss: 1.000212] [G loss: 1.005698]\n",
      "769 [D loss: 1.000224] [G loss: 1.006069]\n",
      "770 [D loss: 1.000016] [G loss: 1.006025]\n",
      "771 [D loss: 1.000161] [G loss: 1.006346]\n",
      "772 [D loss: 0.999639] [G loss: 1.006164]\n",
      "773 [D loss: 0.999727] [G loss: 1.006619]\n",
      "774 [D loss: 0.999839] [G loss: 1.006002]\n",
      "775 [D loss: 0.999746] [G loss: 1.006033]\n",
      "776 [D loss: 0.999760] [G loss: 1.005890]\n",
      "777 [D loss: 0.999900] [G loss: 1.005401]\n",
      "778 [D loss: 0.999976] [G loss: 1.005083]\n",
      "779 [D loss: 1.000077] [G loss: 1.004629]\n",
      "780 [D loss: 1.000066] [G loss: 1.004458]\n",
      "781 [D loss: 1.000080] [G loss: 1.004176]\n",
      "782 [D loss: 1.000053] [G loss: 1.004097]\n",
      "783 [D loss: 1.000050] [G loss: 1.003997]\n",
      "784 [D loss: 1.000074] [G loss: 1.003832]\n",
      "785 [D loss: 1.000004] [G loss: 1.003836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 [D loss: 1.000122] [G loss: 1.003956]\n",
      "787 [D loss: 0.999787] [G loss: 1.004351]\n",
      "788 [D loss: 1.000015] [G loss: 1.004500]\n",
      "789 [D loss: 1.000011] [G loss: 1.004556]\n",
      "790 [D loss: 1.000124] [G loss: 1.004416]\n",
      "791 [D loss: 1.000257] [G loss: 1.004444]\n",
      "792 [D loss: 1.000145] [G loss: 1.004421]\n",
      "793 [D loss: 1.000264] [G loss: 1.004465]\n",
      "794 [D loss: 1.000679] [G loss: 1.004506]\n",
      "795 [D loss: 1.000100] [G loss: 1.004550]\n",
      "796 [D loss: 1.000285] [G loss: 1.004785]\n",
      "797 [D loss: 0.999783] [G loss: 1.004979]\n",
      "798 [D loss: 0.999963] [G loss: 1.005312]\n",
      "799 [D loss: 1.000070] [G loss: 1.004854]\n",
      "800 [D loss: 1.000015] [G loss: 1.005146]\n",
      "801 [D loss: 1.000161] [G loss: 1.005210]\n",
      "802 [D loss: 1.000117] [G loss: 1.005088]\n",
      "803 [D loss: 1.000163] [G loss: 1.005371]\n",
      "804 [D loss: 1.000096] [G loss: 1.005467]\n",
      "805 [D loss: 1.000361] [G loss: 1.005077]\n",
      "806 [D loss: 0.999968] [G loss: 1.005279]\n",
      "807 [D loss: 1.000051] [G loss: 1.005656]\n",
      "808 [D loss: 1.000053] [G loss: 1.005329]\n",
      "809 [D loss: 1.000131] [G loss: 1.005610]\n",
      "810 [D loss: 0.999965] [G loss: 1.005491]\n",
      "811 [D loss: 1.000037] [G loss: 1.005436]\n",
      "812 [D loss: 0.999948] [G loss: 1.005214]\n",
      "813 [D loss: 0.999917] [G loss: 1.004956]\n",
      "814 [D loss: 0.999895] [G loss: 1.005019]\n",
      "815 [D loss: 1.000037] [G loss: 1.004928]\n",
      "816 [D loss: 0.999850] [G loss: 1.005014]\n",
      "817 [D loss: 0.999884] [G loss: 1.004968]\n",
      "818 [D loss: 0.999837] [G loss: 1.005171]\n",
      "819 [D loss: 0.999983] [G loss: 1.005156]\n",
      "820 [D loss: 0.999860] [G loss: 1.005054]\n",
      "821 [D loss: 0.999923] [G loss: 1.005021]\n",
      "822 [D loss: 0.999953] [G loss: 1.004786]\n",
      "823 [D loss: 1.000071] [G loss: 1.004660]\n",
      "824 [D loss: 1.000058] [G loss: 1.004447]\n",
      "825 [D loss: 1.000224] [G loss: 1.004569]\n",
      "826 [D loss: 1.000276] [G loss: 1.004378]\n",
      "827 [D loss: 1.000124] [G loss: 1.004442]\n",
      "828 [D loss: 1.000198] [G loss: 1.004589]\n",
      "829 [D loss: 1.000153] [G loss: 1.004618]\n",
      "830 [D loss: 1.000011] [G loss: 1.004697]\n",
      "831 [D loss: 1.000226] [G loss: 1.005014]\n",
      "832 [D loss: 1.000012] [G loss: 1.005221]\n",
      "833 [D loss: 0.999781] [G loss: 1.004962]\n",
      "834 [D loss: 1.000037] [G loss: 1.004920]\n",
      "835 [D loss: 1.000156] [G loss: 1.005185]\n",
      "836 [D loss: 1.000347] [G loss: 1.005089]\n",
      "837 [D loss: 1.000433] [G loss: 1.005076]\n",
      "838 [D loss: 1.000362] [G loss: 1.005227]\n",
      "839 [D loss: 1.000811] [G loss: 1.005039]\n",
      "840 [D loss: 1.000486] [G loss: 1.005505]\n",
      "841 [D loss: 1.000303] [G loss: 1.005314]\n",
      "842 [D loss: 1.000238] [G loss: 1.006011]\n",
      "843 [D loss: 0.999763] [G loss: 1.006043]\n",
      "844 [D loss: 1.000248] [G loss: 1.005941]\n",
      "845 [D loss: 0.999944] [G loss: 1.005782]\n",
      "846 [D loss: 0.999846] [G loss: 1.005992]\n",
      "847 [D loss: 0.999870] [G loss: 1.006008]\n",
      "848 [D loss: 0.999846] [G loss: 1.005780]\n",
      "849 [D loss: 0.999895] [G loss: 1.005520]\n",
      "850 [D loss: 0.999917] [G loss: 1.005262]\n",
      "851 [D loss: 0.999817] [G loss: 1.005484]\n",
      "852 [D loss: 0.999887] [G loss: 1.005497]\n",
      "853 [D loss: 0.999826] [G loss: 1.005700]\n",
      "854 [D loss: 1.000088] [G loss: 1.004999]\n",
      "855 [D loss: 1.000004] [G loss: 1.005249]\n",
      "856 [D loss: 1.000020] [G loss: 1.005234]\n",
      "857 [D loss: 0.999921] [G loss: 1.005022]\n",
      "858 [D loss: 1.000232] [G loss: 1.004952]\n",
      "859 [D loss: 1.000174] [G loss: 1.004895]\n",
      "860 [D loss: 1.000559] [G loss: 1.004713]\n",
      "861 [D loss: 1.000588] [G loss: 1.004785]\n",
      "862 [D loss: 1.000498] [G loss: 1.004746]\n",
      "863 [D loss: 1.000339] [G loss: 1.004963]\n",
      "864 [D loss: 1.000283] [G loss: 1.004957]\n",
      "865 [D loss: 1.000339] [G loss: 1.005090]\n",
      "866 [D loss: 0.999888] [G loss: 1.005362]\n",
      "867 [D loss: 0.999888] [G loss: 1.005415]\n",
      "868 [D loss: 0.999883] [G loss: 1.005579]\n",
      "869 [D loss: 1.000107] [G loss: 1.005535]\n",
      "870 [D loss: 1.000139] [G loss: 1.005642]\n",
      "871 [D loss: 1.000014] [G loss: 1.005737]\n",
      "872 [D loss: 1.000571] [G loss: 1.005539]\n",
      "873 [D loss: 1.000538] [G loss: 1.006141]\n",
      "874 [D loss: 1.000048] [G loss: 1.005950]\n",
      "875 [D loss: 1.000351] [G loss: 1.006000]\n",
      "876 [D loss: 1.000375] [G loss: 1.006050]\n",
      "877 [D loss: 0.999740] [G loss: 1.006307]\n",
      "878 [D loss: 0.999884] [G loss: 1.006182]\n",
      "879 [D loss: 1.000056] [G loss: 1.006178]\n",
      "880 [D loss: 1.000033] [G loss: 1.006070]\n",
      "881 [D loss: 1.000091] [G loss: 1.005774]\n",
      "882 [D loss: 1.000076] [G loss: 1.005499]\n",
      "883 [D loss: 1.000169] [G loss: 1.005470]\n",
      "884 [D loss: 1.000107] [G loss: 1.005199]\n",
      "885 [D loss: 1.000073] [G loss: 1.005143]\n",
      "886 [D loss: 1.000025] [G loss: 1.005317]\n",
      "887 [D loss: 1.000018] [G loss: 1.005566]\n",
      "888 [D loss: 0.999861] [G loss: 1.005608]\n",
      "889 [D loss: 1.000042] [G loss: 1.005284]\n",
      "890 [D loss: 1.000104] [G loss: 1.005447]\n",
      "891 [D loss: 1.000185] [G loss: 1.005401]\n",
      "892 [D loss: 1.000025] [G loss: 1.005315]\n",
      "893 [D loss: 1.000146] [G loss: 1.005137]\n",
      "894 [D loss: 1.000327] [G loss: 1.005334]\n",
      "895 [D loss: 1.000395] [G loss: 1.005306]\n",
      "896 [D loss: 1.000112] [G loss: 1.005413]\n",
      "897 [D loss: 1.000276] [G loss: 1.005362]\n",
      "898 [D loss: 1.000185] [G loss: 1.005426]\n",
      "899 [D loss: 1.000378] [G loss: 1.005415]\n",
      "900 [D loss: 1.000399] [G loss: 1.005556]\n",
      "901 [D loss: 1.000456] [G loss: 1.005778]\n",
      "902 [D loss: 1.000208] [G loss: 1.005914]\n",
      "903 [D loss: 1.000091] [G loss: 1.006068]\n",
      "904 [D loss: 1.000390] [G loss: 1.006135]\n",
      "905 [D loss: 1.000350] [G loss: 1.006305]\n",
      "906 [D loss: 1.000398] [G loss: 1.006354]\n",
      "907 [D loss: 1.000430] [G loss: 1.006529]\n",
      "908 [D loss: 1.000268] [G loss: 1.006786]\n",
      "909 [D loss: 1.000294] [G loss: 1.007107]\n",
      "910 [D loss: 1.000094] [G loss: 1.007115]\n",
      "911 [D loss: 0.999983] [G loss: 1.006936]\n",
      "912 [D loss: 1.000038] [G loss: 1.006974]\n",
      "913 [D loss: 0.999949] [G loss: 1.006855]\n",
      "914 [D loss: 1.000162] [G loss: 1.006769]\n",
      "915 [D loss: 0.999966] [G loss: 1.006596]\n",
      "916 [D loss: 1.000082] [G loss: 1.006342]\n",
      "917 [D loss: 1.000038] [G loss: 1.006208]\n",
      "918 [D loss: 1.000028] [G loss: 1.005979]\n",
      "919 [D loss: 1.000058] [G loss: 1.005889]\n",
      "920 [D loss: 1.000044] [G loss: 1.005784]\n",
      "921 [D loss: 1.000002] [G loss: 1.005786]\n",
      "922 [D loss: 1.000446] [G loss: 1.005804]\n",
      "923 [D loss: 1.000058] [G loss: 1.005697]\n",
      "924 [D loss: 1.000332] [G loss: 1.005637]\n",
      "925 [D loss: 1.000497] [G loss: 1.005596]\n",
      "926 [D loss: 1.001015] [G loss: 1.005814]\n",
      "927 [D loss: 1.000408] [G loss: 1.005850]\n",
      "928 [D loss: 1.000001] [G loss: 1.006214]\n",
      "929 [D loss: 1.000085] [G loss: 1.006583]\n",
      "930 [D loss: 1.000239] [G loss: 1.006182]\n",
      "931 [D loss: 1.000006] [G loss: 1.006410]\n",
      "932 [D loss: 1.000090] [G loss: 1.006554]\n",
      "933 [D loss: 1.000312] [G loss: 1.006662]\n",
      "934 [D loss: 1.000069] [G loss: 1.006735]\n",
      "935 [D loss: 1.000335] [G loss: 1.006662]\n",
      "936 [D loss: 1.000100] [G loss: 1.006737]\n",
      "937 [D loss: 0.999904] [G loss: 1.006481]\n",
      "938 [D loss: 1.000030] [G loss: 1.007035]\n",
      "939 [D loss: 1.000147] [G loss: 1.006908]\n",
      "940 [D loss: 1.000219] [G loss: 1.007150]\n",
      "941 [D loss: 1.000010] [G loss: 1.006962]\n",
      "942 [D loss: 0.999887] [G loss: 1.006707]\n",
      "943 [D loss: 1.000033] [G loss: 1.006697]\n",
      "944 [D loss: 1.000165] [G loss: 1.006332]\n",
      "945 [D loss: 1.000195] [G loss: 1.006129]\n",
      "946 [D loss: 1.000130] [G loss: 1.005996]\n",
      "947 [D loss: 1.000146] [G loss: 1.005756]\n",
      "948 [D loss: 1.000224] [G loss: 1.005877]\n",
      "949 [D loss: 1.000097] [G loss: 1.005965]\n",
      "950 [D loss: 1.000351] [G loss: 1.006124]\n",
      "951 [D loss: 1.000114] [G loss: 1.006042]\n",
      "952 [D loss: 1.000122] [G loss: 1.005957]\n",
      "953 [D loss: 1.000213] [G loss: 1.005861]\n",
      "954 [D loss: 1.000481] [G loss: 1.005815]\n",
      "955 [D loss: 1.000182] [G loss: 1.005899]\n",
      "956 [D loss: 1.000580] [G loss: 1.006042]\n",
      "957 [D loss: 1.000036] [G loss: 1.005882]\n",
      "958 [D loss: 1.000211] [G loss: 1.006070]\n",
      "959 [D loss: 1.000332] [G loss: 1.005995]\n",
      "960 [D loss: 1.000438] [G loss: 1.006139]\n",
      "961 [D loss: 0.999995] [G loss: 1.006216]\n",
      "962 [D loss: 1.000211] [G loss: 1.006130]\n",
      "963 [D loss: 1.000443] [G loss: 1.006553]\n",
      "964 [D loss: 1.000270] [G loss: 1.006250]\n",
      "965 [D loss: 1.000366] [G loss: 1.006379]\n",
      "966 [D loss: 1.000211] [G loss: 1.006610]\n",
      "967 [D loss: 1.000233] [G loss: 1.006385]\n",
      "968 [D loss: 1.000222] [G loss: 1.006344]\n",
      "969 [D loss: 1.000097] [G loss: 1.006422]\n",
      "970 [D loss: 1.000050] [G loss: 1.006597]\n",
      "971 [D loss: 1.000256] [G loss: 1.006597]\n",
      "972 [D loss: 1.000135] [G loss: 1.006400]\n",
      "973 [D loss: 1.000167] [G loss: 1.006345]\n",
      "974 [D loss: 1.000301] [G loss: 1.006342]\n",
      "975 [D loss: 1.000214] [G loss: 1.006291]\n",
      "976 [D loss: 1.000257] [G loss: 1.006509]\n",
      "977 [D loss: 1.000262] [G loss: 1.006338]\n",
      "978 [D loss: 1.000416] [G loss: 1.006242]\n",
      "979 [D loss: 1.000248] [G loss: 1.006407]\n",
      "980 [D loss: 1.000272] [G loss: 1.006317]\n",
      "981 [D loss: 1.000229] [G loss: 1.005998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 [D loss: 1.000148] [G loss: 1.006306]\n",
      "983 [D loss: 1.000069] [G loss: 1.005913]\n",
      "984 [D loss: 1.000065] [G loss: 1.005987]\n",
      "985 [D loss: 1.000069] [G loss: 1.006218]\n",
      "986 [D loss: 1.000181] [G loss: 1.006116]\n",
      "987 [D loss: 1.000358] [G loss: 1.005928]\n",
      "988 [D loss: 1.000123] [G loss: 1.006064]\n",
      "989 [D loss: 1.000428] [G loss: 1.006056]\n",
      "990 [D loss: 1.000240] [G loss: 1.005940]\n",
      "991 [D loss: 1.000532] [G loss: 1.006015]\n",
      "992 [D loss: 1.000105] [G loss: 1.005980]\n",
      "993 [D loss: 1.000144] [G loss: 1.005823]\n",
      "994 [D loss: 1.000506] [G loss: 1.005904]\n",
      "995 [D loss: 1.000216] [G loss: 1.005935]\n",
      "996 [D loss: 1.000364] [G loss: 1.006134]\n",
      "997 [D loss: 1.000123] [G loss: 1.006330]\n",
      "998 [D loss: 1.000023] [G loss: 1.006414]\n",
      "999 [D loss: 1.000181] [G loss: 1.006572]\n",
      "1000 [D loss: 1.000035] [G loss: 1.006418]\n",
      "1001 [D loss: 1.000312] [G loss: 1.006416]\n",
      "1002 [D loss: 1.000342] [G loss: 1.006541]\n",
      "1003 [D loss: 1.000465] [G loss: 1.006797]\n",
      "1004 [D loss: 1.000555] [G loss: 1.007018]\n",
      "1005 [D loss: 1.000289] [G loss: 1.007156]\n",
      "1006 [D loss: 1.000030] [G loss: 1.007468]\n",
      "1007 [D loss: 1.000103] [G loss: 1.007067]\n",
      "1008 [D loss: 0.999978] [G loss: 1.007339]\n",
      "1009 [D loss: 0.999961] [G loss: 1.007234]\n",
      "1010 [D loss: 0.999830] [G loss: 1.007167]\n",
      "1011 [D loss: 0.999824] [G loss: 1.007162]\n",
      "1012 [D loss: 0.999971] [G loss: 1.007106]\n",
      "1013 [D loss: 1.000142] [G loss: 1.006772]\n",
      "1014 [D loss: 1.000093] [G loss: 1.006663]\n",
      "1015 [D loss: 1.000089] [G loss: 1.006624]\n",
      "1016 [D loss: 1.000159] [G loss: 1.006419]\n",
      "1017 [D loss: 1.000381] [G loss: 1.006156]\n",
      "1018 [D loss: 1.000658] [G loss: 1.005734]\n",
      "1019 [D loss: 1.000376] [G loss: 1.006086]\n",
      "1020 [D loss: 1.001169] [G loss: 1.006118]\n",
      "1021 [D loss: 1.000789] [G loss: 1.006184]\n",
      "1022 [D loss: 1.000409] [G loss: 1.006136]\n",
      "1023 [D loss: 1.000681] [G loss: 1.006475]\n",
      "1024 [D loss: 1.000074] [G loss: 1.006538]\n",
      "1025 [D loss: 0.999857] [G loss: 1.006567]\n",
      "1026 [D loss: 1.000026] [G loss: 1.007146]\n",
      "1027 [D loss: 0.999845] [G loss: 1.007169]\n",
      "1028 [D loss: 1.000154] [G loss: 1.006922]\n",
      "1029 [D loss: 1.000293] [G loss: 1.006910]\n",
      "1030 [D loss: 1.000423] [G loss: 1.007029]\n",
      "1031 [D loss: 1.000240] [G loss: 1.006842]\n",
      "1032 [D loss: 1.000481] [G loss: 1.007332]\n",
      "1033 [D loss: 1.000201] [G loss: 1.007506]\n",
      "1034 [D loss: 1.000845] [G loss: 1.007298]\n",
      "1035 [D loss: 1.000586] [G loss: 1.007045]\n",
      "1036 [D loss: 1.000692] [G loss: 1.007954]\n",
      "1037 [D loss: 1.000543] [G loss: 1.007560]\n",
      "1038 [D loss: 0.999846] [G loss: 1.008119]\n",
      "1039 [D loss: 1.000341] [G loss: 1.008044]\n",
      "1040 [D loss: 0.999818] [G loss: 1.008273]\n",
      "1041 [D loss: 1.000058] [G loss: 1.008530]\n",
      "1042 [D loss: 0.999740] [G loss: 1.008371]\n",
      "1043 [D loss: 0.999869] [G loss: 1.008223]\n",
      "1044 [D loss: 0.999870] [G loss: 1.007729]\n",
      "1045 [D loss: 0.999852] [G loss: 1.007673]\n",
      "1046 [D loss: 0.999860] [G loss: 1.007480]\n",
      "1047 [D loss: 0.999887] [G loss: 1.007214]\n",
      "1048 [D loss: 0.999879] [G loss: 1.007096]\n",
      "1049 [D loss: 0.999904] [G loss: 1.007022]\n",
      "1050 [D loss: 0.999812] [G loss: 1.007140]\n",
      "1051 [D loss: 0.999816] [G loss: 1.007135]\n",
      "1052 [D loss: 1.000066] [G loss: 1.006931]\n",
      "1053 [D loss: 1.000071] [G loss: 1.006955]\n",
      "1054 [D loss: 1.000048] [G loss: 1.006709]\n",
      "1055 [D loss: 1.000077] [G loss: 1.006611]\n",
      "1056 [D loss: 1.000126] [G loss: 1.006482]\n",
      "1057 [D loss: 1.000322] [G loss: 1.006454]\n",
      "1058 [D loss: 1.000133] [G loss: 1.006256]\n",
      "1059 [D loss: 1.000258] [G loss: 1.006368]\n",
      "1060 [D loss: 1.000511] [G loss: 1.006316]\n",
      "1061 [D loss: 1.000526] [G loss: 1.006419]\n",
      "1062 [D loss: 1.000319] [G loss: 1.006386]\n",
      "1063 [D loss: 1.000075] [G loss: 1.006429]\n",
      "1064 [D loss: 1.000391] [G loss: 1.006580]\n",
      "1065 [D loss: 1.000365] [G loss: 1.006788]\n",
      "1066 [D loss: 1.000168] [G loss: 1.006941]\n",
      "1067 [D loss: 1.000171] [G loss: 1.007162]\n",
      "1068 [D loss: 1.000225] [G loss: 1.006883]\n",
      "1069 [D loss: 1.000091] [G loss: 1.007042]\n",
      "1070 [D loss: 1.000269] [G loss: 1.007094]\n",
      "1071 [D loss: 1.000499] [G loss: 1.007225]\n",
      "1072 [D loss: 1.000168] [G loss: 1.007110]\n",
      "1073 [D loss: 1.000429] [G loss: 1.007346]\n",
      "1074 [D loss: 1.000793] [G loss: 1.007510]\n",
      "1075 [D loss: 1.000069] [G loss: 1.007782]\n",
      "1076 [D loss: 1.000378] [G loss: 1.007682]\n",
      "1077 [D loss: 1.000325] [G loss: 1.007937]\n",
      "1078 [D loss: 0.999868] [G loss: 1.007804]\n",
      "1079 [D loss: 0.999743] [G loss: 1.007747]\n",
      "1080 [D loss: 1.000005] [G loss: 1.007623]\n",
      "1081 [D loss: 0.999944] [G loss: 1.007533]\n",
      "1082 [D loss: 0.999909] [G loss: 1.007529]\n",
      "1083 [D loss: 0.999891] [G loss: 1.007316]\n",
      "1084 [D loss: 1.000002] [G loss: 1.007123]\n",
      "1085 [D loss: 1.000049] [G loss: 1.006942]\n",
      "1086 [D loss: 1.000202] [G loss: 1.006828]\n",
      "1087 [D loss: 1.000143] [G loss: 1.006680]\n",
      "1088 [D loss: 1.000234] [G loss: 1.006470]\n",
      "1089 [D loss: 1.000297] [G loss: 1.006475]\n",
      "1090 [D loss: 1.000518] [G loss: 1.006337]\n",
      "1091 [D loss: 1.000585] [G loss: 1.006525]\n",
      "1092 [D loss: 1.000170] [G loss: 1.006414]\n",
      "1093 [D loss: 1.000556] [G loss: 1.006573]\n",
      "1094 [D loss: 1.000346] [G loss: 1.006581]\n",
      "1095 [D loss: 1.000125] [G loss: 1.006762]\n",
      "1096 [D loss: 1.000733] [G loss: 1.006845]\n",
      "1097 [D loss: 1.000574] [G loss: 1.006994]\n",
      "1098 [D loss: 1.000346] [G loss: 1.006910]\n",
      "1099 [D loss: 1.000118] [G loss: 1.007241]\n",
      "1100 [D loss: 1.000065] [G loss: 1.007553]\n",
      "1101 [D loss: 1.000267] [G loss: 1.007384]\n",
      "1102 [D loss: 1.000084] [G loss: 1.007449]\n",
      "1103 [D loss: 1.000193] [G loss: 1.007416]\n",
      "1104 [D loss: 1.000500] [G loss: 1.007197]\n",
      "1105 [D loss: 1.000350] [G loss: 1.007135]\n",
      "1106 [D loss: 1.000216] [G loss: 1.007729]\n",
      "1107 [D loss: 1.000241] [G loss: 1.007425]\n",
      "1108 [D loss: 1.000427] [G loss: 1.007603]\n",
      "1109 [D loss: 1.000811] [G loss: 1.007946]\n",
      "1110 [D loss: 1.000291] [G loss: 1.007980]\n",
      "1111 [D loss: 1.000212] [G loss: 1.007509]\n",
      "1112 [D loss: 1.000219] [G loss: 1.007476]\n",
      "1113 [D loss: 1.000141] [G loss: 1.007172]\n",
      "1114 [D loss: 1.000248] [G loss: 1.007085]\n",
      "1115 [D loss: 1.000221] [G loss: 1.007071]\n",
      "1116 [D loss: 1.000116] [G loss: 1.007163]\n",
      "1117 [D loss: 1.000205] [G loss: 1.006490]\n",
      "1118 [D loss: 1.000081] [G loss: 1.006690]\n",
      "1119 [D loss: 1.000144] [G loss: 1.006726]\n",
      "1120 [D loss: 1.000225] [G loss: 1.006950]\n",
      "1121 [D loss: 1.000064] [G loss: 1.006944]\n",
      "1122 [D loss: 1.000045] [G loss: 1.006748]\n",
      "1123 [D loss: 1.000140] [G loss: 1.006803]\n",
      "1124 [D loss: 1.000239] [G loss: 1.006656]\n",
      "1125 [D loss: 1.000265] [G loss: 1.006528]\n",
      "1126 [D loss: 1.000318] [G loss: 1.006551]\n",
      "1127 [D loss: 1.000315] [G loss: 1.006497]\n",
      "1128 [D loss: 1.000500] [G loss: 1.006422]\n",
      "1129 [D loss: 1.000520] [G loss: 1.006365]\n",
      "1130 [D loss: 1.000161] [G loss: 1.006511]\n",
      "1131 [D loss: 1.000247] [G loss: 1.006513]\n",
      "1132 [D loss: 1.000370] [G loss: 1.006908]\n",
      "1133 [D loss: 1.000195] [G loss: 1.007068]\n",
      "1134 [D loss: 1.000300] [G loss: 1.006931]\n",
      "1135 [D loss: 1.000547] [G loss: 1.007111]\n",
      "1136 [D loss: 1.000426] [G loss: 1.007227]\n",
      "1137 [D loss: 1.000433] [G loss: 1.007518]\n",
      "1138 [D loss: 1.000454] [G loss: 1.007585]\n",
      "1139 [D loss: 1.000443] [G loss: 1.007876]\n",
      "1140 [D loss: 1.000456] [G loss: 1.007808]\n",
      "1141 [D loss: 1.000006] [G loss: 1.007921]\n",
      "1142 [D loss: 1.000310] [G loss: 1.007516]\n",
      "1143 [D loss: 1.000103] [G loss: 1.008035]\n",
      "1144 [D loss: 1.000186] [G loss: 1.007969]\n",
      "1145 [D loss: 1.000070] [G loss: 1.007927]\n",
      "1146 [D loss: 1.000096] [G loss: 1.007668]\n",
      "1147 [D loss: 1.000098] [G loss: 1.007215]\n",
      "1148 [D loss: 1.000192] [G loss: 1.007114]\n",
      "1149 [D loss: 1.000260] [G loss: 1.007108]\n",
      "1150 [D loss: 1.000142] [G loss: 1.007091]\n",
      "1151 [D loss: 1.000131] [G loss: 1.007113]\n",
      "1152 [D loss: 1.000253] [G loss: 1.007199]\n",
      "1153 [D loss: 1.000135] [G loss: 1.007154]\n",
      "1154 [D loss: 1.000068] [G loss: 1.007133]\n",
      "1155 [D loss: 1.000131] [G loss: 1.006997]\n",
      "1156 [D loss: 1.000129] [G loss: 1.006845]\n",
      "1157 [D loss: 1.000188] [G loss: 1.006791]\n",
      "1158 [D loss: 1.000573] [G loss: 1.006951]\n",
      "1159 [D loss: 1.000437] [G loss: 1.006841]\n",
      "1160 [D loss: 1.000359] [G loss: 1.006896]\n",
      "1161 [D loss: 1.000525] [G loss: 1.006997]\n",
      "1162 [D loss: 1.000307] [G loss: 1.007104]\n",
      "1163 [D loss: 1.001180] [G loss: 1.007214]\n",
      "1164 [D loss: 1.000601] [G loss: 1.007571]\n",
      "1165 [D loss: 1.000022] [G loss: 1.007321]\n",
      "1166 [D loss: 1.000417] [G loss: 1.007222]\n",
      "1167 [D loss: 1.000472] [G loss: 1.007297]\n",
      "1168 [D loss: 1.000546] [G loss: 1.007543]\n",
      "1169 [D loss: 1.000656] [G loss: 1.007859]\n",
      "1170 [D loss: 1.000635] [G loss: 1.007770]\n",
      "1171 [D loss: 1.000233] [G loss: 1.008535]\n",
      "1172 [D loss: 1.000010] [G loss: 1.008719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173 [D loss: 1.000171] [G loss: 1.008606]\n",
      "1174 [D loss: 1.000105] [G loss: 1.008472]\n",
      "1175 [D loss: 1.000161] [G loss: 1.008432]\n",
      "1176 [D loss: 0.999981] [G loss: 1.008177]\n",
      "1177 [D loss: 1.000141] [G loss: 1.008096]\n",
      "1178 [D loss: 1.000147] [G loss: 1.008139]\n",
      "1179 [D loss: 1.000037] [G loss: 1.007863]\n",
      "1180 [D loss: 1.000069] [G loss: 1.007838]\n",
      "1181 [D loss: 1.000459] [G loss: 1.007722]\n",
      "1182 [D loss: 1.000296] [G loss: 1.007690]\n",
      "1183 [D loss: 1.000190] [G loss: 1.007558]\n",
      "1184 [D loss: 1.000535] [G loss: 1.007499]\n",
      "1185 [D loss: 1.000439] [G loss: 1.007119]\n",
      "1186 [D loss: 1.000121] [G loss: 1.007302]\n",
      "1187 [D loss: 1.000235] [G loss: 1.007310]\n",
      "1188 [D loss: 0.999958] [G loss: 1.007264]\n",
      "1189 [D loss: 1.000466] [G loss: 1.007251]\n",
      "1190 [D loss: 1.000255] [G loss: 1.007425]\n",
      "1191 [D loss: 1.000254] [G loss: 1.007677]\n",
      "1192 [D loss: 1.000176] [G loss: 1.007636]\n",
      "1193 [D loss: 1.000302] [G loss: 1.007566]\n",
      "1194 [D loss: 1.000256] [G loss: 1.007764]\n",
      "1195 [D loss: 1.000066] [G loss: 1.007764]\n",
      "1196 [D loss: 1.000244] [G loss: 1.007855]\n",
      "1197 [D loss: 1.000175] [G loss: 1.008147]\n",
      "1198 [D loss: 1.000389] [G loss: 1.008379]\n",
      "1199 [D loss: 1.000093] [G loss: 1.008408]\n",
      "1200 [D loss: 1.000131] [G loss: 1.008469]\n",
      "1201 [D loss: 1.000276] [G loss: 1.008187]\n",
      "1202 [D loss: 1.000094] [G loss: 1.008220]\n",
      "1203 [D loss: 1.000060] [G loss: 1.008417]\n",
      "1204 [D loss: 1.000125] [G loss: 1.007849]\n",
      "1205 [D loss: 1.000075] [G loss: 1.008191]\n",
      "1206 [D loss: 1.000291] [G loss: 1.008164]\n",
      "1207 [D loss: 1.000292] [G loss: 1.008073]\n",
      "1208 [D loss: 1.000395] [G loss: 1.007888]\n",
      "1209 [D loss: 1.000089] [G loss: 1.007787]\n",
      "1210 [D loss: 1.000306] [G loss: 1.007782]\n",
      "1211 [D loss: 1.000289] [G loss: 1.007714]\n",
      "1212 [D loss: 1.000206] [G loss: 1.007699]\n",
      "1213 [D loss: 1.000215] [G loss: 1.007514]\n",
      "1214 [D loss: 1.000234] [G loss: 1.007468]\n",
      "1215 [D loss: 1.000080] [G loss: 1.007194]\n",
      "1216 [D loss: 1.000243] [G loss: 1.007457]\n",
      "1217 [D loss: 0.999993] [G loss: 1.007618]\n",
      "1218 [D loss: 1.000107] [G loss: 1.007388]\n",
      "1219 [D loss: 1.000637] [G loss: 1.007071]\n",
      "1220 [D loss: 1.000325] [G loss: 1.007330]\n",
      "1221 [D loss: 1.000059] [G loss: 1.007244]\n",
      "1222 [D loss: 1.000099] [G loss: 1.007258]\n",
      "1223 [D loss: 1.000426] [G loss: 1.007281]\n",
      "1224 [D loss: 1.000130] [G loss: 1.007309]\n",
      "1225 [D loss: 1.000266] [G loss: 1.007303]\n",
      "1226 [D loss: 1.000332] [G loss: 1.007358]\n",
      "1227 [D loss: 1.000219] [G loss: 1.007253]\n",
      "1228 [D loss: 1.000267] [G loss: 1.007308]\n",
      "1229 [D loss: 1.000570] [G loss: 1.007203]\n",
      "1230 [D loss: 1.000218] [G loss: 1.007155]\n",
      "1231 [D loss: 1.000386] [G loss: 1.007124]\n",
      "1232 [D loss: 1.000044] [G loss: 1.007732]\n",
      "1233 [D loss: 1.000348] [G loss: 1.007459]\n",
      "1234 [D loss: 1.000148] [G loss: 1.007870]\n",
      "1235 [D loss: 1.000393] [G loss: 1.007741]\n",
      "1236 [D loss: 1.000146] [G loss: 1.007466]\n",
      "1237 [D loss: 1.000001] [G loss: 1.008240]\n",
      "1238 [D loss: 1.000467] [G loss: 1.007967]\n",
      "1239 [D loss: 1.000137] [G loss: 1.007871]\n",
      "1240 [D loss: 1.000319] [G loss: 1.007699]\n",
      "1241 [D loss: 1.000080] [G loss: 1.007844]\n",
      "1242 [D loss: 1.000397] [G loss: 1.007721]\n",
      "1243 [D loss: 1.000223] [G loss: 1.007784]\n",
      "1244 [D loss: 1.000363] [G loss: 1.007780]\n",
      "1245 [D loss: 1.000266] [G loss: 1.007462]\n",
      "1246 [D loss: 1.000309] [G loss: 1.007503]\n",
      "1247 [D loss: 1.000029] [G loss: 1.007514]\n",
      "1248 [D loss: 1.000095] [G loss: 1.007431]\n",
      "1249 [D loss: 1.000446] [G loss: 1.007364]\n",
      "1250 [D loss: 1.000111] [G loss: 1.007153]\n",
      "1251 [D loss: 1.000133] [G loss: 1.007291]\n",
      "1252 [D loss: 1.000055] [G loss: 1.007285]\n",
      "1253 [D loss: 1.000057] [G loss: 1.007156]\n",
      "1254 [D loss: 1.000088] [G loss: 1.007081]\n",
      "1255 [D loss: 1.000454] [G loss: 1.006960]\n",
      "1256 [D loss: 1.000476] [G loss: 1.007004]\n",
      "1257 [D loss: 1.000458] [G loss: 1.006776]\n",
      "1258 [D loss: 1.000655] [G loss: 1.007014]\n",
      "1259 [D loss: 1.000402] [G loss: 1.006931]\n",
      "1260 [D loss: 0.999998] [G loss: 1.006994]\n",
      "1261 [D loss: 1.000348] [G loss: 1.007016]\n",
      "1262 [D loss: 1.000155] [G loss: 1.006972]\n",
      "1263 [D loss: 1.000357] [G loss: 1.007512]\n",
      "1264 [D loss: 1.000428] [G loss: 1.007327]\n",
      "1265 [D loss: 1.000029] [G loss: 1.007158]\n",
      "1266 [D loss: 1.000271] [G loss: 1.007371]\n",
      "1267 [D loss: 1.000348] [G loss: 1.007451]\n",
      "1268 [D loss: 1.000312] [G loss: 1.007433]\n",
      "1269 [D loss: 1.000385] [G loss: 1.007576]\n",
      "1270 [D loss: 1.000448] [G loss: 1.007601]\n",
      "1271 [D loss: 1.000456] [G loss: 1.008096]\n",
      "1272 [D loss: 1.000734] [G loss: 1.007667]\n",
      "1273 [D loss: 1.000139] [G loss: 1.008389]\n",
      "1274 [D loss: 1.000606] [G loss: 1.008584]\n",
      "1275 [D loss: 1.000526] [G loss: 1.008041]\n",
      "1276 [D loss: 1.000195] [G loss: 1.008010]\n",
      "1277 [D loss: 1.000306] [G loss: 1.007877]\n",
      "1278 [D loss: 1.000214] [G loss: 1.007747]\n",
      "1279 [D loss: 1.000082] [G loss: 1.007532]\n",
      "1280 [D loss: 0.999914] [G loss: 1.007560]\n",
      "1281 [D loss: 1.000354] [G loss: 1.007438]\n",
      "1282 [D loss: 1.000154] [G loss: 1.007160]\n",
      "1283 [D loss: 1.000332] [G loss: 1.007310]\n",
      "1284 [D loss: 1.000107] [G loss: 1.007146]\n",
      "1285 [D loss: 1.000419] [G loss: 1.007125]\n",
      "1286 [D loss: 1.000379] [G loss: 1.007124]\n",
      "1287 [D loss: 1.000386] [G loss: 1.007057]\n",
      "1288 [D loss: 1.000187] [G loss: 1.007082]\n",
      "1289 [D loss: 1.000197] [G loss: 1.006903]\n",
      "1290 [D loss: 1.000247] [G loss: 1.006878]\n",
      "1291 [D loss: 1.000225] [G loss: 1.007183]\n",
      "1292 [D loss: 1.000028] [G loss: 1.006931]\n",
      "1293 [D loss: 1.000417] [G loss: 1.007159]\n",
      "1294 [D loss: 1.000202] [G loss: 1.007013]\n",
      "1295 [D loss: 1.000284] [G loss: 1.007264]\n",
      "1296 [D loss: 1.000416] [G loss: 1.007513]\n",
      "1297 [D loss: 1.000533] [G loss: 1.007577]\n",
      "1298 [D loss: 1.000125] [G loss: 1.007558]\n",
      "1299 [D loss: 1.000603] [G loss: 1.007873]\n",
      "1300 [D loss: 1.000566] [G loss: 1.007655]\n",
      "1301 [D loss: 1.000596] [G loss: 1.007994]\n",
      "1302 [D loss: 1.000381] [G loss: 1.008063]\n",
      "1303 [D loss: 1.000119] [G loss: 1.008058]\n",
      "1304 [D loss: 1.000203] [G loss: 1.008227]\n",
      "1305 [D loss: 1.000064] [G loss: 1.007934]\n",
      "1306 [D loss: 1.000384] [G loss: 1.007850]\n",
      "1307 [D loss: 1.000162] [G loss: 1.007690]\n",
      "1308 [D loss: 1.000162] [G loss: 1.007802]\n",
      "1309 [D loss: 1.000179] [G loss: 1.007600]\n",
      "1310 [D loss: 1.000240] [G loss: 1.007612]\n",
      "1311 [D loss: 1.000275] [G loss: 1.007588]\n",
      "1312 [D loss: 1.000022] [G loss: 1.007586]\n",
      "1313 [D loss: 1.000316] [G loss: 1.007389]\n",
      "1314 [D loss: 1.000110] [G loss: 1.007343]\n",
      "1315 [D loss: 1.000586] [G loss: 1.007296]\n",
      "1316 [D loss: 1.000586] [G loss: 1.007258]\n",
      "1317 [D loss: 1.000713] [G loss: 1.007383]\n",
      "1318 [D loss: 1.000464] [G loss: 1.007324]\n",
      "1319 [D loss: 1.000283] [G loss: 1.007235]\n",
      "1320 [D loss: 1.000818] [G loss: 1.007316]\n",
      "1321 [D loss: 1.000428] [G loss: 1.007692]\n",
      "1322 [D loss: 1.000414] [G loss: 1.007705]\n",
      "1323 [D loss: 0.999977] [G loss: 1.007728]\n",
      "1324 [D loss: 1.000375] [G loss: 1.007604]\n",
      "1325 [D loss: 1.000783] [G loss: 1.007636]\n",
      "1326 [D loss: 1.000564] [G loss: 1.007357]\n",
      "1327 [D loss: 1.000415] [G loss: 1.008356]\n",
      "1328 [D loss: 1.000786] [G loss: 1.007970]\n",
      "1329 [D loss: 1.000630] [G loss: 1.007995]\n",
      "1330 [D loss: 1.000941] [G loss: 1.007945]\n",
      "1331 [D loss: 1.000034] [G loss: 1.008172]\n",
      "1332 [D loss: 1.000227] [G loss: 1.008652]\n",
      "1333 [D loss: 1.000346] [G loss: 1.008574]\n",
      "1334 [D loss: 1.000142] [G loss: 1.008093]\n",
      "1335 [D loss: 1.000248] [G loss: 1.007864]\n",
      "1336 [D loss: 1.000147] [G loss: 1.008012]\n",
      "1337 [D loss: 1.000093] [G loss: 1.008022]\n",
      "1338 [D loss: 1.000255] [G loss: 1.008184]\n",
      "1339 [D loss: 1.000089] [G loss: 1.007961]\n",
      "1340 [D loss: 1.000177] [G loss: 1.007872]\n",
      "1341 [D loss: 0.999991] [G loss: 1.007492]\n",
      "1342 [D loss: 1.000091] [G loss: 1.007628]\n",
      "1343 [D loss: 1.000168] [G loss: 1.007553]\n",
      "1344 [D loss: 1.000287] [G loss: 1.007272]\n",
      "1345 [D loss: 1.000123] [G loss: 1.007325]\n",
      "1346 [D loss: 1.000092] [G loss: 1.007243]\n",
      "1347 [D loss: 1.000341] [G loss: 1.007217]\n",
      "1348 [D loss: 1.000504] [G loss: 1.007137]\n",
      "1349 [D loss: 1.000288] [G loss: 1.007264]\n",
      "1350 [D loss: 1.000168] [G loss: 1.007104]\n",
      "1351 [D loss: 1.000042] [G loss: 1.007095]\n",
      "1352 [D loss: 1.000393] [G loss: 1.007190]\n",
      "1353 [D loss: 1.000373] [G loss: 1.007498]\n",
      "1354 [D loss: 1.000342] [G loss: 1.007341]\n",
      "1355 [D loss: 1.000342] [G loss: 1.007314]\n",
      "1356 [D loss: 1.000192] [G loss: 1.007871]\n",
      "1357 [D loss: 1.000160] [G loss: 1.007870]\n",
      "1358 [D loss: 1.000449] [G loss: 1.007686]\n",
      "1359 [D loss: 1.000508] [G loss: 1.007691]\n",
      "1360 [D loss: 1.000458] [G loss: 1.007705]\n",
      "1361 [D loss: 1.000406] [G loss: 1.008095]\n",
      "1362 [D loss: 1.000499] [G loss: 1.007922]\n",
      "1363 [D loss: 1.000627] [G loss: 1.008323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1364 [D loss: 1.000528] [G loss: 1.007828]\n",
      "1365 [D loss: 1.000317] [G loss: 1.008116]\n",
      "1366 [D loss: 1.000302] [G loss: 1.008186]\n",
      "1367 [D loss: 1.000243] [G loss: 1.008289]\n",
      "1368 [D loss: 1.000150] [G loss: 1.007953]\n",
      "1369 [D loss: 1.000049] [G loss: 1.008056]\n",
      "1370 [D loss: 0.999896] [G loss: 1.008118]\n",
      "1371 [D loss: 0.999886] [G loss: 1.007991]\n",
      "1372 [D loss: 0.999928] [G loss: 1.007960]\n",
      "1373 [D loss: 1.000135] [G loss: 1.007717]\n",
      "1374 [D loss: 1.000481] [G loss: 1.006849]\n",
      "1375 [D loss: 1.000387] [G loss: 1.007483]\n",
      "1376 [D loss: 1.000106] [G loss: 1.007241]\n",
      "1377 [D loss: 1.000262] [G loss: 1.007381]\n",
      "1378 [D loss: 1.000418] [G loss: 1.007164]\n",
      "1379 [D loss: 1.000176] [G loss: 1.007372]\n",
      "1380 [D loss: 1.000659] [G loss: 1.007245]\n",
      "1381 [D loss: 1.000217] [G loss: 1.007105]\n",
      "1382 [D loss: 0.999907] [G loss: 1.007210]\n",
      "1383 [D loss: 1.000409] [G loss: 1.007415]\n",
      "1384 [D loss: 1.000431] [G loss: 1.007463]\n",
      "1385 [D loss: 1.000364] [G loss: 1.007303]\n",
      "1386 [D loss: 1.000343] [G loss: 1.007607]\n",
      "1387 [D loss: 1.000421] [G loss: 1.007714]\n",
      "1388 [D loss: 1.000447] [G loss: 1.007754]\n",
      "1389 [D loss: 1.000420] [G loss: 1.008170]\n",
      "1390 [D loss: 1.000490] [G loss: 1.007817]\n",
      "1391 [D loss: 1.000584] [G loss: 1.007821]\n",
      "1392 [D loss: 1.000716] [G loss: 1.007754]\n",
      "1393 [D loss: 1.000418] [G loss: 1.008137]\n",
      "1394 [D loss: 1.000269] [G loss: 1.008160]\n",
      "1395 [D loss: 1.000495] [G loss: 1.008077]\n",
      "1396 [D loss: 1.000368] [G loss: 1.007976]\n",
      "1397 [D loss: 1.000237] [G loss: 1.008073]\n",
      "1398 [D loss: 1.000147] [G loss: 1.008107]\n",
      "1399 [D loss: 1.000158] [G loss: 1.007877]\n",
      "1400 [D loss: 1.000264] [G loss: 1.008216]\n",
      "1401 [D loss: 1.000340] [G loss: 1.007622]\n",
      "1402 [D loss: 1.000163] [G loss: 1.007739]\n",
      "1403 [D loss: 1.000081] [G loss: 1.007921]\n",
      "1404 [D loss: 1.000112] [G loss: 1.007730]\n",
      "1405 [D loss: 1.000255] [G loss: 1.007742]\n",
      "1406 [D loss: 1.000076] [G loss: 1.007205]\n",
      "1407 [D loss: 1.000176] [G loss: 1.007569]\n",
      "1408 [D loss: 1.000848] [G loss: 1.007536]\n",
      "1409 [D loss: 1.000345] [G loss: 1.007368]\n",
      "1410 [D loss: 1.000396] [G loss: 1.007450]\n",
      "1411 [D loss: 1.000702] [G loss: 1.007396]\n",
      "1412 [D loss: 1.000410] [G loss: 1.007153]\n",
      "1413 [D loss: 1.000422] [G loss: 1.007594]\n",
      "1414 [D loss: 1.000234] [G loss: 1.007510]\n",
      "1415 [D loss: 1.000690] [G loss: 1.007273]\n",
      "1416 [D loss: 1.000137] [G loss: 1.007724]\n",
      "1417 [D loss: 1.000333] [G loss: 1.007681]\n",
      "1418 [D loss: 0.999948] [G loss: 1.007815]\n",
      "1419 [D loss: 1.000253] [G loss: 1.008083]\n",
      "1420 [D loss: 1.000252] [G loss: 1.007511]\n",
      "1421 [D loss: 1.000378] [G loss: 1.007902]\n",
      "1422 [D loss: 1.000732] [G loss: 1.007665]\n",
      "1423 [D loss: 1.000584] [G loss: 1.007730]\n",
      "1424 [D loss: 1.000512] [G loss: 1.007541]\n",
      "1425 [D loss: 1.000846] [G loss: 1.007636]\n",
      "1426 [D loss: 1.000555] [G loss: 1.007771]\n",
      "1427 [D loss: 1.000713] [G loss: 1.007747]\n",
      "1428 [D loss: 1.000363] [G loss: 1.008216]\n",
      "1429 [D loss: 1.000004] [G loss: 1.007946]\n",
      "1430 [D loss: 1.000101] [G loss: 1.008111]\n",
      "1431 [D loss: 1.000181] [G loss: 1.008122]\n",
      "1432 [D loss: 1.000179] [G loss: 1.008112]\n",
      "1433 [D loss: 0.999937] [G loss: 1.008004]\n",
      "1434 [D loss: 0.999919] [G loss: 1.008118]\n",
      "1435 [D loss: 1.000064] [G loss: 1.007724]\n",
      "1436 [D loss: 1.000190] [G loss: 1.007401]\n",
      "1437 [D loss: 1.000094] [G loss: 1.007672]\n",
      "1438 [D loss: 1.000136] [G loss: 1.007759]\n",
      "1439 [D loss: 1.000127] [G loss: 1.007766]\n",
      "1440 [D loss: 1.000124] [G loss: 1.007516]\n",
      "1441 [D loss: 1.000436] [G loss: 1.007326]\n",
      "1442 [D loss: 1.000594] [G loss: 1.007441]\n",
      "1443 [D loss: 1.000389] [G loss: 1.007282]\n",
      "1444 [D loss: 1.000470] [G loss: 1.007443]\n",
      "1445 [D loss: 1.000427] [G loss: 1.007247]\n",
      "1446 [D loss: 1.000101] [G loss: 1.007513]\n",
      "1447 [D loss: 1.000187] [G loss: 1.007752]\n",
      "1448 [D loss: 1.000286] [G loss: 1.007923]\n",
      "1449 [D loss: 1.000028] [G loss: 1.007935]\n",
      "1450 [D loss: 1.000084] [G loss: 1.008021]\n",
      "1451 [D loss: 1.000260] [G loss: 1.008424]\n",
      "1452 [D loss: 1.000356] [G loss: 1.007984]\n",
      "1453 [D loss: 1.000180] [G loss: 1.007992]\n",
      "1454 [D loss: 1.000358] [G loss: 1.008417]\n",
      "1455 [D loss: 1.000755] [G loss: 1.008319]\n",
      "1456 [D loss: 1.000703] [G loss: 1.008411]\n",
      "1457 [D loss: 1.000394] [G loss: 1.008443]\n",
      "1458 [D loss: 1.000623] [G loss: 1.008529]\n",
      "1459 [D loss: 1.000235] [G loss: 1.008518]\n",
      "1460 [D loss: 1.000078] [G loss: 1.008188]\n",
      "1461 [D loss: 1.000436] [G loss: 1.008301]\n",
      "1462 [D loss: 1.000015] [G loss: 1.008567]\n",
      "1463 [D loss: 1.000237] [G loss: 1.008525]\n",
      "1464 [D loss: 1.000022] [G loss: 1.008346]\n",
      "1465 [D loss: 1.000314] [G loss: 1.008333]\n",
      "1466 [D loss: 1.000217] [G loss: 1.007597]\n",
      "1467 [D loss: 1.000229] [G loss: 1.008023]\n",
      "1468 [D loss: 1.000102] [G loss: 1.008058]\n",
      "1469 [D loss: 1.000173] [G loss: 1.008126]\n",
      "1470 [D loss: 1.000277] [G loss: 1.007991]\n",
      "1471 [D loss: 1.000395] [G loss: 1.008032]\n",
      "1472 [D loss: 1.000327] [G loss: 1.007870]\n",
      "1473 [D loss: 1.000496] [G loss: 1.007740]\n",
      "1474 [D loss: 1.000283] [G loss: 1.007799]\n",
      "1475 [D loss: 1.001066] [G loss: 1.007724]\n",
      "1476 [D loss: 1.000403] [G loss: 1.007759]\n",
      "1477 [D loss: 1.000673] [G loss: 1.007622]\n",
      "1478 [D loss: 1.000485] [G loss: 1.007803]\n",
      "1479 [D loss: 1.000285] [G loss: 1.007876]\n",
      "1480 [D loss: 1.000331] [G loss: 1.008158]\n",
      "1481 [D loss: 1.000419] [G loss: 1.008198]\n",
      "1482 [D loss: 1.000647] [G loss: 1.007962]\n",
      "1483 [D loss: 0.999950] [G loss: 1.008239]\n",
      "1484 [D loss: 1.000401] [G loss: 1.008711]\n",
      "1485 [D loss: 1.000581] [G loss: 1.008538]\n",
      "1486 [D loss: 1.000685] [G loss: 1.008414]\n",
      "1487 [D loss: 1.000637] [G loss: 1.008546]\n",
      "1488 [D loss: 1.000697] [G loss: 1.008060]\n",
      "1489 [D loss: 1.000775] [G loss: 1.008743]\n",
      "1490 [D loss: 1.000000] [G loss: 1.008282]\n",
      "1491 [D loss: 1.001000] [G loss: 1.008691]\n",
      "1492 [D loss: 1.000253] [G loss: 1.008795]\n",
      "1493 [D loss: 1.000087] [G loss: 1.009404]\n",
      "1494 [D loss: 0.999859] [G loss: 1.009560]\n",
      "1495 [D loss: 1.000109] [G loss: 1.008991]\n",
      "1496 [D loss: 1.000169] [G loss: 1.008757]\n",
      "1497 [D loss: 0.999836] [G loss: 1.008465]\n",
      "1498 [D loss: 1.000089] [G loss: 1.008616]\n",
      "1499 [D loss: 1.000165] [G loss: 1.008619]\n",
      "1500 [D loss: 1.000061] [G loss: 1.008798]\n",
      "1501 [D loss: 1.000313] [G loss: 1.008691]\n",
      "1502 [D loss: 1.000446] [G loss: 1.008429]\n",
      "1503 [D loss: 1.000327] [G loss: 1.008182]\n",
      "1504 [D loss: 1.000244] [G loss: 1.008189]\n",
      "1505 [D loss: 1.000092] [G loss: 1.008256]\n",
      "1506 [D loss: 1.000144] [G loss: 1.008021]\n",
      "1507 [D loss: 1.000350] [G loss: 1.007992]\n",
      "1508 [D loss: 1.000987] [G loss: 1.007970]\n",
      "1509 [D loss: 1.000361] [G loss: 1.007782]\n",
      "1510 [D loss: 1.000283] [G loss: 1.008062]\n",
      "1511 [D loss: 1.000284] [G loss: 1.007991]\n",
      "1512 [D loss: 1.000091] [G loss: 1.008452]\n",
      "1513 [D loss: 1.000221] [G loss: 1.008313]\n",
      "1514 [D loss: 1.000259] [G loss: 1.008663]\n",
      "1515 [D loss: 1.000206] [G loss: 1.008463]\n",
      "1516 [D loss: 1.000285] [G loss: 1.008659]\n",
      "1517 [D loss: 1.000367] [G loss: 1.008559]\n",
      "1518 [D loss: 1.000252] [G loss: 1.008583]\n",
      "1519 [D loss: 1.000418] [G loss: 1.008470]\n",
      "1520 [D loss: 1.000354] [G loss: 1.008751]\n",
      "1521 [D loss: 1.000756] [G loss: 1.008656]\n",
      "1522 [D loss: 1.000561] [G loss: 1.008832]\n",
      "1523 [D loss: 1.000137] [G loss: 1.008873]\n",
      "1524 [D loss: 1.000282] [G loss: 1.008910]\n",
      "1525 [D loss: 1.000370] [G loss: 1.008779]\n",
      "1526 [D loss: 1.000147] [G loss: 1.008968]\n",
      "1527 [D loss: 1.000268] [G loss: 1.009116]\n",
      "1528 [D loss: 1.000179] [G loss: 1.009142]\n",
      "1529 [D loss: 0.999989] [G loss: 1.008842]\n",
      "1530 [D loss: 0.999861] [G loss: 1.009195]\n",
      "1531 [D loss: 0.999948] [G loss: 1.008772]\n",
      "1532 [D loss: 0.999796] [G loss: 1.009023]\n",
      "1533 [D loss: 1.000110] [G loss: 1.008568]\n",
      "1534 [D loss: 1.000145] [G loss: 1.008301]\n",
      "1535 [D loss: 1.000099] [G loss: 1.008719]\n",
      "1536 [D loss: 1.000241] [G loss: 1.008418]\n",
      "1537 [D loss: 1.000233] [G loss: 1.008177]\n",
      "1538 [D loss: 1.000462] [G loss: 1.008248]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-2b914a92ce82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-1e7b46e4808e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, n_generator, n_critic, dataset, img_frequency, clip_value)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                                   critic_loss_fake)\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-1e7b46e4808e>\u001b[0m in \u001b[0;36m_clip_weights\u001b[0;34m(self, clip_value)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_clip_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2356\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m             \u001b[0mtf_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_assign_placeholder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m                 \u001b[0massign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists('wgan'):\n",
    "    shutil.rmtree('wgan')\n",
    "os.makedirs('wgan')\n",
    "\n",
    "wgan = WGAN(timesteps, latent_dim, generator_type, critic_type)\n",
    "wgan.build_model(lr)\n",
    "wgan.train(batch_size, epochs, n_generator, n_critic, transactions, img_frequency, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
