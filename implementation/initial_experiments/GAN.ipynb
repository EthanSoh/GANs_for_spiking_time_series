{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import Lambda, LSTM, RepeatVector, Dense, TimeDistributed, Bidirectional, concatenate,\\\n",
    "Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, Activation, Flatten, Reshape\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193500 50\n"
     ]
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../../datasets/berka_dataset/usable/normalized_transactions.npy\"\n",
    "\n",
    "timesteps = 50\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions = split_data(transactions, timesteps)\n",
    "np.random.shuffle(transactions)\n",
    "N, D = transactions.shape\n",
    "print(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     1,
     119,
     156,
     177
    ]
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, timesteps, latent_dim, generator_type,\n",
    "                 discriminator_type):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._generator_type = generator_type\n",
    "        self._discriminator_type = discriminator_type\n",
    "\n",
    "    def build_model(self, lr):\n",
    "        optimizer = RMSprop(lr, clipnorm=1.0)\n",
    "\n",
    "        self._generator = self._get_generator(\n",
    "            self._latent_dim, self._timesteps, self._generator_type)\n",
    "        self._generator.compile(\n",
    "            loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        self._discriminator = self._get_discriminator(self._timesteps,\n",
    "                                                      self._discriminator_type)\n",
    "        self._discriminator.compile(\n",
    "            loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        real = Input(shape=[\n",
    "            self._timesteps,\n",
    "        ])\n",
    "\n",
    "        self._discriminator.trainable = False\n",
    "\n",
    "        valid = self._discriminator(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "#         self._gan.summary()\n",
    "#         self._generator.summary()\n",
    "#         self._discriminator.summary()\n",
    "        return self._gan, self._generator, self._discriminator\n",
    "\n",
    "    def _get_generator(self, noise_dim, timesteps, generator_type):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "\n",
    "        if discriminator_type == 'dense':\n",
    "            generated = Dense(timesteps, activation='relu')(generator_inputs)\n",
    "            generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        elif generator_type == 'conv':\n",
    "            generated = Dense(12, activation='tanh')(generator_inputs)\n",
    "            generated = Reshape((4, 3))(generated)\n",
    "            while generated.shape[1] < timesteps:\n",
    "                generated = Conv1D(\n",
    "                    32, 3, activation=None, padding='same')(generated)\n",
    "                generated = BatchNormalization()(generated)\n",
    "                generated = Activation('tanh')(generated)\n",
    "                generated = UpSampling1D(2)(generated)\n",
    "            generated = Conv1D(\n",
    "                1, 3, activation='tanh', padding='same')(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "            generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        elif generator_type == 'lstm':\n",
    "            generated = RepeatVector(timesteps)(generator_inputs)\n",
    "            generated = LSTM(32, return_sequences=True)(generated)\n",
    "            generated = TimeDistributed(Dense(1, activation='tanh'))(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "\n",
    "        elif generator_type == 'blstm':\n",
    "            generated = RepeatVector(timesteps)(generator_inputs)\n",
    "            generated = Bidirectional(LSTM(32,\n",
    "                                           return_sequences=True))(generated)\n",
    "            generated = TimeDistributed(Dense(1, activation='tanh'))(generated)\n",
    "            generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _get_discriminator(self, timesteps, discriminator_type):\n",
    "        discriminator_inputs = Input((timesteps, ))\n",
    "\n",
    "        if discriminator_type == 'dense':\n",
    "            discriminated = Dense(\n",
    "                timesteps, activation='relu')(discriminator_inputs)\n",
    "            discriminated = Dense(timesteps, activation='relu')(discriminated)\n",
    "            discriminated = Dense(1, activation='sigmoid')(discriminated)\n",
    "\n",
    "        elif discriminator_type == 'conv':\n",
    "            discriminated = Lambda(lambda x: K.expand_dims(x))(\n",
    "                discriminator_inputs)\n",
    "            while discriminated.shape[1] > 3:\n",
    "                discriminated = Conv1D(\n",
    "                    32, 3, activation=None, padding='same')(discriminated)\n",
    "                discriminated = BatchNormalization()(discriminated)\n",
    "                discriminated = Activation('tanh')(discriminated)\n",
    "                discriminated = MaxPooling1D(2, padding='same')(discriminated)\n",
    "            discriminated = Flatten()(discriminated)\n",
    "            discriminated = Dense(1, activation='sigmoid')(discriminated)            \n",
    "\n",
    "        elif discriminator_type == 'lstm':\n",
    "            discriminated = Lambda(lambda x: K.expand_dims(x))(\n",
    "                discriminator_inputs)\n",
    "            discriminated = LSTM(32, return_sequences=False)(discriminated)\n",
    "            discriminated = Dense(1, activation='sigmoid')(discriminated)\n",
    "\n",
    "        elif discriminator_type == 'blstm':\n",
    "            discriminated = Lambda(lambda x: K.expand_dims(x))(\n",
    "                discriminator_inputs)\n",
    "            discriminated = Bidirectional(LSTM(\n",
    "                32, return_sequences=False))(discriminated)\n",
    "            discriminated = Dense(1, activation='sigmoid')(discriminated)\n",
    "\n",
    "        discriminator = Model(discriminator_inputs, discriminated,\n",
    "                              'discriminator')\n",
    "        return discriminator\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_discriminator, dataset,\n",
    "              img_frequency):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        losses = [[], []]\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(n_discriminator):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                discriminator_loss_real = self._discriminator.train_on_batch(\n",
    "                    batch_transactions, np.ones((half_batch, 1)))\n",
    "                discriminator_loss_fake = self._discriminator.train_on_batch(\n",
    "                    generated_transactions, np.zeros((half_batch, 1)))\n",
    "                discriminator_loss = 0.5 * np.add(discriminator_loss_real,\n",
    "                                                  discriminator_loss_fake)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, np.ones((batch_size, 1)))[0]\n",
    "\n",
    "            losses[0].append(generator_loss)\n",
    "            losses[1].append(discriminator_loss)\n",
    "\n",
    "            print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, discriminator_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if epoch % img_frequency == 0:\n",
    "                self._save_imgs(epoch)\n",
    "                self._save_losses(losses)\n",
    "\n",
    "    def _save_imgs(self, epoch):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(0, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('gan/%05d.png' % epoch)\n",
    "        plt.savefig('gan/last.png')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_losses(losses):\n",
    "        plt.plot(losses[0])\n",
    "        plt.plot(losses[1])\n",
    "        plt.legend(['generator', 'discriminator'])\n",
    "        plt.savefig('gan/losses.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = int(1e5)\n",
    "n_discriminator = 1\n",
    "n_generator = 5\n",
    "latent_dim = 10\n",
    "lr = 0.00005\n",
    "img_frequency = 100\n",
    "timesteps = timesteps\n",
    "generator_type = 'conv'\n",
    "discriminator_type = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/.local/lib/python3.5/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.781228] [G loss: 0.610225]\n",
      "1 [D loss: 0.705468] [G loss: 0.645783]\n",
      "2 [D loss: 0.754780] [G loss: 0.636253]\n",
      "3 [D loss: 0.680573] [G loss: 0.621176]\n",
      "4 [D loss: 0.727733] [G loss: 0.694229]\n",
      "5 [D loss: 0.682014] [G loss: 0.686220]\n",
      "6 [D loss: 0.688449] [G loss: 0.651591]\n",
      "7 [D loss: 0.590919] [G loss: 0.682804]\n",
      "8 [D loss: 0.696846] [G loss: 0.642654]\n",
      "9 [D loss: 0.673813] [G loss: 0.640989]\n",
      "10 [D loss: 0.676983] [G loss: 0.677521]\n",
      "11 [D loss: 0.676589] [G loss: 0.644284]\n",
      "12 [D loss: 0.678639] [G loss: 0.657079]\n",
      "13 [D loss: 0.697872] [G loss: 0.665781]\n",
      "14 [D loss: 0.642257] [G loss: 0.675854]\n",
      "15 [D loss: 0.779292] [G loss: 0.618375]\n",
      "16 [D loss: 0.595859] [G loss: 0.714812]\n",
      "17 [D loss: 0.672955] [G loss: 0.655860]\n",
      "18 [D loss: 0.588093] [G loss: 0.699263]\n",
      "19 [D loss: 0.642453] [G loss: 0.632001]\n",
      "20 [D loss: 0.601557] [G loss: 0.623174]\n",
      "21 [D loss: 0.674933] [G loss: 0.641624]\n",
      "22 [D loss: 0.587216] [G loss: 0.640304]\n",
      "23 [D loss: 0.674911] [G loss: 0.648670]\n",
      "24 [D loss: 0.664989] [G loss: 0.616810]\n",
      "25 [D loss: 0.615295] [G loss: 0.594662]\n",
      "26 [D loss: 0.660749] [G loss: 0.691074]\n",
      "27 [D loss: 0.663108] [G loss: 0.632897]\n",
      "28 [D loss: 0.679765] [G loss: 0.652457]\n",
      "29 [D loss: 0.624560] [G loss: 0.654973]\n",
      "30 [D loss: 0.716722] [G loss: 0.608164]\n",
      "31 [D loss: 0.591042] [G loss: 0.648393]\n",
      "32 [D loss: 0.685992] [G loss: 0.606016]\n",
      "33 [D loss: 0.624889] [G loss: 0.673643]\n",
      "34 [D loss: 0.645820] [G loss: 0.662011]\n",
      "35 [D loss: 0.582361] [G loss: 0.599959]\n",
      "36 [D loss: 0.601314] [G loss: 0.612054]\n",
      "37 [D loss: 0.577205] [G loss: 0.629193]\n",
      "38 [D loss: 0.649426] [G loss: 0.670237]\n",
      "39 [D loss: 0.642911] [G loss: 0.640368]\n",
      "40 [D loss: 0.619326] [G loss: 0.655070]\n",
      "41 [D loss: 0.656120] [G loss: 0.696694]\n",
      "42 [D loss: 0.558106] [G loss: 0.626746]\n",
      "43 [D loss: 0.672480] [G loss: 0.591941]\n",
      "44 [D loss: 0.534636] [G loss: 0.638020]\n",
      "45 [D loss: 0.631110] [G loss: 0.672477]\n",
      "46 [D loss: 0.615644] [G loss: 0.631752]\n",
      "47 [D loss: 0.599434] [G loss: 0.678665]\n",
      "48 [D loss: 0.569524] [G loss: 0.633586]\n",
      "49 [D loss: 0.598571] [G loss: 0.635417]\n",
      "50 [D loss: 0.635845] [G loss: 0.630368]\n",
      "51 [D loss: 0.542218] [G loss: 0.635744]\n",
      "52 [D loss: 0.607057] [G loss: 0.634550]\n",
      "53 [D loss: 0.643513] [G loss: 0.643656]\n",
      "54 [D loss: 0.636703] [G loss: 0.633715]\n",
      "55 [D loss: 0.639482] [G loss: 0.651195]\n",
      "56 [D loss: 0.618431] [G loss: 0.620645]\n",
      "57 [D loss: 0.576712] [G loss: 0.648926]\n",
      "58 [D loss: 0.558707] [G loss: 0.642618]\n",
      "59 [D loss: 0.593517] [G loss: 0.666257]\n",
      "60 [D loss: 0.609194] [G loss: 0.655767]\n",
      "61 [D loss: 0.515786] [G loss: 0.705659]\n",
      "62 [D loss: 0.570621] [G loss: 0.671908]\n",
      "63 [D loss: 0.625094] [G loss: 0.682787]\n",
      "64 [D loss: 0.642290] [G loss: 0.620384]\n",
      "65 [D loss: 0.650439] [G loss: 0.653523]\n",
      "66 [D loss: 0.519129] [G loss: 0.677579]\n",
      "67 [D loss: 0.596317] [G loss: 0.626905]\n",
      "68 [D loss: 0.595706] [G loss: 0.652715]\n",
      "69 [D loss: 0.616249] [G loss: 0.649406]\n",
      "70 [D loss: 0.567963] [G loss: 0.663524]\n",
      "71 [D loss: 0.591233] [G loss: 0.639300]\n",
      "72 [D loss: 0.574410] [G loss: 0.602558]\n",
      "73 [D loss: 0.494469] [G loss: 0.671624]\n",
      "74 [D loss: 0.629670] [G loss: 0.677233]\n",
      "75 [D loss: 0.557165] [G loss: 0.631725]\n",
      "76 [D loss: 0.597046] [G loss: 0.674339]\n",
      "77 [D loss: 0.555221] [G loss: 0.644352]\n",
      "78 [D loss: 0.565651] [G loss: 0.668047]\n",
      "79 [D loss: 0.589159] [G loss: 0.591093]\n",
      "80 [D loss: 0.586149] [G loss: 0.671309]\n",
      "81 [D loss: 0.490348] [G loss: 0.664816]\n",
      "82 [D loss: 0.554771] [G loss: 0.653729]\n",
      "83 [D loss: 0.578702] [G loss: 0.669595]\n",
      "84 [D loss: 0.523700] [G loss: 0.674415]\n",
      "85 [D loss: 0.609308] [G loss: 0.591712]\n",
      "86 [D loss: 0.571330] [G loss: 0.678601]\n",
      "87 [D loss: 0.525231] [G loss: 0.677043]\n",
      "88 [D loss: 0.489886] [G loss: 0.704410]\n",
      "89 [D loss: 0.523721] [G loss: 0.696889]\n",
      "90 [D loss: 0.515249] [G loss: 0.681290]\n",
      "91 [D loss: 0.538610] [G loss: 0.728329]\n",
      "92 [D loss: 0.531342] [G loss: 0.740097]\n",
      "93 [D loss: 0.530571] [G loss: 0.633647]\n",
      "94 [D loss: 0.552496] [G loss: 0.648790]\n",
      "95 [D loss: 0.522554] [G loss: 0.688779]\n",
      "96 [D loss: 0.505537] [G loss: 0.644225]\n",
      "97 [D loss: 0.578551] [G loss: 0.671638]\n",
      "98 [D loss: 0.534038] [G loss: 0.646570]\n",
      "99 [D loss: 0.558924] [G loss: 0.694754]\n",
      "100 [D loss: 0.504941] [G loss: 0.680904]\n",
      "101 [D loss: 0.568246] [G loss: 0.754614]\n",
      "102 [D loss: 0.484522] [G loss: 0.707214]\n",
      "103 [D loss: 0.531760] [G loss: 0.715838]\n",
      "104 [D loss: 0.487146] [G loss: 0.621076]\n",
      "105 [D loss: 0.431533] [G loss: 0.676581]\n",
      "106 [D loss: 0.509987] [G loss: 0.662124]\n",
      "107 [D loss: 0.543551] [G loss: 0.652637]\n",
      "108 [D loss: 0.465416] [G loss: 0.637573]\n",
      "109 [D loss: 0.532773] [G loss: 0.689292]\n",
      "110 [D loss: 0.513787] [G loss: 0.665818]\n",
      "111 [D loss: 0.525454] [G loss: 0.714085]\n",
      "112 [D loss: 0.456519] [G loss: 0.664404]\n",
      "113 [D loss: 0.530404] [G loss: 0.670883]\n",
      "114 [D loss: 0.581375] [G loss: 0.721000]\n",
      "115 [D loss: 0.517447] [G loss: 0.714614]\n",
      "116 [D loss: 0.457219] [G loss: 0.692932]\n",
      "117 [D loss: 0.525012] [G loss: 0.730464]\n",
      "118 [D loss: 0.558605] [G loss: 0.761587]\n",
      "119 [D loss: 0.503258] [G loss: 0.684857]\n",
      "120 [D loss: 0.566870] [G loss: 0.692855]\n",
      "121 [D loss: 0.535134] [G loss: 0.696692]\n",
      "122 [D loss: 0.491870] [G loss: 0.674625]\n",
      "123 [D loss: 0.469638] [G loss: 0.768499]\n",
      "124 [D loss: 0.555493] [G loss: 0.673062]\n",
      "125 [D loss: 0.449159] [G loss: 0.732915]\n",
      "126 [D loss: 0.427890] [G loss: 0.679984]\n",
      "127 [D loss: 0.530021] [G loss: 0.719205]\n",
      "128 [D loss: 0.494525] [G loss: 0.659325]\n",
      "129 [D loss: 0.483872] [G loss: 0.719875]\n",
      "130 [D loss: 0.517664] [G loss: 0.724237]\n",
      "131 [D loss: 0.482372] [G loss: 0.673308]\n",
      "132 [D loss: 0.444828] [G loss: 0.704856]\n",
      "133 [D loss: 0.514330] [G loss: 0.691909]\n",
      "134 [D loss: 0.504492] [G loss: 0.721945]\n",
      "135 [D loss: 0.454361] [G loss: 0.691459]\n",
      "136 [D loss: 0.513554] [G loss: 0.744665]\n",
      "137 [D loss: 0.443324] [G loss: 0.710339]\n",
      "138 [D loss: 0.474804] [G loss: 0.734332]\n",
      "139 [D loss: 0.480218] [G loss: 0.687040]\n",
      "140 [D loss: 0.510555] [G loss: 0.721936]\n",
      "141 [D loss: 0.462387] [G loss: 0.735152]\n",
      "142 [D loss: 0.401209] [G loss: 0.817680]\n",
      "143 [D loss: 0.463629] [G loss: 0.753239]\n",
      "144 [D loss: 0.449000] [G loss: 0.774719]\n",
      "145 [D loss: 0.439615] [G loss: 0.683654]\n",
      "146 [D loss: 0.474996] [G loss: 0.743471]\n",
      "147 [D loss: 0.462613] [G loss: 0.767273]\n",
      "148 [D loss: 0.482872] [G loss: 0.777588]\n",
      "149 [D loss: 0.492735] [G loss: 0.754902]\n",
      "150 [D loss: 0.479950] [G loss: 0.741004]\n",
      "151 [D loss: 0.411911] [G loss: 0.766273]\n",
      "152 [D loss: 0.430435] [G loss: 0.783234]\n",
      "153 [D loss: 0.431679] [G loss: 0.759262]\n",
      "154 [D loss: 0.418129] [G loss: 0.784599]\n",
      "155 [D loss: 0.449364] [G loss: 0.742016]\n",
      "156 [D loss: 0.425681] [G loss: 0.811080]\n",
      "157 [D loss: 0.442449] [G loss: 0.806068]\n",
      "158 [D loss: 0.425473] [G loss: 0.770661]\n",
      "159 [D loss: 0.404678] [G loss: 0.751964]\n",
      "160 [D loss: 0.409349] [G loss: 0.802549]\n",
      "161 [D loss: 0.429829] [G loss: 0.724903]\n",
      "162 [D loss: 0.422955] [G loss: 0.768519]\n",
      "163 [D loss: 0.427760] [G loss: 0.813205]\n",
      "164 [D loss: 0.387490] [G loss: 0.775891]\n",
      "165 [D loss: 0.387606] [G loss: 0.800814]\n",
      "166 [D loss: 0.417132] [G loss: 0.779527]\n",
      "167 [D loss: 0.432646] [G loss: 0.821352]\n",
      "168 [D loss: 0.450116] [G loss: 0.852097]\n",
      "169 [D loss: 0.445641] [G loss: 0.816946]\n",
      "170 [D loss: 0.406622] [G loss: 0.796296]\n",
      "171 [D loss: 0.484238] [G loss: 0.858413]\n",
      "172 [D loss: 0.413791] [G loss: 0.880545]\n",
      "173 [D loss: 0.425518] [G loss: 0.788168]\n",
      "174 [D loss: 0.448950] [G loss: 0.789197]\n",
      "175 [D loss: 0.382852] [G loss: 0.789258]\n",
      "176 [D loss: 0.405909] [G loss: 0.844165]\n",
      "177 [D loss: 0.401362] [G loss: 0.780940]\n",
      "178 [D loss: 0.404880] [G loss: 0.887619]\n",
      "179 [D loss: 0.477630] [G loss: 0.810101]\n",
      "180 [D loss: 0.397023] [G loss: 0.863017]\n",
      "181 [D loss: 0.427208] [G loss: 0.808671]\n",
      "182 [D loss: 0.414788] [G loss: 0.873244]\n",
      "183 [D loss: 0.495749] [G loss: 0.796050]\n",
      "184 [D loss: 0.429372] [G loss: 0.895582]\n",
      "185 [D loss: 0.419779] [G loss: 0.808677]\n",
      "186 [D loss: 0.379213] [G loss: 0.906088]\n",
      "187 [D loss: 0.425351] [G loss: 0.878488]\n",
      "188 [D loss: 0.429324] [G loss: 0.786872]\n",
      "189 [D loss: 0.370578] [G loss: 0.856639]\n",
      "190 [D loss: 0.418107] [G loss: 0.851533]\n",
      "191 [D loss: 0.404662] [G loss: 0.844472]\n",
      "192 [D loss: 0.368930] [G loss: 0.849915]\n",
      "193 [D loss: 0.379210] [G loss: 0.876088]\n",
      "194 [D loss: 0.397058] [G loss: 0.908914]\n",
      "195 [D loss: 0.447472] [G loss: 0.868617]\n",
      "196 [D loss: 0.346661] [G loss: 0.858923]\n",
      "197 [D loss: 0.346573] [G loss: 0.882201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 [D loss: 0.415226] [G loss: 0.833044]\n",
      "199 [D loss: 0.453705] [G loss: 0.924012]\n",
      "200 [D loss: 0.335197] [G loss: 0.819304]\n",
      "201 [D loss: 0.344700] [G loss: 0.892154]\n",
      "202 [D loss: 0.334815] [G loss: 0.861370]\n",
      "203 [D loss: 0.348250] [G loss: 0.930943]\n",
      "204 [D loss: 0.348828] [G loss: 0.873121]\n",
      "205 [D loss: 0.352869] [G loss: 0.873692]\n",
      "206 [D loss: 0.340974] [G loss: 0.958266]\n",
      "207 [D loss: 0.350065] [G loss: 0.940705]\n",
      "208 [D loss: 0.375177] [G loss: 0.863186]\n",
      "209 [D loss: 0.333482] [G loss: 0.978227]\n",
      "210 [D loss: 0.357901] [G loss: 0.909515]\n",
      "211 [D loss: 0.410041] [G loss: 0.887642]\n",
      "212 [D loss: 0.312153] [G loss: 0.899383]\n",
      "213 [D loss: 0.311687] [G loss: 0.905995]\n",
      "214 [D loss: 0.322605] [G loss: 1.015714]\n",
      "215 [D loss: 0.376870] [G loss: 0.911102]\n",
      "216 [D loss: 0.408094] [G loss: 0.956954]\n",
      "217 [D loss: 0.404608] [G loss: 0.951929]\n",
      "218 [D loss: 0.377350] [G loss: 0.886644]\n",
      "219 [D loss: 0.401885] [G loss: 0.891377]\n",
      "220 [D loss: 0.344300] [G loss: 0.953464]\n",
      "221 [D loss: 0.304903] [G loss: 0.942737]\n",
      "222 [D loss: 0.320020] [G loss: 1.003053]\n",
      "223 [D loss: 0.340984] [G loss: 0.943992]\n",
      "224 [D loss: 0.309175] [G loss: 0.975582]\n",
      "225 [D loss: 0.310788] [G loss: 0.937922]\n",
      "226 [D loss: 0.271445] [G loss: 0.968948]\n",
      "227 [D loss: 0.321562] [G loss: 1.013735]\n",
      "228 [D loss: 0.285761] [G loss: 1.032120]\n",
      "229 [D loss: 0.280349] [G loss: 0.969727]\n",
      "230 [D loss: 0.299561] [G loss: 1.019116]\n",
      "231 [D loss: 0.312859] [G loss: 1.040004]\n",
      "232 [D loss: 0.293515] [G loss: 1.013513]\n",
      "233 [D loss: 0.327060] [G loss: 1.026006]\n",
      "234 [D loss: 0.316136] [G loss: 1.056634]\n",
      "235 [D loss: 0.317840] [G loss: 1.007828]\n",
      "236 [D loss: 0.297909] [G loss: 1.003898]\n",
      "237 [D loss: 0.301714] [G loss: 1.023639]\n",
      "238 [D loss: 0.297801] [G loss: 1.022823]\n",
      "239 [D loss: 0.292742] [G loss: 1.021334]\n",
      "240 [D loss: 0.272728] [G loss: 1.050029]\n",
      "241 [D loss: 0.324588] [G loss: 1.042069]\n",
      "242 [D loss: 0.374821] [G loss: 1.017255]\n",
      "243 [D loss: 0.350254] [G loss: 1.042073]\n",
      "244 [D loss: 0.262984] [G loss: 1.031508]\n",
      "245 [D loss: 0.303852] [G loss: 1.024096]\n",
      "246 [D loss: 0.294935] [G loss: 1.022500]\n",
      "247 [D loss: 0.284739] [G loss: 1.129929]\n",
      "248 [D loss: 0.314160] [G loss: 1.123047]\n",
      "249 [D loss: 0.288063] [G loss: 1.093269]\n",
      "250 [D loss: 0.284482] [G loss: 1.058835]\n",
      "251 [D loss: 0.318956] [G loss: 1.079883]\n",
      "252 [D loss: 0.318387] [G loss: 1.126124]\n",
      "253 [D loss: 0.308315] [G loss: 1.084131]\n",
      "254 [D loss: 0.288514] [G loss: 1.190712]\n",
      "255 [D loss: 0.285703] [G loss: 1.136898]\n",
      "256 [D loss: 0.300260] [G loss: 1.119410]\n",
      "257 [D loss: 0.337581] [G loss: 1.169645]\n",
      "258 [D loss: 0.245962] [G loss: 1.036765]\n",
      "259 [D loss: 0.272630] [G loss: 1.129195]\n",
      "260 [D loss: 0.337677] [G loss: 1.148560]\n",
      "261 [D loss: 0.291902] [G loss: 1.130480]\n",
      "262 [D loss: 0.300321] [G loss: 1.123968]\n",
      "263 [D loss: 0.266297] [G loss: 1.149585]\n",
      "264 [D loss: 0.254196] [G loss: 1.070860]\n",
      "265 [D loss: 0.456859] [G loss: 1.107638]\n",
      "266 [D loss: 0.277590] [G loss: 1.109812]\n",
      "267 [D loss: 0.273933] [G loss: 1.127365]\n",
      "268 [D loss: 0.242254] [G loss: 1.085644]\n",
      "269 [D loss: 0.283659] [G loss: 1.139192]\n",
      "270 [D loss: 0.262807] [G loss: 1.095076]\n",
      "271 [D loss: 0.262164] [G loss: 1.112548]\n",
      "272 [D loss: 0.278845] [G loss: 1.035817]\n",
      "273 [D loss: 0.270726] [G loss: 1.186547]\n",
      "274 [D loss: 0.290757] [G loss: 1.226580]\n",
      "275 [D loss: 0.239994] [G loss: 1.115965]\n",
      "276 [D loss: 0.305278] [G loss: 1.163340]\n",
      "277 [D loss: 0.261245] [G loss: 1.262206]\n",
      "278 [D loss: 0.267563] [G loss: 1.151171]\n",
      "279 [D loss: 0.265117] [G loss: 1.139420]\n",
      "280 [D loss: 0.244934] [G loss: 1.209326]\n",
      "281 [D loss: 0.261473] [G loss: 1.183837]\n",
      "282 [D loss: 0.324878] [G loss: 1.135712]\n",
      "283 [D loss: 0.252624] [G loss: 1.184937]\n",
      "284 [D loss: 0.280020] [G loss: 1.146986]\n",
      "285 [D loss: 0.257452] [G loss: 1.131944]\n",
      "286 [D loss: 0.232037] [G loss: 1.138814]\n",
      "287 [D loss: 0.250891] [G loss: 1.307859]\n",
      "288 [D loss: 0.250237] [G loss: 1.242671]\n",
      "289 [D loss: 0.298503] [G loss: 1.202305]\n",
      "290 [D loss: 0.241627] [G loss: 1.277374]\n",
      "291 [D loss: 0.228354] [G loss: 1.214740]\n",
      "292 [D loss: 0.237134] [G loss: 1.168823]\n",
      "293 [D loss: 0.297085] [G loss: 1.253509]\n",
      "294 [D loss: 0.249515] [G loss: 1.224018]\n",
      "295 [D loss: 0.227499] [G loss: 1.279722]\n",
      "296 [D loss: 0.228548] [G loss: 1.287355]\n",
      "297 [D loss: 0.286866] [G loss: 1.250184]\n",
      "298 [D loss: 0.255682] [G loss: 1.263599]\n",
      "299 [D loss: 0.233271] [G loss: 1.256751]\n",
      "300 [D loss: 0.263851] [G loss: 1.272820]\n",
      "301 [D loss: 0.233648] [G loss: 1.308919]\n",
      "302 [D loss: 0.230851] [G loss: 1.258242]\n",
      "303 [D loss: 0.264285] [G loss: 1.254078]\n",
      "304 [D loss: 0.221073] [G loss: 1.249671]\n",
      "305 [D loss: 0.247497] [G loss: 1.299428]\n",
      "306 [D loss: 0.180541] [G loss: 1.327511]\n",
      "307 [D loss: 0.267551] [G loss: 1.346326]\n",
      "308 [D loss: 0.230437] [G loss: 1.358447]\n",
      "309 [D loss: 0.229033] [G loss: 1.239579]\n",
      "310 [D loss: 0.234108] [G loss: 1.316374]\n",
      "311 [D loss: 0.249112] [G loss: 1.340178]\n",
      "312 [D loss: 0.245134] [G loss: 1.393559]\n",
      "313 [D loss: 0.221928] [G loss: 1.377977]\n",
      "314 [D loss: 0.213867] [G loss: 1.301703]\n",
      "315 [D loss: 0.233727] [G loss: 1.253419]\n",
      "316 [D loss: 0.269107] [G loss: 1.401421]\n",
      "317 [D loss: 0.234276] [G loss: 1.229429]\n",
      "318 [D loss: 0.201889] [G loss: 1.396134]\n",
      "319 [D loss: 0.194886] [G loss: 1.452446]\n",
      "320 [D loss: 0.201205] [G loss: 1.383528]\n",
      "321 [D loss: 0.192433] [G loss: 1.384238]\n",
      "322 [D loss: 0.197165] [G loss: 1.445081]\n",
      "323 [D loss: 0.224306] [G loss: 1.410865]\n",
      "324 [D loss: 0.225292] [G loss: 1.384822]\n",
      "325 [D loss: 0.231747] [G loss: 1.370107]\n",
      "326 [D loss: 0.217242] [G loss: 1.415304]\n",
      "327 [D loss: 0.235303] [G loss: 1.409982]\n",
      "328 [D loss: 0.205776] [G loss: 1.412830]\n",
      "329 [D loss: 0.243723] [G loss: 1.394467]\n",
      "330 [D loss: 0.211409] [G loss: 1.464632]\n",
      "331 [D loss: 0.185594] [G loss: 1.479795]\n",
      "332 [D loss: 0.211157] [G loss: 1.428471]\n",
      "333 [D loss: 0.214097] [G loss: 1.428000]\n",
      "334 [D loss: 0.191459] [G loss: 1.437649]\n",
      "335 [D loss: 0.192379] [G loss: 1.485548]\n",
      "336 [D loss: 0.224371] [G loss: 1.421624]\n",
      "337 [D loss: 0.200610] [G loss: 1.412030]\n",
      "338 [D loss: 0.217395] [G loss: 1.538000]\n",
      "339 [D loss: 0.187163] [G loss: 1.475102]\n",
      "340 [D loss: 0.187617] [G loss: 1.455684]\n",
      "341 [D loss: 0.216656] [G loss: 1.464582]\n",
      "342 [D loss: 0.206033] [G loss: 1.408576]\n",
      "343 [D loss: 0.198921] [G loss: 1.547317]\n",
      "344 [D loss: 0.163372] [G loss: 1.400179]\n",
      "345 [D loss: 0.174620] [G loss: 1.541482]\n",
      "346 [D loss: 0.182660] [G loss: 1.571962]\n",
      "347 [D loss: 0.179537] [G loss: 1.474169]\n",
      "348 [D loss: 0.195374] [G loss: 1.535130]\n",
      "349 [D loss: 0.187427] [G loss: 1.437407]\n",
      "350 [D loss: 0.198390] [G loss: 1.517459]\n",
      "351 [D loss: 0.163771] [G loss: 1.483177]\n",
      "352 [D loss: 0.195466] [G loss: 1.582090]\n",
      "353 [D loss: 0.187441] [G loss: 1.495385]\n",
      "354 [D loss: 0.205305] [G loss: 1.397869]\n",
      "355 [D loss: 0.221566] [G loss: 1.474814]\n",
      "356 [D loss: 0.203666] [G loss: 1.475786]\n",
      "357 [D loss: 0.177893] [G loss: 1.495522]\n",
      "358 [D loss: 0.180811] [G loss: 1.462528]\n",
      "359 [D loss: 0.188444] [G loss: 1.534597]\n",
      "360 [D loss: 0.176159] [G loss: 1.551098]\n",
      "361 [D loss: 0.231143] [G loss: 1.530586]\n",
      "362 [D loss: 0.166177] [G loss: 1.666045]\n",
      "363 [D loss: 0.198334] [G loss: 1.622521]\n",
      "364 [D loss: 0.164837] [G loss: 1.610262]\n",
      "365 [D loss: 0.176398] [G loss: 1.499727]\n",
      "366 [D loss: 0.183656] [G loss: 1.534172]\n",
      "367 [D loss: 0.178170] [G loss: 1.489057]\n",
      "368 [D loss: 0.199864] [G loss: 1.695792]\n",
      "369 [D loss: 0.187274] [G loss: 1.623576]\n",
      "370 [D loss: 0.164546] [G loss: 1.589292]\n",
      "371 [D loss: 0.166594] [G loss: 1.596095]\n",
      "372 [D loss: 0.228749] [G loss: 1.705282]\n",
      "373 [D loss: 0.195079] [G loss: 1.720727]\n",
      "374 [D loss: 0.159089] [G loss: 1.614812]\n",
      "375 [D loss: 0.169261] [G loss: 1.642765]\n",
      "376 [D loss: 0.186033] [G loss: 1.552470]\n",
      "377 [D loss: 0.192210] [G loss: 1.694203]\n",
      "378 [D loss: 0.186460] [G loss: 1.642269]\n",
      "379 [D loss: 0.176487] [G loss: 1.724053]\n",
      "380 [D loss: 0.233753] [G loss: 1.660347]\n",
      "381 [D loss: 0.145620] [G loss: 1.652429]\n",
      "382 [D loss: 0.154670] [G loss: 1.632441]\n",
      "383 [D loss: 0.139111] [G loss: 1.683173]\n",
      "384 [D loss: 0.137426] [G loss: 1.750218]\n",
      "385 [D loss: 0.174431] [G loss: 1.671793]\n",
      "386 [D loss: 0.156282] [G loss: 1.667301]\n",
      "387 [D loss: 0.147996] [G loss: 1.704875]\n",
      "388 [D loss: 0.126146] [G loss: 1.787188]\n",
      "389 [D loss: 0.184935] [G loss: 1.732503]\n",
      "390 [D loss: 0.148296] [G loss: 1.744961]\n",
      "391 [D loss: 0.153470] [G loss: 1.737315]\n",
      "392 [D loss: 0.141434] [G loss: 1.697997]\n",
      "393 [D loss: 0.144619] [G loss: 1.758660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.153214] [G loss: 1.872989]\n",
      "395 [D loss: 0.145466] [G loss: 1.814703]\n",
      "396 [D loss: 0.149923] [G loss: 1.726815]\n",
      "397 [D loss: 0.147406] [G loss: 1.741858]\n",
      "398 [D loss: 0.135158] [G loss: 1.709247]\n",
      "399 [D loss: 0.171099] [G loss: 1.839386]\n",
      "400 [D loss: 0.158703] [G loss: 1.694318]\n",
      "401 [D loss: 0.133735] [G loss: 1.913823]\n",
      "402 [D loss: 0.137716] [G loss: 1.734319]\n",
      "403 [D loss: 0.147918] [G loss: 1.856893]\n",
      "404 [D loss: 0.139814] [G loss: 1.904143]\n",
      "405 [D loss: 0.144415] [G loss: 1.895223]\n",
      "406 [D loss: 0.150034] [G loss: 1.760907]\n",
      "407 [D loss: 0.257670] [G loss: 1.821511]\n",
      "408 [D loss: 0.131784] [G loss: 1.939548]\n",
      "409 [D loss: 0.150273] [G loss: 1.720747]\n",
      "410 [D loss: 0.108371] [G loss: 1.806991]\n",
      "411 [D loss: 0.184211] [G loss: 1.878728]\n",
      "412 [D loss: 0.099380] [G loss: 1.850675]\n",
      "413 [D loss: 0.158137] [G loss: 1.846521]\n",
      "414 [D loss: 0.161967] [G loss: 1.902648]\n",
      "415 [D loss: 0.172744] [G loss: 1.917065]\n",
      "416 [D loss: 0.120145] [G loss: 1.872469]\n",
      "417 [D loss: 0.123637] [G loss: 1.833900]\n",
      "418 [D loss: 0.134781] [G loss: 1.974105]\n",
      "419 [D loss: 0.131095] [G loss: 1.863208]\n",
      "420 [D loss: 0.140305] [G loss: 1.828139]\n",
      "421 [D loss: 0.128815] [G loss: 1.949911]\n",
      "422 [D loss: 0.133681] [G loss: 1.958716]\n",
      "423 [D loss: 0.118968] [G loss: 1.899051]\n",
      "424 [D loss: 0.122509] [G loss: 1.954845]\n",
      "425 [D loss: 0.140449] [G loss: 1.939534]\n",
      "426 [D loss: 0.109588] [G loss: 2.001822]\n",
      "427 [D loss: 0.116234] [G loss: 2.001561]\n",
      "428 [D loss: 0.147030] [G loss: 1.952225]\n",
      "429 [D loss: 0.109573] [G loss: 1.983740]\n",
      "430 [D loss: 0.226085] [G loss: 1.962469]\n",
      "431 [D loss: 0.190976] [G loss: 1.894523]\n",
      "432 [D loss: 0.107264] [G loss: 1.983535]\n",
      "433 [D loss: 0.102569] [G loss: 1.960741]\n",
      "434 [D loss: 0.124661] [G loss: 2.084280]\n",
      "435 [D loss: 0.106369] [G loss: 2.046703]\n",
      "436 [D loss: 0.102045] [G loss: 2.002071]\n",
      "437 [D loss: 0.113111] [G loss: 2.078186]\n",
      "438 [D loss: 0.116575] [G loss: 2.105925]\n",
      "439 [D loss: 0.107344] [G loss: 1.971054]\n",
      "440 [D loss: 0.123453] [G loss: 1.987742]\n",
      "441 [D loss: 0.124530] [G loss: 2.032563]\n",
      "442 [D loss: 0.111247] [G loss: 2.016760]\n",
      "443 [D loss: 0.095464] [G loss: 2.193218]\n",
      "444 [D loss: 0.108288] [G loss: 2.035411]\n",
      "445 [D loss: 0.096384] [G loss: 2.124450]\n",
      "446 [D loss: 0.111550] [G loss: 1.960110]\n",
      "447 [D loss: 0.113389] [G loss: 2.110336]\n",
      "448 [D loss: 0.086452] [G loss: 2.098086]\n",
      "449 [D loss: 0.152329] [G loss: 2.132595]\n",
      "450 [D loss: 0.089640] [G loss: 2.101859]\n",
      "451 [D loss: 0.118286] [G loss: 2.070892]\n",
      "452 [D loss: 0.114995] [G loss: 2.117864]\n",
      "453 [D loss: 0.101426] [G loss: 2.116506]\n",
      "454 [D loss: 0.092642] [G loss: 2.112866]\n",
      "455 [D loss: 0.096351] [G loss: 2.144655]\n",
      "456 [D loss: 0.090777] [G loss: 2.151990]\n",
      "457 [D loss: 0.130182] [G loss: 2.243086]\n",
      "458 [D loss: 0.092064] [G loss: 2.196468]\n",
      "459 [D loss: 0.122102] [G loss: 2.176915]\n",
      "460 [D loss: 0.088423] [G loss: 2.125722]\n",
      "461 [D loss: 0.084257] [G loss: 2.125213]\n",
      "462 [D loss: 0.118737] [G loss: 2.265780]\n",
      "463 [D loss: 0.116554] [G loss: 2.214520]\n",
      "464 [D loss: 0.098235] [G loss: 2.104963]\n",
      "465 [D loss: 0.119355] [G loss: 2.309205]\n",
      "466 [D loss: 0.152747] [G loss: 2.176599]\n",
      "467 [D loss: 0.119322] [G loss: 2.193251]\n",
      "468 [D loss: 0.132888] [G loss: 2.352774]\n",
      "469 [D loss: 0.090167] [G loss: 2.160815]\n",
      "470 [D loss: 0.086805] [G loss: 2.269221]\n",
      "471 [D loss: 0.113624] [G loss: 2.311531]\n",
      "472 [D loss: 0.090776] [G loss: 2.131110]\n",
      "473 [D loss: 0.109613] [G loss: 2.245228]\n",
      "474 [D loss: 0.093825] [G loss: 2.272313]\n",
      "475 [D loss: 0.114996] [G loss: 2.279089]\n",
      "476 [D loss: 0.084817] [G loss: 2.255579]\n",
      "477 [D loss: 0.081011] [G loss: 2.317792]\n",
      "478 [D loss: 0.082792] [G loss: 2.282711]\n",
      "479 [D loss: 0.073136] [G loss: 2.351851]\n",
      "480 [D loss: 0.101337] [G loss: 2.369227]\n",
      "481 [D loss: 0.082213] [G loss: 2.329371]\n",
      "482 [D loss: 0.096683] [G loss: 2.354103]\n",
      "483 [D loss: 0.095905] [G loss: 2.451210]\n",
      "484 [D loss: 0.077526] [G loss: 2.386234]\n",
      "485 [D loss: 0.062346] [G loss: 2.380753]\n",
      "486 [D loss: 0.073827] [G loss: 2.366455]\n",
      "487 [D loss: 0.110350] [G loss: 2.378702]\n",
      "488 [D loss: 0.136825] [G loss: 2.463581]\n",
      "489 [D loss: 0.075612] [G loss: 2.365171]\n",
      "490 [D loss: 0.094095] [G loss: 2.453157]\n",
      "491 [D loss: 0.090261] [G loss: 2.418014]\n",
      "492 [D loss: 0.079513] [G loss: 2.306701]\n",
      "493 [D loss: 0.071256] [G loss: 2.382317]\n",
      "494 [D loss: 0.087803] [G loss: 2.409358]\n",
      "495 [D loss: 0.073610] [G loss: 2.505879]\n",
      "496 [D loss: 0.082470] [G loss: 2.347911]\n",
      "497 [D loss: 0.078956] [G loss: 2.479532]\n",
      "498 [D loss: 0.079877] [G loss: 2.487658]\n",
      "499 [D loss: 0.100068] [G loss: 2.439476]\n",
      "500 [D loss: 0.062011] [G loss: 2.489454]\n",
      "501 [D loss: 0.063692] [G loss: 2.468234]\n",
      "502 [D loss: 0.071999] [G loss: 2.323436]\n",
      "503 [D loss: 0.070834] [G loss: 2.456020]\n",
      "504 [D loss: 0.094837] [G loss: 2.464585]\n",
      "505 [D loss: 0.107920] [G loss: 2.466987]\n",
      "506 [D loss: 0.075833] [G loss: 2.532813]\n",
      "507 [D loss: 0.079753] [G loss: 2.335495]\n",
      "508 [D loss: 0.076333] [G loss: 2.556780]\n",
      "509 [D loss: 0.061294] [G loss: 2.554384]\n",
      "510 [D loss: 0.061408] [G loss: 2.510509]\n",
      "511 [D loss: 0.126129] [G loss: 2.572920]\n",
      "512 [D loss: 0.071151] [G loss: 2.601502]\n",
      "513 [D loss: 0.100329] [G loss: 2.598746]\n",
      "514 [D loss: 0.077310] [G loss: 2.618552]\n",
      "515 [D loss: 0.063593] [G loss: 2.583854]\n",
      "516 [D loss: 0.064165] [G loss: 2.506496]\n",
      "517 [D loss: 0.087566] [G loss: 2.599610]\n",
      "518 [D loss: 0.063470] [G loss: 2.747826]\n",
      "519 [D loss: 0.063446] [G loss: 2.646428]\n",
      "520 [D loss: 0.070464] [G loss: 2.670871]\n",
      "521 [D loss: 0.069770] [G loss: 2.531822]\n",
      "522 [D loss: 0.108436] [G loss: 2.637196]\n",
      "523 [D loss: 0.083676] [G loss: 2.525871]\n",
      "524 [D loss: 0.073870] [G loss: 2.593359]\n",
      "525 [D loss: 0.067104] [G loss: 2.640577]\n",
      "526 [D loss: 0.054468] [G loss: 2.599978]\n",
      "527 [D loss: 0.063390] [G loss: 2.659846]\n",
      "528 [D loss: 0.085023] [G loss: 2.690258]\n",
      "529 [D loss: 0.049483] [G loss: 2.725128]\n",
      "530 [D loss: 0.086437] [G loss: 2.643561]\n",
      "531 [D loss: 0.070645] [G loss: 2.624709]\n",
      "532 [D loss: 0.064327] [G loss: 2.801354]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-772e2605feaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_discriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b1f8ba2c56be>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, n_generator, n_discriminator, dataset, img_frequency)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 generator_loss = self._gan.train_on_batch(\n\u001b[0;32m--> 145\u001b[0;31m                     noise, np.ones((batch_size, 1)))[0]\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists('gan'):\n",
    "    shutil.rmtree('gan')\n",
    "os.makedirs('gan')\n",
    "\n",
    "gan = GAN(timesteps, latent_dim, generator_type, discriminator_type)\n",
    "gan.build_model(lr)\n",
    "gan.train(batch_size, epochs, n_generator, n_discriminator, transactions, img_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
