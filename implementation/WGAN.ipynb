{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data\n",
    "    \n",
    "class MinibatchDiscrimination(Layer):\n",
    "    \"\"\"Concatenates to each sample information about how different the input\n",
    "    features for that sample are from features of other samples in the same\n",
    "    minibatch, as described in Salimans et. al. (2016). Useful for preventing\n",
    "    GANs from collapsing to a single output. When using this layer, generated\n",
    "    samples and reference samples should be in separate batches.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_dim, init='glorot_uniform', weights=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "\n",
    "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
    "            initializer=self.init,\n",
    "            name='kernel',\n",
    "            regularizer=self.W_regularizer,\n",
    "            trainable=True,\n",
    "            constraint=self.W_constraint)\n",
    "\n",
    "        # Set built to true.\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
    "        diffs = K.expand_dims(activation, 3) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
    "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
    "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
    "#         return K.concatenate([x, minibatch_features], 1)\n",
    "        return minibatch_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "#         return input_shape[0], input_shape[1]+self.nb_kernels\n",
    "        return input_shape[0], self.nb_kernels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_kernels': self.nb_kernels,\n",
    "                  'kernel_dim': self.kernel_dim,\n",
    "#                   'init': self.init.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     38,
     78,
     138,
     159,
     180,
     192,
     198,
     211,
     216,
     222,
     226,
     229,
     237
    ]
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self, timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._run_dir = run_dir\n",
    "        self._img_dir = img_dir\n",
    "        self._model_dir = model_dir\n",
    "        self._generated_datesets_dir = generated_datesets_dir\n",
    "        \n",
    "        self._save_config()\n",
    "        \n",
    "        self._epoch = 0\n",
    "        self._losses = [[], []]\n",
    "\n",
    "    def build_models(self, generator_lr, critic_lr):        \n",
    "        self._generator = self._build_generator(self._latent_dim, self._timesteps)\n",
    "\n",
    "        self._critic = self._build_critic(self._timesteps)\n",
    "        self._critic.compile(loss=self._wasserstein_loss, optimizer=RMSprop(critic_lr))\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        for layer in self._critic.layers:\n",
    "            layer.trainable = False\n",
    "        self._critic.trainable = False\n",
    "\n",
    "        valid = self._critic(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss=self._wasserstein_loss,\n",
    "            optimizer=RMSprop(generator_lr),\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "\n",
    "    def _build_generator(self, noise_dim, timesteps):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "        generated = generator_inputs\n",
    "        \n",
    "        generated = Lambda(lambda x: K.expand_dims(x))(generated)\n",
    "        while generated.shape[1] < timesteps:\n",
    "            generated = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(generated)\n",
    "            generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(\n",
    "            1, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "        generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _build_critic(self, timesteps):\n",
    "        critic_inputs = Input((timesteps, ))\n",
    "        criticized = critic_inputs\n",
    "        \n",
    "        mbd = MinibatchDiscrimination(5, 3)(criticized)\n",
    "        mbd = Dense(1, activation='relu')(mbd)\n",
    "        \n",
    "        criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "            criticized)\n",
    "        while criticized.shape[1] > 1:\n",
    "            criticized = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(criticized)\n",
    "            criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        criticized = Flatten()(criticized)\n",
    "        criticized = Dense(32, activation='relu')(criticized)\n",
    "        criticized = Dense(15, activation='relu')(criticized)\n",
    "        criticized = Dense(1, activation='relu')(criticized) \n",
    "        criticized = Concatenate()([criticized, mbd])\n",
    "        criticized = Dense(1)(criticized) \n",
    "\n",
    "        critic = Model(critic_inputs, criticized, 'critic')\n",
    "        return critic\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        \n",
    "        while self._epoch < epochs:\n",
    "            self._epoch += 1\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                critic_loss_real = self._critic.train_on_batch(\n",
    "                    batch_transactions, -np.ones((half_batch, 1)))\n",
    "                critic_loss_fake = self._critic.train_on_batch(\n",
    "                    generated_transactions, np.ones((half_batch, 1)))\n",
    "                critic_loss = 0.5 * np.add(critic_loss_real,\n",
    "                                                  critic_loss_fake)\n",
    "\n",
    "                self._clip_weights(clip_value)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, -np.ones((batch_size, 1)))[0]\n",
    "            \n",
    "            generator_loss = 1 - generator_loss\n",
    "            critic_loss = 1 - critic_loss\n",
    "            \n",
    "            self._losses[0].append(generator_loss)\n",
    "            self._losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [C loss: %f] [G loss: %f]\" % (self._epoch, critic_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if self._epoch % img_frequency == 0:\n",
    "                self._save_imgs()\n",
    "                self._save_latent_space()\n",
    "            \n",
    "            if self._epoch % model_save_frequency == 0:\n",
    "                self._save_models()\n",
    "                \n",
    "            if self._epoch % dataset_generation_frequency == 0:\n",
    "                self._generate_dataset(self._epoch, dataset_generation_size)\n",
    "                \n",
    "            if self._epoch % 250 == 0:\n",
    "                self._save_losses()\n",
    "          \n",
    "        self._generate_dataset(epochs, dataset_generation_size)\n",
    "        self._save_losses(self._losses)\n",
    "        self._save_models()\n",
    "        self._save_imgs()\n",
    "        self._save_latent_space()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _save_imgs(self):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('%05d.png' % self._epoch)))\n",
    "        plt.savefig(str(self._img_dir / 'last.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_latent_space(self):\n",
    "        if self._latent_dim > 2:\n",
    "            latent_vector = np.random.normal(0, 1, latent_dim)\n",
    "        plt.subplots(5, 5, figsize=(15, 5))\n",
    "\n",
    "        for i, v_i in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "            for j, v_j in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                if self._latent_dim > 2:\n",
    "                    latent_vector[-2:] = [v_i, v_j]\n",
    "                else:\n",
    "                    latent_vector = np.array([v_i, v_j])\n",
    "                    \n",
    "                plt.subplot(5, 5, i*5+j+1)\n",
    "                plt.plot(self._generator.predict(latent_vector.reshape((1, self._latent_dim))).T)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('latent_space.png')))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    def _save_losses(self):\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(self._losses[0])\n",
    "        plt.plot(self._losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig(str(self._img_dir / 'losses.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        with open(str(self._run_dir / 'losses.p'), 'wb') as f:\n",
    "            pickle.dump(self._losses, f)\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        for l in self._critic.layers:\n",
    "            if 'minibatch_discrimination' not in l.name:\n",
    "                weights = [np.clip(w, -clip_value, clip_value) for w in l.get_weights()]\n",
    "                l.set_weights(weights)\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            'timesteps' : self._timesteps,\n",
    "            'latent_dim' : self._latent_dim,\n",
    "            'run_dir' : self._run_dir,\n",
    "            'img_dir' : self._img_dir,\n",
    "            'model_dir' : self._model_dir,\n",
    "            'generated_datesets_dir' : self._generated_datesets_dir\n",
    "        }\n",
    "        \n",
    "        with open(str(self._run_dir / 'config.p'), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "    def _save_models(self):\n",
    "        self._gan.save(self._model_dir / 'wgan.h5')\n",
    "        self._generator.save(self._model_dir / 'generator.h5')\n",
    "        self._critic.save(self._model_dir / 'critic.h5')\n",
    "        \n",
    "    def _generate_dataset(self, epoch, dataset_generation_size):\n",
    "        z_samples = np.random.normal(0, 1, (dataset_generation_size, self._latent_dim))\n",
    "        generated_dataset = self._generator.predict(z_samples)\n",
    "        np.save(self._generated_datesets_dir / ('%d_generated_data' % epoch), generated_dataset)\n",
    "        np.save(self._generated_datesets_dir / 'last', generated_dataset)\n",
    "        \n",
    "    def get_models(self):\n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def restore_training(self):\n",
    "        self.load_models()\n",
    "        with open(str(self._run_dir / 'losses.p'), 'rb') as f:\n",
    "            self._losses = pickle.load(f)\n",
    "            self._epoch = len(self._losses[0])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    def load_models(self):\n",
    "        custom_objects = {\n",
    "            'MinibatchDiscrimination':MinibatchDiscrimination,\n",
    "            '_wasserstein_loss':self._wasserstein_loss\n",
    "        }\n",
    "        self._gan = load_model(self._model_dir / 'wgan.h5', custom_objects=custom_objects)\n",
    "        self._generator = load_model(self._model_dir / 'generator.h5')\n",
    "        self._critic = load_model(self._model_dir / 'critic.h5', custom_objects=custom_objects)\n",
    "        \n",
    "        return self._gan, self._generator, self._critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53888, 100)\n"
     ]
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../datasets/berka_dataset/usable/normalized_transactions.npy\"\n",
    "\n",
    "timesteps = 100\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions = split_data(transactions, timesteps)\n",
    "transactions = transactions[np.std(transactions, 1) > float(1e-7)]\n",
    "N, D = transactions.shape\n",
    "print(transactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50000\n",
    "n_critic = 10\n",
    "n_generator = 1\n",
    "latent_dim = 2\n",
    "generator_lr = 0.00005\n",
    "critic_lr = 0.00005\n",
    "clip_value = 0.05\n",
    "img_frequency = 250\n",
    "model_save_frequency = 3000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('wgan')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [C loss: 0.999978] [G loss: 1.000071]\n",
      "2 [C loss: 0.999977] [G loss: 1.000066]\n",
      "3 [C loss: 0.999977] [G loss: 1.000061]\n",
      "4 [C loss: 0.999985] [G loss: 1.000058]\n",
      "5 [C loss: 0.999995] [G loss: 1.000054]\n",
      "6 [C loss: 1.000006] [G loss: 1.000049]\n",
      "7 [C loss: 0.999973] [G loss: 1.000044]\n",
      "8 [C loss: 0.999999] [G loss: 1.000040]\n",
      "9 [C loss: 1.000029] [G loss: 1.000034]\n",
      "10 [C loss: 1.000029] [G loss: 1.000027]\n",
      "11 [C loss: 1.000071] [G loss: 1.000020]\n",
      "12 [C loss: 1.000010] [G loss: 1.000010]\n",
      "13 [C loss: 1.000131] [G loss: 1.000002]\n",
      "14 [C loss: 1.000123] [G loss: 0.999996]\n",
      "15 [C loss: 1.000078] [G loss: 0.999989]\n",
      "16 [C loss: 1.000108] [G loss: 0.999985]\n",
      "17 [C loss: 1.000137] [G loss: 0.999978]\n",
      "18 [C loss: 1.000183] [G loss: 0.999973]\n",
      "19 [C loss: 1.000149] [G loss: 0.999969]\n",
      "20 [C loss: 1.000118] [G loss: 0.999962]\n",
      "21 [C loss: 1.000168] [G loss: 0.999958]\n",
      "22 [C loss: 1.000128] [G loss: 0.999950]\n",
      "23 [C loss: 1.000163] [G loss: 0.999946]\n",
      "24 [C loss: 1.000277] [G loss: 0.999940]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d3603f1f69a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n\u001b[0;32m---> 10\u001b[0;31m            img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2cbcaf08885e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value, img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m                                                   critic_loss_fake)\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2cbcaf08885e>\u001b[0m in \u001b[0;36m_clip_weights\u001b[0;34m(self, clip_value)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'minibatch_discrimination'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \"\"\"\n\u001b[1;32m   2319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wgan = WGAN(timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir)\n",
    "# gan, generator, critic = wgan.restore_training()\n",
    "gan, generator, critic = wgan.build_models(generator_lr, critic_lr)\n",
    "\n",
    "# gan.summary()\n",
    "# generator.summary()\n",
    "# critic.summary()\n",
    "        \n",
    "losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
