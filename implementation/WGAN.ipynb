{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, timesteps):\n",
    "    D = dataset.shape[1]\n",
    "    if D < timesteps:\n",
    "        return None\n",
    "    elif D == timesteps:\n",
    "        return dataset\n",
    "    else:\n",
    "        splitted_data, remaining_data = np.hsplit(dataset, [timesteps])\n",
    "        remaining_data = split_data(remaining_data, timesteps)\n",
    "        if remaining_data is not None:\n",
    "            return np.vstack([splitted_data, remaining_data])\n",
    "        return splitted_data\n",
    "    \n",
    "class MinibatchDiscrimination(Layer):\n",
    "    \"\"\"Concatenates to each sample information about how different the input\n",
    "    features for that sample are from features of other samples in the same\n",
    "    minibatch, as described in Salimans et. al. (2016). Useful for preventing\n",
    "    GANs from collapsing to a single output. When using this layer, generated\n",
    "    samples and reference samples should be in separate batches.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_dim, init='glorot_uniform', weights=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "\n",
    "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
    "            initializer=self.init,\n",
    "            name='kernel',\n",
    "            regularizer=self.W_regularizer,\n",
    "            trainable=True,\n",
    "            constraint=self.W_constraint)\n",
    "\n",
    "        # Set built to true.\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
    "        diffs = K.expand_dims(activation, 3) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
    "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
    "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
    "#         return K.concatenate([x, minibatch_features], 1)\n",
    "        return minibatch_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "#         return input_shape[0], input_shape[1]+self.nb_kernels\n",
    "        return input_shape[0], self.nb_kernels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_kernels': self.nb_kernels,\n",
    "                  'kernel_dim': self.kernel_dim,\n",
    "#                   'init': self.init.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0,
     1,
     14,
     38,
     178,
     190,
     196,
     209,
     214,
     220,
     224
    ]
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self, timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir):\n",
    "        self._timesteps = timesteps\n",
    "        self._latent_dim = latent_dim\n",
    "        self._run_dir = run_dir\n",
    "        self._img_dir = img_dir\n",
    "        self._model_dir = model_dir\n",
    "        self._generated_datesets_dir = generated_datesets_dir\n",
    "        \n",
    "        self._save_config()\n",
    "        \n",
    "        self._epoch = 0\n",
    "        self._losses = [[], []]\n",
    "\n",
    "    def build_models(self, generator_lr, critic_lr):        \n",
    "        self._generator = self._build_generator(self._latent_dim, self._timesteps)\n",
    "\n",
    "        self._critic = self._build_critic(self._timesteps)\n",
    "        self._critic.compile(loss=self._wasserstein_loss, optimizer=RMSprop(critic_lr))\n",
    "\n",
    "        z = Input(shape=(self._latent_dim, ))\n",
    "        fake = self._generator(z)\n",
    "\n",
    "        for layer in self._critic.layers:\n",
    "            layer.trainable = False\n",
    "        self._critic.trainable = False\n",
    "\n",
    "        valid = self._critic(fake)\n",
    "\n",
    "        self._gan = Model(z, valid, 'GAN')\n",
    "\n",
    "        self._gan.compile(\n",
    "            loss=self._wasserstein_loss,\n",
    "            optimizer=RMSprop(generator_lr),\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "\n",
    "    def _build_generator(self, noise_dim, timesteps):\n",
    "        generator_inputs = Input((latent_dim, ))\n",
    "        generated = generator_inputs\n",
    "        \n",
    "        generated = Lambda(lambda x: K.expand_dims(x))(generated)\n",
    "        while generated.shape[1] < timesteps:\n",
    "            generated = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(generated)\n",
    "            generated = UpSampling1D(2)(generated)\n",
    "        generated = Conv1D(\n",
    "            1, 3, activation='relu', padding='same')(generated)\n",
    "        generated = Lambda(lambda x: K.squeeze(x, -1))(generated)\n",
    "        generated = Dense(timesteps, activation='tanh')(generated)\n",
    "\n",
    "        generator = Model(generator_inputs, generated, 'generator')\n",
    "        return generator\n",
    "\n",
    "    def _build_critic(self, timesteps):\n",
    "        critic_inputs = Input((timesteps, ))\n",
    "        criticized = critic_inputs\n",
    "        \n",
    "        mbd = MinibatchDiscrimination(5, 3)(criticized)\n",
    "        \n",
    "        criticized = Lambda(lambda x: K.expand_dims(x))(\n",
    "            criticized)\n",
    "        while criticized.shape[1] > 1:\n",
    "            criticized = Conv1D(\n",
    "                32, 3, activation='relu', padding='same')(criticized)\n",
    "            criticized = MaxPooling1D(2, padding='same')(criticized)\n",
    "        criticized = Flatten()(criticized)\n",
    "        criticized = Concatenate()([criticized, mbd])\n",
    "        criticized = Dense(32, activation='relu')(criticized)\n",
    "        criticized = Dense(15, activation='relu')(criticized)\n",
    "        criticized = Dense(1)(criticized) \n",
    "\n",
    "        critic = Model(critic_inputs, criticized, 'critic')\n",
    "        return critic\n",
    "\n",
    "    def train(self, batch_size, epochs, n_generator, n_critic, dataset, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size):\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        \n",
    "        while self._epoch < epochs:\n",
    "            self._epoch += 1\n",
    "            for _ in range(n_critic):\n",
    "                indexes = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "                batch_transactions = dataset[indexes]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (half_batch, self._latent_dim))\n",
    "\n",
    "                generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "                critic_loss_real = self._critic.train_on_batch(\n",
    "                    batch_transactions, -np.ones((half_batch, 1)))\n",
    "                critic_loss_fake = self._critic.train_on_batch(\n",
    "                    generated_transactions, np.ones((half_batch, 1)))\n",
    "                critic_loss = 0.5 * np.add(critic_loss_real,\n",
    "                                                  critic_loss_fake)\n",
    "\n",
    "                self._clip_weights(clip_value)\n",
    "\n",
    "            for _ in range(n_generator):\n",
    "                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "                generator_loss = self._gan.train_on_batch(\n",
    "                    noise, -np.ones((batch_size, 1)))[0]\n",
    "            \n",
    "            generator_loss = 1 - generator_loss\n",
    "            critic_loss = 1 - critic_loss\n",
    "            \n",
    "            self._losses[0].append(generator_loss)\n",
    "            self._losses[1].append(critic_loss)\n",
    "\n",
    "            print(\"%d [C loss: %f] [G loss: %f]\" % (self._epoch, critic_loss,\n",
    "                                                    generator_loss))\n",
    "\n",
    "            if self._epoch % img_frequency == 0:\n",
    "                self._save_imgs()\n",
    "                self._save_latent_space()\n",
    "            \n",
    "            if self._epoch % model_save_frequency == 0:\n",
    "                self._save_models()\n",
    "                \n",
    "            if self._epoch % dataset_generation_frequency == 0:\n",
    "                self._generate_dataset(self._epoch, dataset_generation_size)\n",
    "                \n",
    "            if self._epoch % 250 == 0:\n",
    "                self._save_losses()\n",
    "          \n",
    "        self._generate_dataset(epochs, dataset_generation_size)\n",
    "        self._save_losses(self._losses)\n",
    "        self._save_models()\n",
    "        self._save_imgs()\n",
    "        self._save_latent_space()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _save_imgs(self):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, latent_dim))\n",
    "        generated_transactions = self._generator.predict(noise)\n",
    "\n",
    "        plt.subplots(rows, columns, figsize=(15, 5))\n",
    "        k = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                plt.subplot(rows, columns, k)\n",
    "                plt.plot(generated_transactions[k - 1])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "                k += 1\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('%05d.png' % self._epoch)))\n",
    "        plt.savefig(str(self._img_dir / 'last.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    def _save_latent_space(self):\n",
    "        if self._latent_dim > 2:\n",
    "            latent_vector = np.random.normal(0, 1, latent_dim)\n",
    "        plt.subplots(5, 5, figsize=(15, 5))\n",
    "\n",
    "        for i, v_i in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "            for j, v_j in enumerate(np.linspace(-2, 2, 5, True)):\n",
    "                if self._latent_dim > 2:\n",
    "                    latent_vector[-2:] = [v_i, v_j]\n",
    "                else:\n",
    "                    latent_vector = np.array([v_i, v_j])\n",
    "                    \n",
    "                plt.subplot(5, 5, i*5+j+1)\n",
    "                plt.plot(self._generator.predict(latent_vector.reshape((1, self._latent_dim))).T)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.ylim(-1, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(self._img_dir / ('latent_space.png')))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    def _save_losses(self):\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(self._losses[0])\n",
    "        plt.plot(self._losses[1])\n",
    "        plt.legend(['generator', 'critic'])\n",
    "        plt.savefig(str(self._img_dir / 'losses.png'))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        with open(str(self._run_dir / 'losses.p'), 'wb') as f:\n",
    "            pickle.dump(self._losses, f)\n",
    "        \n",
    "    def _clip_weights(self, clip_value):\n",
    "        for l in self._critic.layers:\n",
    "#             if 'minibatch_discrimination' not in l.name:\n",
    "            weights = [np.clip(w, -clip_value, clip_value) for w in l.get_weights()]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            'timesteps' : self._timesteps,\n",
    "            'latent_dim' : self._latent_dim,\n",
    "            'run_dir' : self._run_dir,\n",
    "            'img_dir' : self._img_dir,\n",
    "            'model_dir' : self._model_dir,\n",
    "            'generated_datesets_dir' : self._generated_datesets_dir\n",
    "        }\n",
    "        \n",
    "        with open(str(self._run_dir / 'config.p'), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "    def _save_models(self):\n",
    "        self._gan.save(self._model_dir / 'wgan.h5')\n",
    "        self._generator.save(self._model_dir / 'generator.h5')\n",
    "        self._critic.save(self._model_dir / 'critic.h5')\n",
    "        \n",
    "    def _generate_dataset(self, epoch, dataset_generation_size):\n",
    "        z_samples = np.random.normal(0, 1, (dataset_generation_size, self._latent_dim))\n",
    "        generated_dataset = self._generator.predict(z_samples)\n",
    "        np.save(self._generated_datesets_dir / ('%d_generated_data' % epoch), generated_dataset)\n",
    "        np.save(self._generated_datesets_dir / 'last', generated_dataset)\n",
    "        \n",
    "    def get_models(self):\n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def restore_training(self):\n",
    "        self.load_models()\n",
    "        with open(str(self._run_dir / 'losses.p'), 'rb') as f:\n",
    "            self._losses = pickle.load(f)\n",
    "            self._epoch = len(self._losses[0])\n",
    "        \n",
    "        return self._gan, self._generator, self._critic\n",
    "    \n",
    "    def load_models(self):\n",
    "        custom_objects = {\n",
    "            'MinibatchDiscrimination':MinibatchDiscrimination,\n",
    "            '_wasserstein_loss':self._wasserstein_loss\n",
    "        }\n",
    "        self._gan = load_model(self._model_dir / 'wgan.h5', custom_objects=custom_objects)\n",
    "        self._generator = load_model(self._model_dir / 'generator.h5')\n",
    "        self._critic = load_model(self._model_dir / 'critic.h5', custom_objects=custom_objects)\n",
    "        \n",
    "        return self._gan, self._generator, self._critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53888, 100)\n"
     ]
    }
   ],
   "source": [
    "normalized_transactions_filepath = \"../datasets/berka_dataset/usable/normalized_transactions.npy\"\n",
    "\n",
    "timesteps = 100\n",
    "transactions = np.load(normalized_transactions_filepath)\n",
    "transactions = split_data(transactions, timesteps)\n",
    "transactions = transactions[np.std(transactions, 1) > float(1e-7)]\n",
    "N, D = transactions.shape\n",
    "print(transactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50000\n",
    "n_critic = 10\n",
    "n_generator = 1\n",
    "latent_dim = 2\n",
    "generator_lr = 0.00005\n",
    "critic_lr = 0.00005\n",
    "clip_value = 0.05\n",
    "img_frequency = 250\n",
    "model_save_frequency = 3000\n",
    "dataset_generation_frequency = 25000\n",
    "dataset_generation_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('wgan')\n",
    "if not root_path.exists():\n",
    "    root_path.mkdir()\n",
    "    \n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "run_dir = root_path / current_datetime\n",
    "img_dir = run_dir / 'img'\n",
    "model_dir = run_dir / 'models'\n",
    "generated_datesets_dir = run_dir / 'generated_datasets'\n",
    "\n",
    "img_dir.mkdir(parents=True)\n",
    "model_dir.mkdir(parents=True)\n",
    "generated_datesets_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ing-luca/.local/lib/python3.5/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [C loss: 0.993073] [G loss: 1.122786]\n",
      "2 [C loss: 0.993184] [G loss: 1.120527]\n",
      "3 [C loss: 0.993156] [G loss: 1.116927]\n",
      "4 [C loss: 0.994011] [G loss: 1.112862]\n",
      "5 [C loss: 0.994898] [G loss: 1.108846]\n",
      "6 [C loss: 0.993928] [G loss: 1.104602]\n",
      "7 [C loss: 0.993715] [G loss: 1.099973]\n",
      "8 [C loss: 0.994741] [G loss: 1.095441]\n",
      "9 [C loss: 0.994759] [G loss: 1.090491]\n",
      "10 [C loss: 0.995501] [G loss: 1.085775]\n",
      "11 [C loss: 0.996383] [G loss: 1.081729]\n",
      "12 [C loss: 0.996571] [G loss: 1.076770]\n",
      "13 [C loss: 0.997148] [G loss: 1.073577]\n",
      "14 [C loss: 0.996616] [G loss: 1.070647]\n",
      "15 [C loss: 0.996680] [G loss: 1.067704]\n",
      "16 [C loss: 0.996339] [G loss: 1.064939]\n",
      "17 [C loss: 0.997534] [G loss: 1.062069]\n",
      "18 [C loss: 0.998651] [G loss: 1.059287]\n",
      "19 [C loss: 0.998689] [G loss: 1.056718]\n",
      "20 [C loss: 0.998001] [G loss: 1.054070]\n",
      "21 [C loss: 0.998027] [G loss: 1.051318]\n",
      "22 [C loss: 0.998593] [G loss: 1.048653]\n",
      "23 [C loss: 0.999052] [G loss: 1.046230]\n",
      "24 [C loss: 1.000187] [G loss: 1.043712]\n",
      "25 [C loss: 0.999215] [G loss: 1.041160]\n",
      "26 [C loss: 1.000648] [G loss: 1.038597]\n",
      "27 [C loss: 0.999918] [G loss: 1.035952]\n",
      "28 [C loss: 1.000136] [G loss: 1.033714]\n",
      "29 [C loss: 1.000637] [G loss: 1.031203]\n",
      "30 [C loss: 1.000232] [G loss: 1.028829]\n",
      "31 [C loss: 1.001359] [G loss: 1.026487]\n",
      "32 [C loss: 1.000214] [G loss: 1.024114]\n",
      "33 [C loss: 1.002232] [G loss: 1.021934]\n",
      "34 [C loss: 1.002193] [G loss: 1.019622]\n",
      "35 [C loss: 1.000803] [G loss: 1.017485]\n",
      "36 [C loss: 1.001689] [G loss: 1.015364]\n",
      "37 [C loss: 1.002793] [G loss: 1.012950]\n",
      "38 [C loss: 1.002381] [G loss: 1.010855]\n",
      "39 [C loss: 1.001866] [G loss: 1.008712]\n",
      "40 [C loss: 1.003094] [G loss: 1.006392]\n",
      "41 [C loss: 1.004339] [G loss: 1.004380]\n",
      "42 [C loss: 1.002258] [G loss: 1.002128]\n",
      "43 [C loss: 1.003726] [G loss: 1.000082]\n",
      "44 [C loss: 1.004704] [G loss: 0.997860]\n",
      "45 [C loss: 1.004796] [G loss: 0.995925]\n",
      "46 [C loss: 1.003724] [G loss: 0.993932]\n",
      "47 [C loss: 1.005236] [G loss: 0.991815]\n",
      "48 [C loss: 1.004310] [G loss: 0.989919]\n",
      "49 [C loss: 1.003767] [G loss: 0.988048]\n",
      "50 [C loss: 1.006210] [G loss: 0.986071]\n",
      "51 [C loss: 1.005976] [G loss: 0.984166]\n",
      "52 [C loss: 1.004278] [G loss: 0.982354]\n",
      "53 [C loss: 1.006187] [G loss: 0.980324]\n",
      "54 [C loss: 1.006206] [G loss: 0.978580]\n",
      "55 [C loss: 1.006336] [G loss: 0.976696]\n",
      "56 [C loss: 1.006228] [G loss: 0.974861]\n",
      "57 [C loss: 1.006183] [G loss: 0.973225]\n",
      "58 [C loss: 1.006576] [G loss: 0.971402]\n",
      "59 [C loss: 1.007245] [G loss: 0.969733]\n",
      "60 [C loss: 1.006152] [G loss: 0.968056]\n",
      "61 [C loss: 1.007064] [G loss: 0.966575]\n",
      "62 [C loss: 1.007724] [G loss: 0.964520]\n",
      "63 [C loss: 1.007829] [G loss: 0.962680]\n",
      "64 [C loss: 1.007811] [G loss: 0.961280]\n",
      "65 [C loss: 1.006953] [G loss: 0.959521]\n",
      "66 [C loss: 1.008623] [G loss: 0.957952]\n",
      "67 [C loss: 1.007202] [G loss: 0.956759]\n",
      "68 [C loss: 1.007940] [G loss: 0.954997]\n",
      "69 [C loss: 1.009478] [G loss: 0.953503]\n",
      "70 [C loss: 1.010601] [G loss: 0.951625]\n",
      "71 [C loss: 1.009051] [G loss: 0.950558]\n",
      "72 [C loss: 1.010914] [G loss: 0.948920]\n",
      "73 [C loss: 1.009503] [G loss: 0.947613]\n",
      "74 [C loss: 1.009191] [G loss: 0.946282]\n",
      "75 [C loss: 1.008912] [G loss: 0.944249]\n",
      "76 [C loss: 1.008949] [G loss: 0.942978]\n",
      "77 [C loss: 1.010788] [G loss: 0.941555]\n",
      "78 [C loss: 1.011359] [G loss: 0.939949]\n",
      "79 [C loss: 1.012020] [G loss: 0.938551]\n",
      "80 [C loss: 1.011949] [G loss: 0.937057]\n",
      "81 [C loss: 1.011564] [G loss: 0.936121]\n",
      "82 [C loss: 1.015655] [G loss: 0.934842]\n",
      "83 [C loss: 1.014294] [G loss: 0.933493]\n",
      "84 [C loss: 1.015517] [G loss: 0.931907]\n",
      "85 [C loss: 1.017348] [G loss: 0.930982]\n",
      "86 [C loss: 1.016059] [G loss: 0.929537]\n",
      "87 [C loss: 1.015061] [G loss: 0.928652]\n",
      "88 [C loss: 1.016180] [G loss: 0.927326]\n",
      "89 [C loss: 1.019438] [G loss: 0.925407]\n",
      "90 [C loss: 1.015322] [G loss: 0.924095]\n",
      "91 [C loss: 1.017058] [G loss: 0.922825]\n",
      "92 [C loss: 1.020167] [G loss: 0.921186]\n",
      "93 [C loss: 1.020153] [G loss: 0.919974]\n",
      "94 [C loss: 1.022700] [G loss: 0.918792]\n",
      "95 [C loss: 1.022956] [G loss: 0.918594]\n",
      "96 [C loss: 1.022686] [G loss: 0.916963]\n",
      "97 [C loss: 1.027757] [G loss: 0.915974]\n",
      "98 [C loss: 1.024162] [G loss: 0.913989]\n",
      "99 [C loss: 1.027365] [G loss: 0.913038]\n",
      "100 [C loss: 1.027459] [G loss: 0.912264]\n",
      "101 [C loss: 1.025042] [G loss: 0.911336]\n",
      "102 [C loss: 1.027786] [G loss: 0.910305]\n",
      "103 [C loss: 1.031612] [G loss: 0.908146]\n",
      "104 [C loss: 1.028385] [G loss: 0.907428]\n",
      "105 [C loss: 1.031468] [G loss: 0.906180]\n",
      "106 [C loss: 1.030670] [G loss: 0.905323]\n",
      "107 [C loss: 1.031686] [G loss: 0.904419]\n",
      "108 [C loss: 1.039046] [G loss: 0.903089]\n",
      "109 [C loss: 1.032763] [G loss: 0.902074]\n",
      "110 [C loss: 1.036936] [G loss: 0.901446]\n",
      "111 [C loss: 1.034518] [G loss: 0.899428]\n",
      "112 [C loss: 1.040068] [G loss: 0.899207]\n",
      "113 [C loss: 1.040873] [G loss: 0.898865]\n",
      "114 [C loss: 1.041967] [G loss: 0.897390]\n",
      "115 [C loss: 1.042698] [G loss: 0.895440]\n",
      "116 [C loss: 1.045358] [G loss: 0.895274]\n",
      "117 [C loss: 1.046129] [G loss: 0.893848]\n",
      "118 [C loss: 1.046379] [G loss: 0.893251]\n",
      "119 [C loss: 1.053639] [G loss: 0.892773]\n",
      "120 [C loss: 1.052018] [G loss: 0.891821]\n",
      "121 [C loss: 1.055486] [G loss: 0.891017]\n",
      "122 [C loss: 1.056624] [G loss: 0.888938]\n",
      "123 [C loss: 1.060970] [G loss: 0.889171]\n",
      "124 [C loss: 1.061161] [G loss: 0.887765]\n",
      "125 [C loss: 1.067564] [G loss: 0.886223]\n",
      "126 [C loss: 1.074031] [G loss: 0.885267]\n",
      "127 [C loss: 1.070533] [G loss: 0.882652]\n",
      "128 [C loss: 1.069966] [G loss: 0.883260]\n",
      "129 [C loss: 1.077915] [G loss: 0.880190]\n",
      "130 [C loss: 1.082340] [G loss: 0.878676]\n",
      "131 [C loss: 1.084258] [G loss: 0.877340]\n",
      "132 [C loss: 1.092825] [G loss: 0.875734]\n",
      "133 [C loss: 1.091737] [G loss: 0.873038]\n",
      "134 [C loss: 1.099942] [G loss: 0.871377]\n",
      "135 [C loss: 1.106304] [G loss: 0.868952]\n",
      "136 [C loss: 1.105010] [G loss: 0.866406]\n",
      "137 [C loss: 1.108771] [G loss: 0.862655]\n",
      "138 [C loss: 1.114549] [G loss: 0.861629]\n",
      "139 [C loss: 1.120259] [G loss: 0.857701]\n",
      "140 [C loss: 1.118559] [G loss: 0.856852]\n",
      "141 [C loss: 1.135297] [G loss: 0.853726]\n",
      "142 [C loss: 1.138165] [G loss: 0.851478]\n",
      "143 [C loss: 1.146926] [G loss: 0.846461]\n",
      "144 [C loss: 1.152270] [G loss: 0.841334]\n",
      "145 [C loss: 1.159401] [G loss: 0.838453]\n",
      "146 [C loss: 1.158960] [G loss: 0.837836]\n",
      "147 [C loss: 1.169097] [G loss: 0.831619]\n",
      "148 [C loss: 1.180514] [G loss: 0.828927]\n",
      "149 [C loss: 1.186521] [G loss: 0.826965]\n",
      "150 [C loss: 1.187666] [G loss: 0.821597]\n",
      "151 [C loss: 1.196632] [G loss: 0.818737]\n",
      "152 [C loss: 1.205254] [G loss: 0.816468]\n",
      "153 [C loss: 1.206497] [G loss: 0.810929]\n",
      "154 [C loss: 1.217468] [G loss: 0.806847]\n",
      "155 [C loss: 1.225033] [G loss: 0.799915]\n",
      "156 [C loss: 1.236459] [G loss: 0.795241]\n",
      "157 [C loss: 1.242329] [G loss: 0.791652]\n",
      "158 [C loss: 1.245638] [G loss: 0.784290]\n",
      "159 [C loss: 1.234686] [G loss: 0.784902]\n",
      "160 [C loss: 1.253862] [G loss: 0.782450]\n",
      "161 [C loss: 1.271676] [G loss: 0.777548]\n",
      "162 [C loss: 1.281285] [G loss: 0.769910]\n",
      "163 [C loss: 1.272126] [G loss: 0.767602]\n",
      "164 [C loss: 1.297407] [G loss: 0.777932]\n",
      "165 [C loss: 1.296311] [G loss: 0.768084]\n",
      "166 [C loss: 1.291475] [G loss: 0.758833]\n",
      "167 [C loss: 1.311583] [G loss: 0.753884]\n",
      "168 [C loss: 1.308700] [G loss: 0.757766]\n",
      "169 [C loss: 1.310263] [G loss: 0.756421]\n",
      "170 [C loss: 1.307132] [G loss: 0.765926]\n",
      "171 [C loss: 1.338064] [G loss: 0.749023]\n",
      "172 [C loss: 1.314820] [G loss: 0.762235]\n",
      "173 [C loss: 1.313535] [G loss: 0.778759]\n",
      "174 [C loss: 1.321751] [G loss: 0.752223]\n",
      "175 [C loss: 1.340588] [G loss: 0.762439]\n",
      "176 [C loss: 1.322918] [G loss: 0.750129]\n",
      "177 [C loss: 1.375353] [G loss: 0.766478]\n",
      "178 [C loss: 1.334266] [G loss: 0.763180]\n",
      "179 [C loss: 1.352240] [G loss: 0.766187]\n",
      "180 [C loss: 1.348875] [G loss: 0.769738]\n",
      "181 [C loss: 1.356112] [G loss: 0.768795]\n",
      "182 [C loss: 1.365308] [G loss: 0.768062]\n",
      "183 [C loss: 1.343296] [G loss: 0.792936]\n",
      "184 [C loss: 1.341089] [G loss: 0.787559]\n",
      "185 [C loss: 1.343877] [G loss: 0.801067]\n",
      "186 [C loss: 1.362358] [G loss: 0.807988]\n",
      "187 [C loss: 1.358631] [G loss: 0.783104]\n",
      "188 [C loss: 1.343330] [G loss: 0.834092]\n",
      "189 [C loss: 1.355248] [G loss: 0.812356]\n",
      "190 [C loss: 1.317755] [G loss: 0.805560]\n",
      "191 [C loss: 1.304805] [G loss: 0.828151]\n",
      "192 [C loss: 1.321137] [G loss: 0.868632]\n",
      "193 [C loss: 1.319181] [G loss: 0.833649]\n",
      "194 [C loss: 1.286457] [G loss: 0.925762]\n",
      "195 [C loss: 1.311767] [G loss: 0.892145]\n",
      "196 [C loss: 1.345330] [G loss: 0.904566]\n",
      "197 [C loss: 1.349516] [G loss: 0.875737]\n",
      "198 [C loss: 1.305203] [G loss: 0.915351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 [C loss: 1.312496] [G loss: 0.885491]\n",
      "200 [C loss: 1.305782] [G loss: 0.949417]\n",
      "201 [C loss: 1.269079] [G loss: 0.880783]\n",
      "202 [C loss: 1.248957] [G loss: 0.957727]\n",
      "203 [C loss: 1.281342] [G loss: 0.931132]\n",
      "204 [C loss: 1.239685] [G loss: 0.973109]\n",
      "205 [C loss: 1.260051] [G loss: 0.929300]\n",
      "206 [C loss: 1.264979] [G loss: 1.048449]\n",
      "207 [C loss: 1.191493] [G loss: 0.996838]\n",
      "208 [C loss: 1.193752] [G loss: 1.000010]\n",
      "209 [C loss: 1.215144] [G loss: 1.069320]\n",
      "210 [C loss: 1.165568] [G loss: 1.062668]\n",
      "211 [C loss: 1.193196] [G loss: 1.034280]\n",
      "212 [C loss: 1.154040] [G loss: 1.072757]\n",
      "213 [C loss: 1.129109] [G loss: 1.105373]\n",
      "214 [C loss: 1.167810] [G loss: 1.099753]\n",
      "215 [C loss: 1.179615] [G loss: 1.107084]\n",
      "216 [C loss: 1.081284] [G loss: 1.039549]\n",
      "217 [C loss: 1.136434] [G loss: 1.069490]\n",
      "218 [C loss: 1.087105] [G loss: 1.110586]\n",
      "219 [C loss: 1.073676] [G loss: 1.184105]\n",
      "220 [C loss: 1.072563] [G loss: 1.083028]\n",
      "221 [C loss: 1.045569] [G loss: 1.103777]\n",
      "222 [C loss: 1.059805] [G loss: 1.201218]\n",
      "223 [C loss: 1.069032] [G loss: 1.070617]\n",
      "224 [C loss: 1.025541] [G loss: 1.076004]\n",
      "225 [C loss: 1.057539] [G loss: 1.116433]\n",
      "226 [C loss: 1.014956] [G loss: 1.091039]\n",
      "227 [C loss: 1.129895] [G loss: 1.150061]\n",
      "228 [C loss: 1.036012] [G loss: 1.083903]\n",
      "229 [C loss: 1.026160] [G loss: 1.044823]\n",
      "230 [C loss: 0.993847] [G loss: 1.088079]\n",
      "231 [C loss: 1.042922] [G loss: 1.050680]\n",
      "232 [C loss: 1.015450] [G loss: 1.076529]\n",
      "233 [C loss: 0.989192] [G loss: 1.066178]\n",
      "234 [C loss: 0.986826] [G loss: 1.038090]\n",
      "235 [C loss: 1.009250] [G loss: 1.044437]\n",
      "236 [C loss: 1.001836] [G loss: 1.055745]\n",
      "237 [C loss: 1.012033] [G loss: 0.955087]\n",
      "238 [C loss: 1.009249] [G loss: 0.963645]\n",
      "239 [C loss: 0.981567] [G loss: 0.950721]\n",
      "240 [C loss: 0.989157] [G loss: 0.934522]\n",
      "241 [C loss: 1.040253] [G loss: 0.969007]\n",
      "242 [C loss: 1.054284] [G loss: 0.870752]\n",
      "243 [C loss: 1.000959] [G loss: 0.909091]\n",
      "244 [C loss: 0.996813] [G loss: 0.870036]\n",
      "245 [C loss: 1.030578] [G loss: 0.890795]\n",
      "246 [C loss: 0.989433] [G loss: 0.832053]\n",
      "247 [C loss: 1.049398] [G loss: 0.849444]\n",
      "248 [C loss: 1.060076] [G loss: 0.823734]\n",
      "249 [C loss: 1.028570] [G loss: 0.817410]\n",
      "250 [C loss: 1.028848] [G loss: 0.799718]\n",
      "251 [C loss: 1.016197] [G loss: 0.776928]\n",
      "252 [C loss: 1.051902] [G loss: 0.786898]\n",
      "253 [C loss: 1.030737] [G loss: 0.761801]\n",
      "254 [C loss: 1.046420] [G loss: 0.757948]\n",
      "255 [C loss: 1.065227] [G loss: 0.746953]\n",
      "256 [C loss: 1.045751] [G loss: 0.727517]\n",
      "257 [C loss: 1.026065] [G loss: 0.725507]\n",
      "258 [C loss: 1.057722] [G loss: 0.721911]\n",
      "259 [C loss: 1.056613] [G loss: 0.716924]\n",
      "260 [C loss: 1.076766] [G loss: 0.714639]\n",
      "261 [C loss: 1.086351] [G loss: 0.712672]\n",
      "262 [C loss: 1.051971] [G loss: 0.713442]\n",
      "263 [C loss: 1.068136] [G loss: 0.708522]\n",
      "264 [C loss: 1.064974] [G loss: 0.706527]\n",
      "265 [C loss: 1.060715] [G loss: 0.701812]\n",
      "266 [C loss: 1.078660] [G loss: 0.699029]\n",
      "267 [C loss: 1.093260] [G loss: 0.696461]\n",
      "268 [C loss: 1.101219] [G loss: 0.696071]\n",
      "269 [C loss: 1.095202] [G loss: 0.695071]\n",
      "270 [C loss: 1.111154] [G loss: 0.693237]\n",
      "271 [C loss: 1.113494] [G loss: 0.691627]\n",
      "272 [C loss: 1.116901] [G loss: 0.690829]\n",
      "273 [C loss: 1.075355] [G loss: 0.689808]\n",
      "274 [C loss: 1.118210] [G loss: 0.687094]\n",
      "275 [C loss: 1.108717] [G loss: 0.685509]\n",
      "276 [C loss: 1.109100] [G loss: 0.684854]\n",
      "277 [C loss: 1.113575] [G loss: 0.684454]\n",
      "278 [C loss: 1.114171] [G loss: 0.681721]\n",
      "279 [C loss: 1.121318] [G loss: 0.680722]\n",
      "280 [C loss: 1.121897] [G loss: 0.679034]\n",
      "281 [C loss: 1.144127] [G loss: 0.678288]\n",
      "282 [C loss: 1.159870] [G loss: 0.677075]\n",
      "283 [C loss: 1.121481] [G loss: 0.675924]\n",
      "284 [C loss: 1.143942] [G loss: 0.673458]\n",
      "285 [C loss: 1.126408] [G loss: 0.671945]\n",
      "286 [C loss: 1.165080] [G loss: 0.670665]\n",
      "287 [C loss: 1.153954] [G loss: 0.670083]\n",
      "288 [C loss: 1.136252] [G loss: 0.668399]\n",
      "289 [C loss: 1.138266] [G loss: 0.665729]\n",
      "290 [C loss: 1.147024] [G loss: 0.664504]\n",
      "291 [C loss: 1.183559] [G loss: 0.664040]\n",
      "292 [C loss: 1.169771] [G loss: 0.662207]\n",
      "293 [C loss: 1.151175] [G loss: 0.660445]\n",
      "294 [C loss: 1.150904] [G loss: 0.659341]\n",
      "295 [C loss: 1.166990] [G loss: 0.657836]\n",
      "296 [C loss: 1.163947] [G loss: 0.656620]\n",
      "297 [C loss: 1.169622] [G loss: 0.656231]\n",
      "298 [C loss: 1.173703] [G loss: 0.655739]\n",
      "299 [C loss: 1.174799] [G loss: 0.654081]\n",
      "300 [C loss: 1.171821] [G loss: 0.653631]\n",
      "301 [C loss: 1.164287] [G loss: 0.651985]\n",
      "302 [C loss: 1.154026] [G loss: 0.651426]\n",
      "303 [C loss: 1.178723] [G loss: 0.651444]\n",
      "304 [C loss: 1.193076] [G loss: 0.654822]\n",
      "305 [C loss: 1.183689] [G loss: 0.648331]\n",
      "306 [C loss: 1.177224] [G loss: 0.650199]\n",
      "307 [C loss: 1.191098] [G loss: 0.647080]\n",
      "308 [C loss: 1.193988] [G loss: 0.646260]\n",
      "309 [C loss: 1.180675] [G loss: 0.647467]\n",
      "310 [C loss: 1.190225] [G loss: 0.644024]\n",
      "311 [C loss: 1.183455] [G loss: 0.643910]\n",
      "312 [C loss: 1.201400] [G loss: 0.643186]\n",
      "313 [C loss: 1.197243] [G loss: 0.642371]\n",
      "314 [C loss: 1.195008] [G loss: 0.641325]\n",
      "315 [C loss: 1.204780] [G loss: 0.643513]\n",
      "316 [C loss: 1.190135] [G loss: 0.639172]\n",
      "317 [C loss: 1.210414] [G loss: 0.638900]\n",
      "318 [C loss: 1.190426] [G loss: 0.638908]\n",
      "319 [C loss: 1.200214] [G loss: 0.635696]\n",
      "320 [C loss: 1.223033] [G loss: 0.637153]\n",
      "321 [C loss: 1.211144] [G loss: 0.636725]\n",
      "322 [C loss: 1.213795] [G loss: 0.637112]\n",
      "323 [C loss: 1.198720] [G loss: 0.637208]\n",
      "324 [C loss: 1.213480] [G loss: 0.633186]\n",
      "325 [C loss: 1.233178] [G loss: 0.632917]\n",
      "326 [C loss: 1.231213] [G loss: 0.632350]\n",
      "327 [C loss: 1.254720] [G loss: 0.631902]\n",
      "328 [C loss: 1.229104] [G loss: 0.631735]\n",
      "329 [C loss: 1.239591] [G loss: 0.631464]\n",
      "330 [C loss: 1.233317] [G loss: 0.639661]\n",
      "331 [C loss: 1.209044] [G loss: 0.639587]\n",
      "332 [C loss: 1.240098] [G loss: 0.628908]\n",
      "333 [C loss: 1.237916] [G loss: 0.635080]\n",
      "334 [C loss: 1.206066] [G loss: 0.638607]\n",
      "335 [C loss: 1.235601] [G loss: 0.629651]\n",
      "336 [C loss: 1.236776] [G loss: 0.635536]\n",
      "337 [C loss: 1.266395] [G loss: 0.628188]\n",
      "338 [C loss: 1.239935] [G loss: 0.626508]\n",
      "339 [C loss: 1.217572] [G loss: 0.624602]\n",
      "340 [C loss: 1.242351] [G loss: 0.632537]\n",
      "341 [C loss: 1.240791] [G loss: 0.642327]\n",
      "342 [C loss: 1.253003] [G loss: 0.623364]\n",
      "343 [C loss: 1.249999] [G loss: 0.645206]\n",
      "344 [C loss: 1.291791] [G loss: 0.634259]\n",
      "345 [C loss: 1.227149] [G loss: 0.646235]\n",
      "346 [C loss: 1.263422] [G loss: 0.652370]\n",
      "347 [C loss: 1.248638] [G loss: 0.650314]\n",
      "348 [C loss: 1.251434] [G loss: 0.632833]\n",
      "349 [C loss: 1.237479] [G loss: 0.639288]\n",
      "350 [C loss: 1.249329] [G loss: 0.639697]\n",
      "351 [C loss: 1.239466] [G loss: 0.663626]\n",
      "352 [C loss: 1.240186] [G loss: 0.639593]\n",
      "353 [C loss: 1.247553] [G loss: 0.647684]\n",
      "354 [C loss: 1.252183] [G loss: 0.642767]\n",
      "355 [C loss: 1.246510] [G loss: 0.637387]\n",
      "356 [C loss: 1.259163] [G loss: 0.646860]\n",
      "357 [C loss: 1.245490] [G loss: 0.649610]\n",
      "358 [C loss: 1.251096] [G loss: 0.652507]\n",
      "359 [C loss: 1.249401] [G loss: 0.675095]\n",
      "360 [C loss: 1.252637] [G loss: 0.671480]\n",
      "361 [C loss: 1.242204] [G loss: 0.676433]\n",
      "362 [C loss: 1.260950] [G loss: 0.658332]\n",
      "363 [C loss: 1.234902] [G loss: 0.644739]\n",
      "364 [C loss: 1.218441] [G loss: 0.653576]\n",
      "365 [C loss: 1.253617] [G loss: 0.672387]\n",
      "366 [C loss: 1.252024] [G loss: 0.664352]\n",
      "367 [C loss: 1.239033] [G loss: 0.657385]\n",
      "368 [C loss: 1.267429] [G loss: 0.660756]\n",
      "369 [C loss: 1.215323] [G loss: 0.671516]\n",
      "370 [C loss: 1.263411] [G loss: 0.689291]\n",
      "371 [C loss: 1.244732] [G loss: 0.694065]\n",
      "372 [C loss: 1.230639] [G loss: 0.728161]\n",
      "373 [C loss: 1.230054] [G loss: 0.697221]\n",
      "374 [C loss: 1.210645] [G loss: 0.681669]\n",
      "375 [C loss: 1.225858] [G loss: 0.716562]\n",
      "376 [C loss: 1.201264] [G loss: 0.696906]\n",
      "377 [C loss: 1.252351] [G loss: 0.669811]\n",
      "378 [C loss: 1.261463] [G loss: 0.743967]\n",
      "379 [C loss: 1.232597] [G loss: 0.712635]\n",
      "380 [C loss: 1.209860] [G loss: 0.717608]\n",
      "381 [C loss: 1.204711] [G loss: 0.736748]\n",
      "382 [C loss: 1.180556] [G loss: 0.764042]\n",
      "383 [C loss: 1.238034] [G loss: 0.779312]\n",
      "384 [C loss: 1.213714] [G loss: 0.755315]\n",
      "385 [C loss: 1.209135] [G loss: 0.813786]\n",
      "386 [C loss: 1.220895] [G loss: 0.814499]\n",
      "387 [C loss: 1.173561] [G loss: 0.816485]\n",
      "388 [C loss: 1.223316] [G loss: 0.876706]\n",
      "389 [C loss: 1.176299] [G loss: 0.835040]\n",
      "390 [C loss: 1.192915] [G loss: 0.845193]\n",
      "391 [C loss: 1.179689] [G loss: 0.866506]\n",
      "392 [C loss: 1.173738] [G loss: 0.893171]\n",
      "393 [C loss: 1.211475] [G loss: 0.871478]\n",
      "394 [C loss: 1.193933] [G loss: 0.891395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 [C loss: 1.201990] [G loss: 0.920832]\n",
      "396 [C loss: 1.216894] [G loss: 0.911046]\n",
      "397 [C loss: 1.198990] [G loss: 0.901313]\n",
      "398 [C loss: 1.176858] [G loss: 0.927491]\n",
      "399 [C loss: 1.180320] [G loss: 0.945057]\n",
      "400 [C loss: 1.182829] [G loss: 0.935656]\n",
      "401 [C loss: 1.189802] [G loss: 0.934412]\n",
      "402 [C loss: 1.200998] [G loss: 0.908960]\n",
      "403 [C loss: 1.181212] [G loss: 0.943789]\n",
      "404 [C loss: 1.182106] [G loss: 0.944480]\n",
      "405 [C loss: 1.173662] [G loss: 0.974256]\n",
      "406 [C loss: 1.187816] [G loss: 0.977275]\n",
      "407 [C loss: 1.179362] [G loss: 0.950644]\n",
      "408 [C loss: 1.173190] [G loss: 0.964725]\n",
      "409 [C loss: 1.190720] [G loss: 0.938123]\n",
      "410 [C loss: 1.205533] [G loss: 0.956070]\n",
      "411 [C loss: 1.173567] [G loss: 0.940222]\n",
      "412 [C loss: 1.165762] [G loss: 0.970313]\n",
      "413 [C loss: 1.190525] [G loss: 0.957427]\n",
      "414 [C loss: 1.180587] [G loss: 0.928970]\n",
      "415 [C loss: 1.162624] [G loss: 0.953805]\n",
      "416 [C loss: 1.219877] [G loss: 0.959039]\n",
      "417 [C loss: 1.165294] [G loss: 0.948054]\n",
      "418 [C loss: 1.198491] [G loss: 0.946711]\n",
      "419 [C loss: 1.141934] [G loss: 0.938992]\n",
      "420 [C loss: 1.179394] [G loss: 0.933070]\n",
      "421 [C loss: 1.186301] [G loss: 0.935232]\n",
      "422 [C loss: 1.162524] [G loss: 0.929197]\n",
      "423 [C loss: 1.117396] [G loss: 0.932947]\n",
      "424 [C loss: 1.152706] [G loss: 0.898593]\n",
      "425 [C loss: 1.143065] [G loss: 0.932778]\n",
      "426 [C loss: 1.150103] [G loss: 0.908225]\n",
      "427 [C loss: 1.159452] [G loss: 0.911872]\n",
      "428 [C loss: 1.190023] [G loss: 0.885206]\n",
      "429 [C loss: 1.145325] [G loss: 0.893451]\n",
      "430 [C loss: 1.143985] [G loss: 0.885060]\n",
      "431 [C loss: 1.166076] [G loss: 0.877714]\n",
      "432 [C loss: 1.148958] [G loss: 0.856604]\n",
      "433 [C loss: 1.140111] [G loss: 0.860993]\n",
      "434 [C loss: 1.153398] [G loss: 0.851219]\n",
      "435 [C loss: 1.148297] [G loss: 0.838817]\n",
      "436 [C loss: 1.122062] [G loss: 0.807749]\n",
      "437 [C loss: 1.154838] [G loss: 0.813272]\n",
      "438 [C loss: 1.146908] [G loss: 0.802450]\n",
      "439 [C loss: 1.138200] [G loss: 0.802657]\n",
      "440 [C loss: 1.136884] [G loss: 0.789534]\n",
      "441 [C loss: 1.168782] [G loss: 0.775868]\n",
      "442 [C loss: 1.136912] [G loss: 0.760461]\n",
      "443 [C loss: 1.120799] [G loss: 0.748811]\n",
      "444 [C loss: 1.130795] [G loss: 0.744758]\n",
      "445 [C loss: 1.175615] [G loss: 0.733255]\n",
      "446 [C loss: 1.175968] [G loss: 0.728570]\n",
      "447 [C loss: 1.141131] [G loss: 0.710903]\n",
      "448 [C loss: 1.172917] [G loss: 0.692989]\n",
      "449 [C loss: 1.108149] [G loss: 0.692935]\n",
      "450 [C loss: 1.117860] [G loss: 0.680619]\n",
      "451 [C loss: 1.137280] [G loss: 0.671909]\n",
      "452 [C loss: 1.142472] [G loss: 0.667339]\n",
      "453 [C loss: 1.152264] [G loss: 0.665816]\n",
      "454 [C loss: 1.144600] [G loss: 0.658198]\n",
      "455 [C loss: 1.133048] [G loss: 0.648798]\n",
      "456 [C loss: 1.138522] [G loss: 0.647596]\n",
      "457 [C loss: 1.139894] [G loss: 0.643758]\n",
      "458 [C loss: 1.145527] [G loss: 0.639674]\n",
      "459 [C loss: 1.129389] [G loss: 0.635496]\n",
      "460 [C loss: 1.161021] [G loss: 0.632799]\n",
      "461 [C loss: 1.162473] [G loss: 0.633249]\n",
      "462 [C loss: 1.153021] [G loss: 0.630084]\n",
      "463 [C loss: 1.164717] [G loss: 0.629820]\n",
      "464 [C loss: 1.160525] [G loss: 0.628354]\n",
      "465 [C loss: 1.160081] [G loss: 0.627391]\n",
      "466 [C loss: 1.166485] [G loss: 0.626417]\n",
      "467 [C loss: 1.161214] [G loss: 0.625947]\n",
      "468 [C loss: 1.159459] [G loss: 0.626161]\n",
      "469 [C loss: 1.158923] [G loss: 0.622930]\n",
      "470 [C loss: 1.173586] [G loss: 0.622481]\n",
      "471 [C loss: 1.187591] [G loss: 0.622140]\n",
      "472 [C loss: 1.153959] [G loss: 0.620378]\n",
      "473 [C loss: 1.205177] [G loss: 0.621972]\n",
      "474 [C loss: 1.187981] [G loss: 0.621698]\n",
      "475 [C loss: 1.185355] [G loss: 0.620464]\n",
      "476 [C loss: 1.184904] [G loss: 0.620747]\n",
      "477 [C loss: 1.156925] [G loss: 0.621348]\n",
      "478 [C loss: 1.211945] [G loss: 0.618395]\n",
      "479 [C loss: 1.174295] [G loss: 0.620340]\n",
      "480 [C loss: 1.176974] [G loss: 0.619457]\n",
      "481 [C loss: 1.156290] [G loss: 0.618479]\n",
      "482 [C loss: 1.192538] [G loss: 0.619895]\n",
      "483 [C loss: 1.158929] [G loss: 0.617216]\n",
      "484 [C loss: 1.173019] [G loss: 0.618806]\n",
      "485 [C loss: 1.194049] [G loss: 0.617136]\n",
      "486 [C loss: 1.188873] [G loss: 0.618315]\n",
      "487 [C loss: 1.185012] [G loss: 0.617248]\n",
      "488 [C loss: 1.149222] [G loss: 0.617885]\n",
      "489 [C loss: 1.207845] [G loss: 0.617277]\n",
      "490 [C loss: 1.215390] [G loss: 0.617600]\n",
      "491 [C loss: 1.215198] [G loss: 0.617160]\n",
      "492 [C loss: 1.169723] [G loss: 0.618134]\n",
      "493 [C loss: 1.171465] [G loss: 0.616430]\n",
      "494 [C loss: 1.205265] [G loss: 0.616577]\n",
      "495 [C loss: 1.198345] [G loss: 0.616108]\n",
      "496 [C loss: 1.201589] [G loss: 0.616151]\n",
      "497 [C loss: 1.194752] [G loss: 0.614459]\n",
      "498 [C loss: 1.188987] [G loss: 0.614730]\n",
      "499 [C loss: 1.204631] [G loss: 0.615169]\n",
      "500 [C loss: 1.174977] [G loss: 0.617163]\n",
      "501 [C loss: 1.194911] [G loss: 0.615423]\n",
      "502 [C loss: 1.204324] [G loss: 0.615756]\n",
      "503 [C loss: 1.181278] [G loss: 0.616754]\n",
      "504 [C loss: 1.175296] [G loss: 0.616220]\n",
      "505 [C loss: 1.204115] [G loss: 0.614408]\n",
      "506 [C loss: 1.216869] [G loss: 0.615360]\n",
      "507 [C loss: 1.171728] [G loss: 0.615501]\n",
      "508 [C loss: 1.213494] [G loss: 0.615005]\n",
      "509 [C loss: 1.194152] [G loss: 0.616089]\n",
      "510 [C loss: 1.217986] [G loss: 0.612725]\n",
      "511 [C loss: 1.203334] [G loss: 0.613155]\n",
      "512 [C loss: 1.171634] [G loss: 0.616987]\n",
      "513 [C loss: 1.167663] [G loss: 0.614310]\n",
      "514 [C loss: 1.224797] [G loss: 0.613931]\n",
      "515 [C loss: 1.201985] [G loss: 0.614027]\n",
      "516 [C loss: 1.177038] [G loss: 0.613984]\n",
      "517 [C loss: 1.169700] [G loss: 0.614174]\n",
      "518 [C loss: 1.194891] [G loss: 0.615200]\n",
      "519 [C loss: 1.178076] [G loss: 0.616761]\n",
      "520 [C loss: 1.174636] [G loss: 0.613223]\n",
      "521 [C loss: 1.196508] [G loss: 0.613993]\n",
      "522 [C loss: 1.191162] [G loss: 0.612859]\n",
      "523 [C loss: 1.159502] [G loss: 0.614206]\n",
      "524 [C loss: 1.171903] [G loss: 0.615821]\n",
      "525 [C loss: 1.166012] [G loss: 0.613722]\n",
      "526 [C loss: 1.221151] [G loss: 0.612974]\n",
      "527 [C loss: 1.195784] [G loss: 0.613547]\n",
      "528 [C loss: 1.198460] [G loss: 0.610784]\n",
      "529 [C loss: 1.180549] [G loss: 0.613426]\n",
      "530 [C loss: 1.201554] [G loss: 0.613007]\n",
      "531 [C loss: 1.190802] [G loss: 0.613776]\n",
      "532 [C loss: 1.205620] [G loss: 0.616657]\n",
      "533 [C loss: 1.172778] [G loss: 0.613618]\n",
      "534 [C loss: 1.183639] [G loss: 0.614423]\n",
      "535 [C loss: 1.200580] [G loss: 0.613001]\n",
      "536 [C loss: 1.211806] [G loss: 0.615262]\n",
      "537 [C loss: 1.160578] [G loss: 0.614048]\n",
      "538 [C loss: 1.228255] [G loss: 0.613906]\n",
      "539 [C loss: 1.191443] [G loss: 0.617788]\n",
      "540 [C loss: 1.201742] [G loss: 0.616036]\n",
      "541 [C loss: 1.222346] [G loss: 0.613394]\n",
      "542 [C loss: 1.173573] [G loss: 0.614855]\n",
      "543 [C loss: 1.201318] [G loss: 0.616189]\n",
      "544 [C loss: 1.193287] [G loss: 0.616016]\n",
      "545 [C loss: 1.204031] [G loss: 0.617244]\n",
      "546 [C loss: 1.191536] [G loss: 0.617667]\n",
      "547 [C loss: 1.187695] [G loss: 0.617087]\n",
      "548 [C loss: 1.192213] [G loss: 0.613459]\n",
      "549 [C loss: 1.155919] [G loss: 0.614468]\n",
      "550 [C loss: 1.175004] [G loss: 0.617276]\n",
      "551 [C loss: 1.184648] [G loss: 0.614082]\n",
      "552 [C loss: 1.168561] [G loss: 0.614274]\n",
      "553 [C loss: 1.179743] [G loss: 0.617149]\n",
      "554 [C loss: 1.199489] [G loss: 0.617650]\n",
      "555 [C loss: 1.178381] [G loss: 0.614720]\n",
      "556 [C loss: 1.178695] [G loss: 0.616521]\n",
      "557 [C loss: 1.175115] [G loss: 0.619059]\n",
      "558 [C loss: 1.200051] [G loss: 0.615497]\n",
      "559 [C loss: 1.165436] [G loss: 0.617938]\n",
      "560 [C loss: 1.180716] [G loss: 0.612708]\n",
      "561 [C loss: 1.179119] [G loss: 0.611276]\n",
      "562 [C loss: 1.178569] [G loss: 0.615911]\n",
      "563 [C loss: 1.197923] [G loss: 0.615413]\n",
      "564 [C loss: 1.175760] [G loss: 0.612583]\n",
      "565 [C loss: 1.191613] [G loss: 0.616056]\n",
      "566 [C loss: 1.185921] [G loss: 0.618033]\n",
      "567 [C loss: 1.183401] [G loss: 0.614776]\n",
      "568 [C loss: 1.200227] [G loss: 0.614312]\n",
      "569 [C loss: 1.199231] [G loss: 0.613435]\n",
      "570 [C loss: 1.193270] [G loss: 0.615573]\n",
      "571 [C loss: 1.211662] [G loss: 0.614485]\n",
      "572 [C loss: 1.216623] [G loss: 0.613412]\n",
      "573 [C loss: 1.202680] [G loss: 0.612024]\n",
      "574 [C loss: 1.229580] [G loss: 0.615390]\n",
      "575 [C loss: 1.164463] [G loss: 0.616374]\n",
      "576 [C loss: 1.235185] [G loss: 0.613775]\n",
      "577 [C loss: 1.226148] [G loss: 0.612754]\n",
      "578 [C loss: 1.223501] [G loss: 0.613764]\n",
      "579 [C loss: 1.220887] [G loss: 0.615383]\n",
      "580 [C loss: 1.191677] [G loss: 0.611527]\n",
      "581 [C loss: 1.257378] [G loss: 0.616640]\n",
      "582 [C loss: 1.246467] [G loss: 0.614251]\n",
      "583 [C loss: 1.217822] [G loss: 0.613567]\n",
      "584 [C loss: 1.214984] [G loss: 0.612005]\n",
      "585 [C loss: 1.244845] [G loss: 0.614521]\n",
      "586 [C loss: 1.252780] [G loss: 0.613492]\n",
      "587 [C loss: 1.244044] [G loss: 0.613228]\n",
      "588 [C loss: 1.280187] [G loss: 0.613173]\n",
      "589 [C loss: 1.238261] [G loss: 0.614778]\n",
      "590 [C loss: 1.236512] [G loss: 0.614647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [C loss: 1.230583] [G loss: 0.615198]\n",
      "592 [C loss: 1.247595] [G loss: 0.611361]\n",
      "593 [C loss: 1.250569] [G loss: 0.614429]\n",
      "594 [C loss: 1.241164] [G loss: 0.612477]\n",
      "595 [C loss: 1.247645] [G loss: 0.614337]\n",
      "596 [C loss: 1.222721] [G loss: 0.611082]\n",
      "597 [C loss: 1.240395] [G loss: 0.612036]\n",
      "598 [C loss: 1.248802] [G loss: 0.611874]\n",
      "599 [C loss: 1.228392] [G loss: 0.611937]\n",
      "600 [C loss: 1.235847] [G loss: 0.609261]\n",
      "601 [C loss: 1.198141] [G loss: 0.610017]\n",
      "602 [C loss: 1.186047] [G loss: 0.610254]\n",
      "603 [C loss: 1.184451] [G loss: 0.608402]\n",
      "604 [C loss: 1.202771] [G loss: 0.604830]\n",
      "605 [C loss: 1.185530] [G loss: 0.608380]\n",
      "606 [C loss: 1.201447] [G loss: 0.606302]\n",
      "607 [C loss: 1.237316] [G loss: 0.606061]\n",
      "608 [C loss: 1.192293] [G loss: 0.604511]\n",
      "609 [C loss: 1.202302] [G loss: 0.602270]\n",
      "610 [C loss: 1.175618] [G loss: 0.603850]\n",
      "611 [C loss: 1.187757] [G loss: 0.601997]\n",
      "612 [C loss: 1.152892] [G loss: 0.600354]\n",
      "613 [C loss: 1.165831] [G loss: 0.599790]\n",
      "614 [C loss: 1.175818] [G loss: 0.599906]\n",
      "615 [C loss: 1.170169] [G loss: 0.598095]\n",
      "616 [C loss: 1.187229] [G loss: 0.598952]\n",
      "617 [C loss: 1.250243] [G loss: 0.600240]\n",
      "618 [C loss: 1.199712] [G loss: 0.601035]\n",
      "619 [C loss: 1.213139] [G loss: 0.599290]\n",
      "620 [C loss: 1.181590] [G loss: 0.601200]\n",
      "621 [C loss: 1.208208] [G loss: 0.599980]\n",
      "622 [C loss: 1.199831] [G loss: 0.600859]\n",
      "623 [C loss: 1.170036] [G loss: 0.601482]\n",
      "624 [C loss: 1.226532] [G loss: 0.600618]\n",
      "625 [C loss: 1.231514] [G loss: 0.601645]\n",
      "626 [C loss: 1.224994] [G loss: 0.601157]\n",
      "627 [C loss: 1.200499] [G loss: 0.599678]\n",
      "628 [C loss: 1.226087] [G loss: 0.599416]\n",
      "629 [C loss: 1.186447] [G loss: 0.600383]\n",
      "630 [C loss: 1.195404] [G loss: 0.600956]\n",
      "631 [C loss: 1.221166] [G loss: 0.600012]\n",
      "632 [C loss: 1.213135] [G loss: 0.600835]\n",
      "633 [C loss: 1.242685] [G loss: 0.602060]\n",
      "634 [C loss: 1.228536] [G loss: 0.601847]\n",
      "635 [C loss: 1.169399] [G loss: 0.601062]\n",
      "636 [C loss: 1.184136] [G loss: 0.600695]\n",
      "637 [C loss: 1.208820] [G loss: 0.600701]\n",
      "638 [C loss: 1.185955] [G loss: 0.599213]\n",
      "639 [C loss: 1.202919] [G loss: 0.597328]\n",
      "640 [C loss: 1.153895] [G loss: 0.596307]\n",
      "641 [C loss: 1.206797] [G loss: 0.595719]\n",
      "642 [C loss: 1.199867] [G loss: 0.598976]\n",
      "643 [C loss: 1.160042] [G loss: 0.597551]\n",
      "644 [C loss: 1.206292] [G loss: 0.596952]\n",
      "645 [C loss: 1.142567] [G loss: 0.596285]\n",
      "646 [C loss: 1.223635] [G loss: 0.597499]\n",
      "647 [C loss: 1.186800] [G loss: 0.595781]\n",
      "648 [C loss: 1.242098] [G loss: 0.596645]\n",
      "649 [C loss: 1.164904] [G loss: 0.595910]\n",
      "650 [C loss: 1.189368] [G loss: 0.595132]\n",
      "651 [C loss: 1.216540] [G loss: 0.594796]\n",
      "652 [C loss: 1.165276] [G loss: 0.597679]\n",
      "653 [C loss: 1.182204] [G loss: 0.592918]\n",
      "654 [C loss: 1.179796] [G loss: 0.595855]\n",
      "655 [C loss: 1.204417] [G loss: 0.594350]\n",
      "656 [C loss: 1.174955] [G loss: 0.595381]\n",
      "657 [C loss: 1.160398] [G loss: 0.597837]\n",
      "658 [C loss: 1.163819] [G loss: 0.595032]\n",
      "659 [C loss: 1.212120] [G loss: 0.596260]\n",
      "660 [C loss: 1.186031] [G loss: 0.597687]\n",
      "661 [C loss: 1.236960] [G loss: 0.595352]\n",
      "662 [C loss: 1.204153] [G loss: 0.597247]\n",
      "663 [C loss: 1.182661] [G loss: 0.598225]\n",
      "664 [C loss: 1.192906] [G loss: 0.594735]\n",
      "665 [C loss: 1.188613] [G loss: 0.597193]\n",
      "666 [C loss: 1.159076] [G loss: 0.596260]\n",
      "667 [C loss: 1.202693] [G loss: 0.593580]\n",
      "668 [C loss: 1.167666] [G loss: 0.592854]\n",
      "669 [C loss: 1.126170] [G loss: 0.595326]\n",
      "670 [C loss: 1.191169] [G loss: 0.596129]\n",
      "671 [C loss: 1.180536] [G loss: 0.595655]\n",
      "672 [C loss: 1.191108] [G loss: 0.593764]\n",
      "673 [C loss: 1.180961] [G loss: 0.596664]\n",
      "674 [C loss: 1.192235] [G loss: 0.592502]\n",
      "675 [C loss: 1.203341] [G loss: 0.598411]\n",
      "676 [C loss: 1.197444] [G loss: 0.596032]\n",
      "677 [C loss: 1.178536] [G loss: 0.594624]\n",
      "678 [C loss: 1.171261] [G loss: 0.596028]\n",
      "679 [C loss: 1.173676] [G loss: 0.595062]\n",
      "680 [C loss: 1.181441] [G loss: 0.596523]\n",
      "681 [C loss: 1.185236] [G loss: 0.598595]\n",
      "682 [C loss: 1.165125] [G loss: 0.596368]\n",
      "683 [C loss: 1.159082] [G loss: 0.595832]\n",
      "684 [C loss: 1.203706] [G loss: 0.593939]\n",
      "685 [C loss: 1.189888] [G loss: 0.596251]\n",
      "686 [C loss: 1.226200] [G loss: 0.592461]\n",
      "687 [C loss: 1.180470] [G loss: 0.592129]\n",
      "688 [C loss: 1.172221] [G loss: 0.593776]\n",
      "689 [C loss: 1.201128] [G loss: 0.593574]\n",
      "690 [C loss: 1.198266] [G loss: 0.594276]\n",
      "691 [C loss: 1.186203] [G loss: 0.593762]\n",
      "692 [C loss: 1.196437] [G loss: 0.595009]\n",
      "693 [C loss: 1.171146] [G loss: 0.594206]\n",
      "694 [C loss: 1.188445] [G loss: 0.594378]\n",
      "695 [C loss: 1.229330] [G loss: 0.597245]\n",
      "696 [C loss: 1.159901] [G loss: 0.596218]\n",
      "697 [C loss: 1.192330] [G loss: 0.594723]\n",
      "698 [C loss: 1.176413] [G loss: 0.596245]\n",
      "699 [C loss: 1.166751] [G loss: 0.594607]\n",
      "700 [C loss: 1.194160] [G loss: 0.595188]\n",
      "701 [C loss: 1.198368] [G loss: 0.596761]\n",
      "702 [C loss: 1.195757] [G loss: 0.599006]\n",
      "703 [C loss: 1.210733] [G loss: 0.598698]\n",
      "704 [C loss: 1.198015] [G loss: 0.599854]\n",
      "705 [C loss: 1.198381] [G loss: 0.601322]\n",
      "706 [C loss: 1.211711] [G loss: 0.596979]\n",
      "707 [C loss: 1.227213] [G loss: 0.600846]\n",
      "708 [C loss: 1.179604] [G loss: 0.602171]\n",
      "709 [C loss: 1.214661] [G loss: 0.603139]\n",
      "710 [C loss: 1.244071] [G loss: 0.606367]\n",
      "711 [C loss: 1.239806] [G loss: 0.605163]\n",
      "712 [C loss: 1.167669] [G loss: 0.606900]\n",
      "713 [C loss: 1.267225] [G loss: 0.604519]\n",
      "714 [C loss: 1.227704] [G loss: 0.607316]\n",
      "715 [C loss: 1.259572] [G loss: 0.608253]\n",
      "716 [C loss: 1.255126] [G loss: 0.608655]\n",
      "717 [C loss: 1.248751] [G loss: 0.605059]\n",
      "718 [C loss: 1.240345] [G loss: 0.608381]\n",
      "719 [C loss: 1.212444] [G loss: 0.607191]\n",
      "720 [C loss: 1.192472] [G loss: 0.604688]\n",
      "721 [C loss: 1.189631] [G loss: 0.609852]\n",
      "722 [C loss: 1.203536] [G loss: 0.607837]\n",
      "723 [C loss: 1.230382] [G loss: 0.604358]\n",
      "724 [C loss: 1.223318] [G loss: 0.603794]\n",
      "725 [C loss: 1.261169] [G loss: 0.602304]\n",
      "726 [C loss: 1.240754] [G loss: 0.603840]\n",
      "727 [C loss: 1.194103] [G loss: 0.599092]\n",
      "728 [C loss: 1.200460] [G loss: 0.600235]\n",
      "729 [C loss: 1.201509] [G loss: 0.596871]\n",
      "730 [C loss: 1.215733] [G loss: 0.599697]\n",
      "731 [C loss: 1.219166] [G loss: 0.597807]\n",
      "732 [C loss: 1.228954] [G loss: 0.599313]\n",
      "733 [C loss: 1.216213] [G loss: 0.598939]\n",
      "734 [C loss: 1.202516] [G loss: 0.598687]\n",
      "735 [C loss: 1.226068] [G loss: 0.599955]\n",
      "736 [C loss: 1.225384] [G loss: 0.597974]\n",
      "737 [C loss: 1.216831] [G loss: 0.598428]\n",
      "738 [C loss: 1.217248] [G loss: 0.598545]\n",
      "739 [C loss: 1.209359] [G loss: 0.599584]\n",
      "740 [C loss: 1.199041] [G loss: 0.598955]\n",
      "741 [C loss: 1.156399] [G loss: 0.602066]\n",
      "742 [C loss: 1.213532] [G loss: 0.599586]\n",
      "743 [C loss: 1.251967] [G loss: 0.600900]\n",
      "744 [C loss: 1.194504] [G loss: 0.600473]\n",
      "745 [C loss: 1.214916] [G loss: 0.600904]\n",
      "746 [C loss: 1.224243] [G loss: 0.601639]\n",
      "747 [C loss: 1.221215] [G loss: 0.599981]\n",
      "748 [C loss: 1.218468] [G loss: 0.600975]\n",
      "749 [C loss: 1.200063] [G loss: 0.601724]\n",
      "750 [C loss: 1.234986] [G loss: 0.601819]\n",
      "751 [C loss: 1.209527] [G loss: 0.600914]\n",
      "752 [C loss: 1.204664] [G loss: 0.602409]\n",
      "753 [C loss: 1.196965] [G loss: 0.600156]\n",
      "754 [C loss: 1.248647] [G loss: 0.601112]\n",
      "755 [C loss: 1.173293] [G loss: 0.599610]\n",
      "756 [C loss: 1.222865] [G loss: 0.600277]\n",
      "757 [C loss: 1.233836] [G loss: 0.598816]\n",
      "758 [C loss: 1.186855] [G loss: 0.598445]\n",
      "759 [C loss: 1.216507] [G loss: 0.598563]\n",
      "760 [C loss: 1.204242] [G loss: 0.597715]\n",
      "761 [C loss: 1.216509] [G loss: 0.597558]\n",
      "762 [C loss: 1.204613] [G loss: 0.597727]\n",
      "763 [C loss: 1.175021] [G loss: 0.597256]\n",
      "764 [C loss: 1.194646] [G loss: 0.596760]\n",
      "765 [C loss: 1.239417] [G loss: 0.597796]\n",
      "766 [C loss: 1.206767] [G loss: 0.596622]\n",
      "767 [C loss: 1.180047] [G loss: 0.596330]\n",
      "768 [C loss: 1.170048] [G loss: 0.595852]\n",
      "769 [C loss: 1.221251] [G loss: 0.597330]\n",
      "770 [C loss: 1.188855] [G loss: 0.597682]\n",
      "771 [C loss: 1.226001] [G loss: 0.596264]\n",
      "772 [C loss: 1.208284] [G loss: 0.597218]\n",
      "773 [C loss: 1.192701] [G loss: 0.596971]\n",
      "774 [C loss: 1.214964] [G loss: 0.596898]\n",
      "775 [C loss: 1.213930] [G loss: 0.597293]\n",
      "776 [C loss: 1.199504] [G loss: 0.598781]\n",
      "777 [C loss: 1.220676] [G loss: 0.596743]\n",
      "778 [C loss: 1.221104] [G loss: 0.597564]\n",
      "779 [C loss: 1.186742] [G loss: 0.596965]\n",
      "780 [C loss: 1.224056] [G loss: 0.596736]\n",
      "781 [C loss: 1.173245] [G loss: 0.596681]\n",
      "782 [C loss: 1.185374] [G loss: 0.597939]\n",
      "783 [C loss: 1.217999] [G loss: 0.596754]\n",
      "784 [C loss: 1.196953] [G loss: 0.597253]\n",
      "785 [C loss: 1.197430] [G loss: 0.597341]\n",
      "786 [C loss: 1.184284] [G loss: 0.597465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787 [C loss: 1.182673] [G loss: 0.598517]\n",
      "788 [C loss: 1.173946] [G loss: 0.595949]\n",
      "789 [C loss: 1.230877] [G loss: 0.596565]\n",
      "790 [C loss: 1.244332] [G loss: 0.596270]\n",
      "791 [C loss: 1.208407] [G loss: 0.596227]\n",
      "792 [C loss: 1.247495] [G loss: 0.597222]\n",
      "793 [C loss: 1.227332] [G loss: 0.594970]\n",
      "794 [C loss: 1.237584] [G loss: 0.595783]\n",
      "795 [C loss: 1.183408] [G loss: 0.597291]\n",
      "796 [C loss: 1.156683] [G loss: 0.596918]\n",
      "797 [C loss: 1.232455] [G loss: 0.597050]\n",
      "798 [C loss: 1.231256] [G loss: 0.596322]\n",
      "799 [C loss: 1.201827] [G loss: 0.595361]\n",
      "800 [C loss: 1.215535] [G loss: 0.595203]\n",
      "801 [C loss: 1.215014] [G loss: 0.596422]\n",
      "802 [C loss: 1.208579] [G loss: 0.596378]\n",
      "803 [C loss: 1.221024] [G loss: 0.596063]\n",
      "804 [C loss: 1.233395] [G loss: 0.596568]\n",
      "805 [C loss: 1.207743] [G loss: 0.597762]\n",
      "806 [C loss: 1.175224] [G loss: 0.599318]\n",
      "807 [C loss: 1.201460] [G loss: 0.597158]\n",
      "808 [C loss: 1.197478] [G loss: 0.598060]\n",
      "809 [C loss: 1.213081] [G loss: 0.598494]\n",
      "810 [C loss: 1.201293] [G loss: 0.598288]\n",
      "811 [C loss: 1.204928] [G loss: 0.597678]\n",
      "812 [C loss: 1.222163] [G loss: 0.597988]\n",
      "813 [C loss: 1.223020] [G loss: 0.596236]\n",
      "814 [C loss: 1.159570] [G loss: 0.598405]\n",
      "815 [C loss: 1.234930] [G loss: 0.597267]\n",
      "816 [C loss: 1.226662] [G loss: 0.600154]\n",
      "817 [C loss: 1.214063] [G loss: 0.599178]\n",
      "818 [C loss: 1.210686] [G loss: 0.597185]\n",
      "819 [C loss: 1.222150] [G loss: 0.597573]\n",
      "820 [C loss: 1.227892] [G loss: 0.596878]\n",
      "821 [C loss: 1.186056] [G loss: 0.597942]\n",
      "822 [C loss: 1.242023] [G loss: 0.596859]\n",
      "823 [C loss: 1.254056] [G loss: 0.597962]\n",
      "824 [C loss: 1.239145] [G loss: 0.597344]\n",
      "825 [C loss: 1.215422] [G loss: 0.596717]\n",
      "826 [C loss: 1.224683] [G loss: 0.598393]\n",
      "827 [C loss: 1.211397] [G loss: 0.595376]\n",
      "828 [C loss: 1.186865] [G loss: 0.596027]\n",
      "829 [C loss: 1.200069] [G loss: 0.597049]\n",
      "830 [C loss: 1.212760] [G loss: 0.597097]\n",
      "831 [C loss: 1.202668] [G loss: 0.596292]\n",
      "832 [C loss: 1.209775] [G loss: 0.599877]\n",
      "833 [C loss: 1.218709] [G loss: 0.598265]\n",
      "834 [C loss: 1.219208] [G loss: 0.599084]\n",
      "835 [C loss: 1.213153] [G loss: 0.598716]\n",
      "836 [C loss: 1.208788] [G loss: 0.599886]\n",
      "837 [C loss: 1.203288] [G loss: 0.599082]\n",
      "838 [C loss: 1.213105] [G loss: 0.597727]\n",
      "839 [C loss: 1.215331] [G loss: 0.598918]\n",
      "840 [C loss: 1.210617] [G loss: 0.598505]\n",
      "841 [C loss: 1.206265] [G loss: 0.598010]\n",
      "842 [C loss: 1.203389] [G loss: 0.598738]\n",
      "843 [C loss: 1.205785] [G loss: 0.598903]\n",
      "844 [C loss: 1.198353] [G loss: 0.596580]\n",
      "845 [C loss: 1.204680] [G loss: 0.599486]\n",
      "846 [C loss: 1.235244] [G loss: 0.598083]\n",
      "847 [C loss: 1.232432] [G loss: 0.598020]\n",
      "848 [C loss: 1.222181] [G loss: 0.601638]\n",
      "849 [C loss: 1.223858] [G loss: 0.598567]\n",
      "850 [C loss: 1.238955] [G loss: 0.596834]\n",
      "851 [C loss: 1.167785] [G loss: 0.598626]\n",
      "852 [C loss: 1.227426] [G loss: 0.596359]\n",
      "853 [C loss: 1.167562] [G loss: 0.599727]\n",
      "854 [C loss: 1.206629] [G loss: 0.599647]\n",
      "855 [C loss: 1.202350] [G loss: 0.595598]\n",
      "856 [C loss: 1.212741] [G loss: 0.598186]\n",
      "857 [C loss: 1.223666] [G loss: 0.599356]\n",
      "858 [C loss: 1.190044] [G loss: 0.597455]\n",
      "859 [C loss: 1.239891] [G loss: 0.599531]\n",
      "860 [C loss: 1.207221] [G loss: 0.599635]\n",
      "861 [C loss: 1.240638] [G loss: 0.596926]\n",
      "862 [C loss: 1.207703] [G loss: 0.597027]\n",
      "863 [C loss: 1.200833] [G loss: 0.598218]\n",
      "864 [C loss: 1.230683] [G loss: 0.597511]\n",
      "865 [C loss: 1.225993] [G loss: 0.600710]\n",
      "866 [C loss: 1.246446] [G loss: 0.598652]\n",
      "867 [C loss: 1.269512] [G loss: 0.599318]\n",
      "868 [C loss: 1.221772] [G loss: 0.600253]\n",
      "869 [C loss: 1.238139] [G loss: 0.597979]\n",
      "870 [C loss: 1.233384] [G loss: 0.599991]\n",
      "871 [C loss: 1.249630] [G loss: 0.598756]\n",
      "872 [C loss: 1.227683] [G loss: 0.600557]\n",
      "873 [C loss: 1.211426] [G loss: 0.601891]\n",
      "874 [C loss: 1.237954] [G loss: 0.598311]\n",
      "875 [C loss: 1.264790] [G loss: 0.598443]\n",
      "876 [C loss: 1.201520] [G loss: 0.600070]\n",
      "877 [C loss: 1.216105] [G loss: 0.600360]\n",
      "878 [C loss: 1.203981] [G loss: 0.601080]\n",
      "879 [C loss: 1.229147] [G loss: 0.602039]\n",
      "880 [C loss: 1.240852] [G loss: 0.601237]\n",
      "881 [C loss: 1.215330] [G loss: 0.601763]\n",
      "882 [C loss: 1.232956] [G loss: 0.601419]\n",
      "883 [C loss: 1.200274] [G loss: 0.602230]\n",
      "884 [C loss: 1.221538] [G loss: 0.599577]\n",
      "885 [C loss: 1.230174] [G loss: 0.601125]\n",
      "886 [C loss: 1.203375] [G loss: 0.602483]\n",
      "887 [C loss: 1.244385] [G loss: 0.599351]\n",
      "888 [C loss: 1.215360] [G loss: 0.600448]\n",
      "889 [C loss: 1.214717] [G loss: 0.599748]\n",
      "890 [C loss: 1.189340] [G loss: 0.600793]\n",
      "891 [C loss: 1.227892] [G loss: 0.603034]\n",
      "892 [C loss: 1.169808] [G loss: 0.601835]\n",
      "893 [C loss: 1.210616] [G loss: 0.599166]\n",
      "894 [C loss: 1.233292] [G loss: 0.604167]\n",
      "895 [C loss: 1.227481] [G loss: 0.602437]\n",
      "896 [C loss: 1.227039] [G loss: 0.602037]\n",
      "897 [C loss: 1.224136] [G loss: 0.601132]\n",
      "898 [C loss: 1.218941] [G loss: 0.600499]\n",
      "899 [C loss: 1.237159] [G loss: 0.599862]\n",
      "900 [C loss: 1.224321] [G loss: 0.599063]\n",
      "901 [C loss: 1.274392] [G loss: 0.600688]\n",
      "902 [C loss: 1.237566] [G loss: 0.599387]\n",
      "903 [C loss: 1.256021] [G loss: 0.599010]\n",
      "904 [C loss: 1.267138] [G loss: 0.600939]\n",
      "905 [C loss: 1.177633] [G loss: 0.599189]\n",
      "906 [C loss: 1.221687] [G loss: 0.599804]\n",
      "907 [C loss: 1.244825] [G loss: 0.602982]\n",
      "908 [C loss: 1.236931] [G loss: 0.602321]\n",
      "909 [C loss: 1.234942] [G loss: 0.600167]\n",
      "910 [C loss: 1.223723] [G loss: 0.600582]\n",
      "911 [C loss: 1.217872] [G loss: 0.600896]\n",
      "912 [C loss: 1.282850] [G loss: 0.601160]\n",
      "913 [C loss: 1.199950] [G loss: 0.599243]\n",
      "914 [C loss: 1.217865] [G loss: 0.603610]\n",
      "915 [C loss: 1.203072] [G loss: 0.599920]\n",
      "916 [C loss: 1.217326] [G loss: 0.604859]\n",
      "917 [C loss: 1.164935] [G loss: 0.603812]\n",
      "918 [C loss: 1.225501] [G loss: 0.601863]\n",
      "919 [C loss: 1.169736] [G loss: 0.599587]\n",
      "920 [C loss: 1.215225] [G loss: 0.598429]\n",
      "921 [C loss: 1.187245] [G loss: 0.603114]\n",
      "922 [C loss: 1.227461] [G loss: 0.602361]\n",
      "923 [C loss: 1.230314] [G loss: 0.601618]\n",
      "924 [C loss: 1.218935] [G loss: 0.601799]\n",
      "925 [C loss: 1.209157] [G loss: 0.597892]\n",
      "926 [C loss: 1.227682] [G loss: 0.598675]\n",
      "927 [C loss: 1.222990] [G loss: 0.599214]\n",
      "928 [C loss: 1.242386] [G loss: 0.601452]\n",
      "929 [C loss: 1.180403] [G loss: 0.599359]\n",
      "930 [C loss: 1.219443] [G loss: 0.598407]\n",
      "931 [C loss: 1.204087] [G loss: 0.599159]\n",
      "932 [C loss: 1.200971] [G loss: 0.600568]\n",
      "933 [C loss: 1.221427] [G loss: 0.600445]\n",
      "934 [C loss: 1.197214] [G loss: 0.599899]\n",
      "935 [C loss: 1.181469] [G loss: 0.599203]\n",
      "936 [C loss: 1.219743] [G loss: 0.598991]\n",
      "937 [C loss: 1.202909] [G loss: 0.598755]\n",
      "938 [C loss: 1.230107] [G loss: 0.599021]\n",
      "939 [C loss: 1.195497] [G loss: 0.598322]\n",
      "940 [C loss: 1.171182] [G loss: 0.600680]\n",
      "941 [C loss: 1.203093] [G loss: 0.600645]\n",
      "942 [C loss: 1.216320] [G loss: 0.601560]\n",
      "943 [C loss: 1.265508] [G loss: 0.602199]\n",
      "944 [C loss: 1.233941] [G loss: 0.602324]\n",
      "945 [C loss: 1.240865] [G loss: 0.599939]\n",
      "946 [C loss: 1.235996] [G loss: 0.600231]\n",
      "947 [C loss: 1.232723] [G loss: 0.605346]\n",
      "948 [C loss: 1.234273] [G loss: 0.600598]\n",
      "949 [C loss: 1.248103] [G loss: 0.603157]\n",
      "950 [C loss: 1.223606] [G loss: 0.603683]\n",
      "951 [C loss: 1.214264] [G loss: 0.604285]\n",
      "952 [C loss: 1.270328] [G loss: 0.603282]\n",
      "953 [C loss: 1.215951] [G loss: 0.604495]\n",
      "954 [C loss: 1.237593] [G loss: 0.608037]\n",
      "955 [C loss: 1.198257] [G loss: 0.604267]\n",
      "956 [C loss: 1.220328] [G loss: 0.607652]\n",
      "957 [C loss: 1.262170] [G loss: 0.604928]\n",
      "958 [C loss: 1.239465] [G loss: 0.603418]\n",
      "959 [C loss: 1.212962] [G loss: 0.609015]\n",
      "960 [C loss: 1.248598] [G loss: 0.610333]\n",
      "961 [C loss: 1.205417] [G loss: 0.610330]\n",
      "962 [C loss: 1.256377] [G loss: 0.609385]\n",
      "963 [C loss: 1.240218] [G loss: 0.609510]\n",
      "964 [C loss: 1.204714] [G loss: 0.610124]\n",
      "965 [C loss: 1.232055] [G loss: 0.610735]\n",
      "966 [C loss: 1.259804] [G loss: 0.607557]\n",
      "967 [C loss: 1.262170] [G loss: 0.610027]\n",
      "968 [C loss: 1.242402] [G loss: 0.609531]\n",
      "969 [C loss: 1.261492] [G loss: 0.610422]\n",
      "970 [C loss: 1.253178] [G loss: 0.610780]\n",
      "971 [C loss: 1.211034] [G loss: 0.610049]\n",
      "972 [C loss: 1.209214] [G loss: 0.607860]\n",
      "973 [C loss: 1.259176] [G loss: 0.609547]\n",
      "974 [C loss: 1.275431] [G loss: 0.609261]\n",
      "975 [C loss: 1.250780] [G loss: 0.610744]\n",
      "976 [C loss: 1.228204] [G loss: 0.611278]\n",
      "977 [C loss: 1.200772] [G loss: 0.609063]\n",
      "978 [C loss: 1.272056] [G loss: 0.610474]\n",
      "979 [C loss: 1.283086] [G loss: 0.609321]\n",
      "980 [C loss: 1.251588] [G loss: 0.611343]\n",
      "981 [C loss: 1.249260] [G loss: 0.608765]\n",
      "982 [C loss: 1.268872] [G loss: 0.609783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 [C loss: 1.221638] [G loss: 0.605105]\n",
      "984 [C loss: 1.187890] [G loss: 0.607070]\n",
      "985 [C loss: 1.218799] [G loss: 0.607740]\n",
      "986 [C loss: 1.267716] [G loss: 0.610671]\n",
      "987 [C loss: 1.222787] [G loss: 0.612479]\n",
      "988 [C loss: 1.280317] [G loss: 0.610687]\n",
      "989 [C loss: 1.255935] [G loss: 0.608362]\n",
      "990 [C loss: 1.243675] [G loss: 0.611553]\n",
      "991 [C loss: 1.253761] [G loss: 0.608150]\n",
      "992 [C loss: 1.253801] [G loss: 0.608142]\n",
      "993 [C loss: 1.254948] [G loss: 0.611886]\n",
      "994 [C loss: 1.229728] [G loss: 0.606692]\n",
      "995 [C loss: 1.240099] [G loss: 0.608883]\n",
      "996 [C loss: 1.250936] [G loss: 0.611420]\n",
      "997 [C loss: 1.270787] [G loss: 0.608145]\n",
      "998 [C loss: 1.271960] [G loss: 0.606102]\n",
      "999 [C loss: 1.233629] [G loss: 0.608473]\n",
      "1000 [C loss: 1.261127] [G loss: 0.611657]\n",
      "1001 [C loss: 1.220443] [G loss: 0.609456]\n",
      "1002 [C loss: 1.240139] [G loss: 0.607351]\n",
      "1003 [C loss: 1.241197] [G loss: 0.607775]\n",
      "1004 [C loss: 1.243644] [G loss: 0.608301]\n",
      "1005 [C loss: 1.279077] [G loss: 0.610549]\n",
      "1006 [C loss: 1.234931] [G loss: 0.606965]\n",
      "1007 [C loss: 1.234744] [G loss: 0.602746]\n",
      "1008 [C loss: 1.251446] [G loss: 0.607729]\n",
      "1009 [C loss: 1.238260] [G loss: 0.610117]\n",
      "1010 [C loss: 1.254770] [G loss: 0.607384]\n",
      "1011 [C loss: 1.273900] [G loss: 0.608848]\n",
      "1012 [C loss: 1.222349] [G loss: 0.606532]\n",
      "1013 [C loss: 1.279921] [G loss: 0.607515]\n",
      "1014 [C loss: 1.251939] [G loss: 0.606822]\n",
      "1015 [C loss: 1.206249] [G loss: 0.608142]\n",
      "1016 [C loss: 1.233299] [G loss: 0.606155]\n",
      "1017 [C loss: 1.209460] [G loss: 0.606614]\n",
      "1018 [C loss: 1.244548] [G loss: 0.608184]\n",
      "1019 [C loss: 1.218681] [G loss: 0.606476]\n",
      "1020 [C loss: 1.238053] [G loss: 0.608972]\n",
      "1021 [C loss: 1.237989] [G loss: 0.607641]\n",
      "1022 [C loss: 1.254137] [G loss: 0.608111]\n",
      "1023 [C loss: 1.241045] [G loss: 0.608255]\n",
      "1024 [C loss: 1.287380] [G loss: 0.609181]\n",
      "1025 [C loss: 1.255370] [G loss: 0.608754]\n",
      "1026 [C loss: 1.264760] [G loss: 0.609382]\n",
      "1027 [C loss: 1.268786] [G loss: 0.607228]\n",
      "1028 [C loss: 1.260716] [G loss: 0.607774]\n",
      "1029 [C loss: 1.244332] [G loss: 0.612643]\n",
      "1030 [C loss: 1.241612] [G loss: 0.611105]\n",
      "1031 [C loss: 1.241219] [G loss: 0.609457]\n",
      "1032 [C loss: 1.259459] [G loss: 0.611796]\n",
      "1033 [C loss: 1.257075] [G loss: 0.609990]\n",
      "1034 [C loss: 1.245235] [G loss: 0.609999]\n",
      "1035 [C loss: 1.289328] [G loss: 0.610740]\n",
      "1036 [C loss: 1.271949] [G loss: 0.609298]\n",
      "1037 [C loss: 1.288220] [G loss: 0.611042]\n",
      "1038 [C loss: 1.249706] [G loss: 0.611760]\n",
      "1039 [C loss: 1.250421] [G loss: 0.610166]\n",
      "1040 [C loss: 1.284393] [G loss: 0.613535]\n",
      "1041 [C loss: 1.253042] [G loss: 0.609761]\n",
      "1042 [C loss: 1.215740] [G loss: 0.611956]\n",
      "1043 [C loss: 1.233899] [G loss: 0.612584]\n",
      "1044 [C loss: 1.257436] [G loss: 0.610111]\n",
      "1045 [C loss: 1.252928] [G loss: 0.611849]\n",
      "1046 [C loss: 1.263395] [G loss: 0.610849]\n",
      "1047 [C loss: 1.230646] [G loss: 0.609838]\n",
      "1048 [C loss: 1.247459] [G loss: 0.610257]\n",
      "1049 [C loss: 1.266957] [G loss: 0.608985]\n",
      "1050 [C loss: 1.263402] [G loss: 0.613618]\n",
      "1051 [C loss: 1.265235] [G loss: 0.614504]\n",
      "1052 [C loss: 1.261801] [G loss: 0.613993]\n",
      "1053 [C loss: 1.226443] [G loss: 0.610669]\n",
      "1054 [C loss: 1.273138] [G loss: 0.612388]\n",
      "1055 [C loss: 1.273273] [G loss: 0.612776]\n",
      "1056 [C loss: 1.282672] [G loss: 0.613852]\n",
      "1057 [C loss: 1.287534] [G loss: 0.611057]\n",
      "1058 [C loss: 1.243776] [G loss: 0.613248]\n",
      "1059 [C loss: 1.288065] [G loss: 0.612851]\n",
      "1060 [C loss: 1.265845] [G loss: 0.615872]\n",
      "1061 [C loss: 1.297945] [G loss: 0.611681]\n",
      "1062 [C loss: 1.223174] [G loss: 0.614930]\n",
      "1063 [C loss: 1.284161] [G loss: 0.613457]\n",
      "1064 [C loss: 1.265420] [G loss: 0.617081]\n",
      "1065 [C loss: 1.266890] [G loss: 0.609925]\n",
      "1066 [C loss: 1.274557] [G loss: 0.612191]\n",
      "1067 [C loss: 1.284726] [G loss: 0.610763]\n",
      "1068 [C loss: 1.305850] [G loss: 0.612815]\n",
      "1069 [C loss: 1.321586] [G loss: 0.610167]\n",
      "1070 [C loss: 1.299199] [G loss: 0.612325]\n",
      "1071 [C loss: 1.298271] [G loss: 0.611859]\n",
      "1072 [C loss: 1.301441] [G loss: 0.611581]\n",
      "1073 [C loss: 1.281569] [G loss: 0.613682]\n",
      "1074 [C loss: 1.294565] [G loss: 0.609697]\n",
      "1075 [C loss: 1.298295] [G loss: 0.610926]\n",
      "1076 [C loss: 1.303408] [G loss: 0.611295]\n",
      "1077 [C loss: 1.302185] [G loss: 0.609754]\n",
      "1078 [C loss: 1.319757] [G loss: 0.612248]\n",
      "1079 [C loss: 1.307363] [G loss: 0.607288]\n",
      "1080 [C loss: 1.263404] [G loss: 0.611912]\n",
      "1081 [C loss: 1.322945] [G loss: 0.611034]\n",
      "1082 [C loss: 1.264036] [G loss: 0.609343]\n",
      "1083 [C loss: 1.297555] [G loss: 0.610194]\n",
      "1084 [C loss: 1.322934] [G loss: 0.611119]\n",
      "1085 [C loss: 1.296577] [G loss: 0.610105]\n",
      "1086 [C loss: 1.316279] [G loss: 0.608321]\n",
      "1087 [C loss: 1.331414] [G loss: 0.609248]\n",
      "1088 [C loss: 1.259758] [G loss: 0.608441]\n",
      "1089 [C loss: 1.321653] [G loss: 0.608592]\n",
      "1090 [C loss: 1.329683] [G loss: 0.611107]\n",
      "1091 [C loss: 1.321786] [G loss: 0.609010]\n",
      "1092 [C loss: 1.327844] [G loss: 0.606535]\n",
      "1093 [C loss: 1.277771] [G loss: 0.606520]\n",
      "1094 [C loss: 1.331005] [G loss: 0.607148]\n",
      "1095 [C loss: 1.350247] [G loss: 0.606261]\n",
      "1096 [C loss: 1.326820] [G loss: 0.608107]\n",
      "1097 [C loss: 1.327009] [G loss: 0.608818]\n",
      "1098 [C loss: 1.351560] [G loss: 0.607312]\n",
      "1099 [C loss: 1.321155] [G loss: 0.604739]\n",
      "1100 [C loss: 1.325511] [G loss: 0.605992]\n",
      "1101 [C loss: 1.333813] [G loss: 0.607954]\n",
      "1102 [C loss: 1.320948] [G loss: 0.605280]\n",
      "1103 [C loss: 1.313587] [G loss: 0.604559]\n",
      "1104 [C loss: 1.341483] [G loss: 0.605429]\n",
      "1105 [C loss: 1.347473] [G loss: 0.605564]\n",
      "1106 [C loss: 1.361203] [G loss: 0.604724]\n",
      "1107 [C loss: 1.333171] [G loss: 0.604707]\n",
      "1108 [C loss: 1.351241] [G loss: 0.604297]\n",
      "1109 [C loss: 1.340647] [G loss: 0.604492]\n",
      "1110 [C loss: 1.310574] [G loss: 0.603949]\n",
      "1111 [C loss: 1.312136] [G loss: 0.602167]\n",
      "1112 [C loss: 1.324830] [G loss: 0.603558]\n",
      "1113 [C loss: 1.314040] [G loss: 0.602262]\n",
      "1114 [C loss: 1.349136] [G loss: 0.601699]\n",
      "1115 [C loss: 1.354040] [G loss: 0.603371]\n",
      "1116 [C loss: 1.336430] [G loss: 0.602521]\n",
      "1117 [C loss: 1.309681] [G loss: 0.602178]\n",
      "1118 [C loss: 1.306546] [G loss: 0.605138]\n",
      "1119 [C loss: 1.307755] [G loss: 0.606824]\n",
      "1120 [C loss: 1.273669] [G loss: 0.602459]\n",
      "1121 [C loss: 1.338082] [G loss: 0.603574]\n",
      "1122 [C loss: 1.303589] [G loss: 0.601640]\n",
      "1123 [C loss: 1.332176] [G loss: 0.600446]\n",
      "1124 [C loss: 1.351524] [G loss: 0.606826]\n",
      "1125 [C loss: 1.320994] [G loss: 0.602239]\n",
      "1126 [C loss: 1.296066] [G loss: 0.596214]\n",
      "1127 [C loss: 1.332926] [G loss: 0.599544]\n",
      "1128 [C loss: 1.324134] [G loss: 0.599194]\n",
      "1129 [C loss: 1.330099] [G loss: 0.593713]\n",
      "1130 [C loss: 1.355432] [G loss: 0.599247]\n",
      "1131 [C loss: 1.299839] [G loss: 0.594061]\n",
      "1132 [C loss: 1.288266] [G loss: 0.593808]\n",
      "1133 [C loss: 1.339738] [G loss: 0.592091]\n",
      "1134 [C loss: 1.329129] [G loss: 0.591479]\n",
      "1135 [C loss: 1.317511] [G loss: 0.590443]\n",
      "1136 [C loss: 1.372764] [G loss: 0.590625]\n",
      "1137 [C loss: 1.338273] [G loss: 0.590559]\n",
      "1138 [C loss: 1.404186] [G loss: 0.587516]\n",
      "1139 [C loss: 1.320406] [G loss: 0.590730]\n",
      "1140 [C loss: 1.316723] [G loss: 0.589325]\n",
      "1141 [C loss: 1.310423] [G loss: 0.595589]\n",
      "1142 [C loss: 1.328296] [G loss: 0.593095]\n",
      "1143 [C loss: 1.310077] [G loss: 0.590948]\n",
      "1144 [C loss: 1.349648] [G loss: 0.592523]\n",
      "1145 [C loss: 1.322341] [G loss: 0.592654]\n",
      "1146 [C loss: 1.322640] [G loss: 0.591728]\n",
      "1147 [C loss: 1.351300] [G loss: 0.598568]\n",
      "1148 [C loss: 1.345358] [G loss: 0.593331]\n",
      "1149 [C loss: 1.365674] [G loss: 0.595580]\n",
      "1150 [C loss: 1.343057] [G loss: 0.595403]\n",
      "1151 [C loss: 1.333396] [G loss: 0.594229]\n",
      "1152 [C loss: 1.378839] [G loss: 0.594572]\n",
      "1153 [C loss: 1.383308] [G loss: 0.592587]\n",
      "1154 [C loss: 1.330620] [G loss: 0.592182]\n",
      "1155 [C loss: 1.313304] [G loss: 0.593325]\n",
      "1156 [C loss: 1.344550] [G loss: 0.592803]\n",
      "1157 [C loss: 1.335008] [G loss: 0.593081]\n",
      "1158 [C loss: 1.353112] [G loss: 0.593571]\n",
      "1159 [C loss: 1.375460] [G loss: 0.591874]\n",
      "1160 [C loss: 1.356108] [G loss: 0.592320]\n",
      "1161 [C loss: 1.350874] [G loss: 0.591907]\n",
      "1162 [C loss: 1.321416] [G loss: 0.591827]\n",
      "1163 [C loss: 1.359609] [G loss: 0.590429]\n",
      "1164 [C loss: 1.360306] [G loss: 0.591393]\n",
      "1165 [C loss: 1.348506] [G loss: 0.591786]\n",
      "1166 [C loss: 1.340462] [G loss: 0.592171]\n",
      "1167 [C loss: 1.377970] [G loss: 0.593306]\n",
      "1168 [C loss: 1.344688] [G loss: 0.591516]\n",
      "1169 [C loss: 1.301558] [G loss: 0.590361]\n",
      "1170 [C loss: 1.352820] [G loss: 0.589113]\n",
      "1171 [C loss: 1.313827] [G loss: 0.590419]\n",
      "1172 [C loss: 1.397799] [G loss: 0.586543]\n",
      "1173 [C loss: 1.308838] [G loss: 0.589152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174 [C loss: 1.335597] [G loss: 0.588969]\n",
      "1175 [C loss: 1.333192] [G loss: 0.588454]\n",
      "1176 [C loss: 1.307215] [G loss: 0.589037]\n",
      "1177 [C loss: 1.358552] [G loss: 0.586809]\n",
      "1178 [C loss: 1.344676] [G loss: 0.586657]\n",
      "1179 [C loss: 1.355948] [G loss: 0.587068]\n",
      "1180 [C loss: 1.331233] [G loss: 0.587570]\n",
      "1181 [C loss: 1.318292] [G loss: 0.587527]\n",
      "1182 [C loss: 1.360862] [G loss: 0.586848]\n",
      "1183 [C loss: 1.329904] [G loss: 0.585330]\n",
      "1184 [C loss: 1.339904] [G loss: 0.587894]\n",
      "1185 [C loss: 1.304279] [G loss: 0.586185]\n",
      "1186 [C loss: 1.314224] [G loss: 0.586210]\n",
      "1187 [C loss: 1.282005] [G loss: 0.584900]\n",
      "1188 [C loss: 1.317690] [G loss: 0.585411]\n",
      "1189 [C loss: 1.346822] [G loss: 0.587110]\n",
      "1190 [C loss: 1.305357] [G loss: 0.583852]\n",
      "1191 [C loss: 1.370957] [G loss: 0.586160]\n",
      "1192 [C loss: 1.398076] [G loss: 0.585315]\n",
      "1193 [C loss: 1.343447] [G loss: 0.584322]\n",
      "1194 [C loss: 1.342057] [G loss: 0.585368]\n",
      "1195 [C loss: 1.379256] [G loss: 0.584804]\n",
      "1196 [C loss: 1.342069] [G loss: 0.587407]\n",
      "1197 [C loss: 1.351795] [G loss: 0.584340]\n",
      "1198 [C loss: 1.332726] [G loss: 0.585236]\n",
      "1199 [C loss: 1.343934] [G loss: 0.583625]\n",
      "1200 [C loss: 1.362943] [G loss: 0.585565]\n",
      "1201 [C loss: 1.320375] [G loss: 0.585568]\n",
      "1202 [C loss: 1.361242] [G loss: 0.586010]\n",
      "1203 [C loss: 1.360958] [G loss: 0.584993]\n",
      "1204 [C loss: 1.313230] [G loss: 0.588257]\n",
      "1205 [C loss: 1.326462] [G loss: 0.584942]\n",
      "1206 [C loss: 1.355304] [G loss: 0.584302]\n",
      "1207 [C loss: 1.359457] [G loss: 0.586057]\n",
      "1208 [C loss: 1.327711] [G loss: 0.585238]\n",
      "1209 [C loss: 1.351686] [G loss: 0.582970]\n",
      "1210 [C loss: 1.343677] [G loss: 0.584880]\n",
      "1211 [C loss: 1.382753] [G loss: 0.584914]\n",
      "1212 [C loss: 1.346551] [G loss: 0.580728]\n",
      "1213 [C loss: 1.343130] [G loss: 0.582311]\n",
      "1214 [C loss: 1.357033] [G loss: 0.580373]\n",
      "1215 [C loss: 1.347068] [G loss: 0.580485]\n",
      "1216 [C loss: 1.335388] [G loss: 0.581017]\n",
      "1217 [C loss: 1.369513] [G loss: 0.574717]\n",
      "1218 [C loss: 1.349305] [G loss: 0.576387]\n",
      "1219 [C loss: 1.345539] [G loss: 0.574840]\n",
      "1220 [C loss: 1.353195] [G loss: 0.578191]\n",
      "1221 [C loss: 1.351761] [G loss: 0.573183]\n",
      "1222 [C loss: 1.336481] [G loss: 0.574953]\n",
      "1223 [C loss: 1.345625] [G loss: 0.573936]\n",
      "1224 [C loss: 1.381747] [G loss: 0.567416]\n",
      "1225 [C loss: 1.365276] [G loss: 0.569939]\n",
      "1226 [C loss: 1.353945] [G loss: 0.573949]\n",
      "1227 [C loss: 1.377097] [G loss: 0.572458]\n",
      "1228 [C loss: 1.343905] [G loss: 0.570599]\n",
      "1229 [C loss: 1.368283] [G loss: 0.568053]\n",
      "1230 [C loss: 1.382318] [G loss: 0.568588]\n",
      "1231 [C loss: 1.343560] [G loss: 0.572986]\n",
      "1232 [C loss: 1.348669] [G loss: 0.571453]\n",
      "1233 [C loss: 1.377197] [G loss: 0.571032]\n",
      "1234 [C loss: 1.329838] [G loss: 0.576295]\n",
      "1235 [C loss: 1.337661] [G loss: 0.576208]\n",
      "1236 [C loss: 1.363353] [G loss: 0.573398]\n",
      "1237 [C loss: 1.347086] [G loss: 0.572160]\n",
      "1238 [C loss: 1.324644] [G loss: 0.577510]\n",
      "1239 [C loss: 1.318160] [G loss: 0.576953]\n",
      "1240 [C loss: 1.336345] [G loss: 0.579125]\n",
      "1241 [C loss: 1.290632] [G loss: 0.579260]\n",
      "1242 [C loss: 1.310133] [G loss: 0.581876]\n",
      "1243 [C loss: 1.339337] [G loss: 0.579008]\n",
      "1244 [C loss: 1.291405] [G loss: 0.578543]\n",
      "1245 [C loss: 1.255376] [G loss: 0.586341]\n",
      "1246 [C loss: 1.312946] [G loss: 0.576547]\n",
      "1247 [C loss: 1.317602] [G loss: 0.579364]\n",
      "1248 [C loss: 1.316349] [G loss: 0.579978]\n",
      "1249 [C loss: 1.285668] [G loss: 0.580466]\n",
      "1250 [C loss: 1.298294] [G loss: 0.578516]\n",
      "1251 [C loss: 1.313762] [G loss: 0.574950]\n",
      "1252 [C loss: 1.332574] [G loss: 0.575033]\n",
      "1253 [C loss: 1.289792] [G loss: 0.573781]\n",
      "1254 [C loss: 1.303194] [G loss: 0.569451]\n",
      "1255 [C loss: 1.315980] [G loss: 0.572315]\n",
      "1256 [C loss: 1.306045] [G loss: 0.574190]\n",
      "1257 [C loss: 1.345704] [G loss: 0.570895]\n",
      "1258 [C loss: 1.365372] [G loss: 0.570076]\n",
      "1259 [C loss: 1.338644] [G loss: 0.567250]\n",
      "1260 [C loss: 1.264364] [G loss: 0.568659]\n",
      "1261 [C loss: 1.264715] [G loss: 0.567290]\n",
      "1262 [C loss: 1.344471] [G loss: 0.567213]\n",
      "1263 [C loss: 1.284755] [G loss: 0.568646]\n",
      "1264 [C loss: 1.301794] [G loss: 0.566901]\n",
      "1265 [C loss: 1.295884] [G loss: 0.568067]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(timesteps, latent_dim, run_dir, img_dir, model_dir, generated_datesets_dir)\n",
    "# gan, generator, critic = wgan.restore_training()\n",
    "gan, generator, critic = wgan.build_models(generator_lr, critic_lr)\n",
    "\n",
    "# gan.summary()\n",
    "# generator.summary()\n",
    "# critic.summary()\n",
    "        \n",
    "losses = wgan.train(batch_size, epochs, n_generator, n_critic, transactions, clip_value,\n",
    "           img_frequency, model_save_frequency, dataset_generation_frequency, dataset_generation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
